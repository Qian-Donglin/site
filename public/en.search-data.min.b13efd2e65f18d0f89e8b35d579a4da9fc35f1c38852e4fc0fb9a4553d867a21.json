[{"id":0,"href":"/docs/editorial/Graph/BrABC292-D/","title":"(Br) abc292 D","section":"Graph","content":" 問題リンク\n問題概要 # あるN頂点M辺無向グラフ(多重辺、自己ループあり)を与えられる。\n全ての連結成分の頂点数と辺の数が同じかを判定せよ。\n制約: $ N, M \u0026lt; 2 \\times 10^5 $\n自分の解法 # 木は頂点数=辺の数+1であると知られているので、木に1つだけ辺を追加したものであるか？の判定に帰着。\nグラフのまだ見てない頂点に関して、DFSしながら、すでに訪ねたことのある頂点にブチ当たった回数をカウントする。\n基本的には、2回ならば(DFSの行きで1回ぶちあたり、親頂点に戻って来てまた掘り進めようとしたら前に掘った兄弟の子孫にぶちあたる)条件に該当しそう。\n以下のように実装。\nint encountSame = 0;//visitedな頂点に当たった回数 int massSize = 1;//今の連結成分のサイズ void dfs(int now, int bef) { for (int i = 0; i \u0026lt; E[now].size(); i++) { auto next = E[now][i]; if (bef == next)continue; if (!visited[next]) { visited[next] = true; massSize++; dfs(next, now); } else { encountSame++; } } } しかし、今回の問題は多重辺、自己ループを許容していたので、面倒な例外処理をすることに。\n例外処理 # サイズ1の場合条件を満たすのは、自己ループが1つだけの場合。これを上のアルゴリズムに適用させると、encuntSame == 2, massSize == 1となり、例外。\nサイズ2の場合は、多重辺の場合(2-\u0026gt;3, 2-\u0026gt;3のように)、encountSame == 1, massSize == 2となる。理由は、例だと頂点2から3へ移動したとき、戻るときはさっき使った多重辺以外を使えばいいが、DFSでbefを保持してる以上、戻れないから(サイズ2だとループ完成=直前に戻る)\nよって以下のように。\nfor (int i = 0; i \u0026lt; N; i++) { if (!visited[i]) { massSize = 1; visited[i] = true; encountSame = 0; dfs(i, -1); if ((massSize != 2 \u0026amp;\u0026amp; encountSame == 2) || (massSize == 2 \u0026amp;\u0026amp; encountSame == 1)) ; else isOK = false; } } テストケースにこれが置いてあったので気づけたけどなかったら気づくのはかなり遅れたはず。この方法はよくない。\nUnion-Find # グラフの連結成分の個数といえば、同じラベルの要素の検索とサイズを高速取得できるUnion-Find木。\n今回の場合連結成分の頂点数と辺の数を調べたいわけだが、同じ連結成分について親のIDに頂点数が何個、辺が何本と毎回確認することで事足りる。\nなお、実装する際に、いつもの無向辺隣接リストで行う場合、\n辺の二重カウント防止に辺の始点or終点の頂点だけ見て、それの親頂点に加算。 2-\u0026gt;3で親が例えば2の場合、3-\u0026gt;2の逆辺も持つので、始点見るならroot(2)==2, root(3)==2と親に2回足されるので、最後に割る2しておく。 UnionFind uf(N); for (int i = 0; i \u0026lt; M; i++) { int a, b; cin \u0026gt;\u0026gt; a \u0026gt;\u0026gt; b; a--, b--; E[a].push_back(b); E[b].push_back(a); uf.unite(a, b); } for (int i = 0; i \u0026lt; N; i++) { countNode[uf.getroot(i)]++; } for (int i = 0; i \u0026lt; N; i++) { for(auto to : E[i]) countEdge[uf.getroot(to)]++; } bool isOK = true; for (int i = 0; i \u0026lt; N; i++) { if (countNode[i] != countEdge[i] / 2) isOK = false; } ACソース\n"},{"id":1,"href":"/docs/editorial/SquareDivide/CABC293-E%E5%B9%B3%E6%96%B9%E5%88%86%E5%89%B2/","title":"(C) abc293 E(平方分割)","section":"Square Divide","content":" 問題文\nこれは平方分割の解法であり、別解として\n行列累乗 int128と数論で計算。 がある。\n概要 # $$\\Sigma_{i=0}^{X-1} A^i$$を$M$で割ったあまりを求めよ。\n制約: $$ A, M \\in [1, 10^9], X \\in [1, 10^{12}] $$\n解法 # 明らかに全部計算するのは間に合わないが、$\\sqrt{X}$回の加算なら間に合うところから出発する。\n繰り返し二乗法で使われているように、$a^{20}=(a^{10})^2=((a^5)^2)^2$で、一度まとまって計算したものをまた他と累乗させると手間が減る事を利用。\n今回の場合、$[0, \\lfloor \\sqrt{X} \\rfloor)$までにiを実際に増加させて加算させてみる。これは$O(\\sqrt{X})$。\n$$ B = 1 + A^1 + A^2 + \\cdots + A^{\\lfloor \\sqrt{X} \\rfloor - 1} $$\nこの時、\n$$ BA^{\\lfloor \\sqrt{X} \\rfloor} = A^{0 + \\lfloor \\sqrt{X} \\rfloor} + A^{1 + \\lfloor \\sqrt{X} \\rfloor} + \\cdots + A^{2\\lfloor \\sqrt{X} \\rfloor - 1} $$\nとなる。よって、$BA^{\\lfloor \\sqrt{X} \\rfloor}, BA^{2\\lfloor \\sqrt{X} \\rfloor} \\cdots$をそれぞれ$O(1)$で求められる。 これも$O(\\sqrt{X})$。\n肩の指数が大きい方から取った何個かが残るかもしれないので、それについても考える。\nAtcoderの解説では、残ったものは1つ1つ足し集めればよい。これはたかだか$O(\\sqrt{X})$しかないので、全体でこのアルゴリズムは$O(\\sqrt{X})$に重めの定数倍がついている。これで通るはずだが、重い定数倍があるからか自分は通らなかった。\n残ったものの集計の高速化 # 残ったものは肩の指数が連続していて、$A^{k \\lfloor \\sqrt{X} \\rfloor}, A^{k \\lfloor \\sqrt{X} \\rfloor + 1} \\cdots$のような値。これに関しては、ステップ1の集計の時に一緒にこちらも集めて、最後に倍率を乗じて足せばよい。\n何個余るかに関しては、明らかに$X \\div \\lfloor \\sqrt{X} \\rfloor$の余りの数だけなので、実装も容易。\n実装について # いろいろと境界条件がこんがらがるが、半開区間で処理、具体例でシミュレーションである程度対策できる。\nACコード\n"},{"id":2,"href":"/docs/editorial/Implement/G-abc315-E/","title":"(G) Abc315 E Index","section":"Implement","content":" 問題リンク\n問題概要 # $H \\times W$のフィールドの各マスに、色a-zである石を置いている。次の動きを繰り返せるだけ繰り返した時、石は何個残る？\n各行について、全てが同じ色ならば、その石全てに印をつける。 各列について、全てが同じ色ならば、その石全てに印をつける。 印がついた石をすべて取り除く。 制約: $H, W \\in [1, 2000]$\n考え方 # 愚直にシミュレーションしかなさそう。各行を見るたびに$O(x)$かかるなら、全体で$I((H + W)^2 x)$かかる。$o(x)$は可能ならば定数レベルに抑えたい。(毎回の操作では最悪$O(H + W)$の行と列についての操作。全体ではたかだか$H + W$回までしか使われない。同時に達成されることはないが、かなり大きい上限値。実際はずっと小さい。)\nここで、色がたかだか26色しかないことを見る。各行と列ごとにcolor[i] := 色iの石の個数とすれば、各行や列を取り除くときは1減らす、各行や列が同じ色かどうかの判定も26回走査すればよいので、$o(x)$は26という定数と考えていい。\n後は実装すればいい。\n実装上のやり方 # ステップ1とステップ2は実質同時である(取り除くことは同時)ので、実装するときも指示通り、\nステップ1で印をつける(印をつける代わりに行iをマークする) ステップ2でも印をつける(印をつける代わりに行jをマークする) 印がついてるところを取り除く 何色を取り除くのかに関しても、ステップ1,2の時点でメモしておく。この行or列はどうせ同じだからあとで見ればいい、はミスのもと！(今回の場合実際に取り除かれた石は以後の判定に影響しないので実際にWAを産む。愚直に実装してくれ) 終了条件判定(行と列の残存がいずれかが1になる、取り除けなかったなど) で行うべき。\nACコード\n追記 # 毎回26色を見なくても済むやり方はある。('a', 4), ('b', 2), ('f', 1)みたいに保持すること。 そして例えばここからfを取り除くと0になるので、('a', 4), ('b', 2)に簡約すればよい。\nこれも後で実装する。\n"},{"id":3,"href":"/docs/editorial/Grid/G-abc317-E/","title":"(G+) abc317 E","section":"Grid","content":" https://atcoder.jp/contests/abc317/tasks/abc317_e\n$H \\times W$のグリッドがある。各グリッドは壁は通路で、始点終点がある。グリッドには人が立っている可能性があり、人の向いてる方向は上下左右^v\u0026lt;\u0026gt;である。人は向いてる方向で他の人や壁に当たらない限り視線が通っている。\n人の視線を完全に避けて始点から終点まで行く補数を求める。\n制約: $H, W \\in [1, 2000]$\n解法 # 上手く視線が通る場所をすべて印をつけたい。しかし人はいっぱいいて全てのマス付けると間に合わなさそう。\n**ん？？？？？**間に合わない？本当に？よく考えて。\n各マスは上下左右から見られるが、視線がさえぎられる関係上、各マスの上下左右はそれぞれたかだか1人までにしかカバーされない。\nつまり、各マスはたかだか4回までしか見られない！実は大丈夫じゃないか！\nACコード\n間に合わなさそうだと思っても、それぞれはたかだかX回とちゃんと評価することが大事。グリッドに限った話ではないが。\n実装に関しては各方向でそれぞれ書くと大変なので、埋める部分は次のように書いた。\n//視線が通っている処に印をつける。 for (int i = 0; i \u0026lt; H; i++) { for (int j = 0; j \u0026lt; W; j++) { if (field[i][j] == \u0026#39;\u0026gt;\u0026#39; || field[i][j] == \u0026#39;v\u0026#39; || field[i][j] == \u0026#39;\u0026lt;\u0026#39; || field[i][j] == \u0026#39;^\u0026#39;) { isNG[i][j] = true; //その方向に印をつけていく。 int direction; if (field[i][j] == \u0026#39;\u0026gt;\u0026#39;) direction = 0; if (field[i][j] == \u0026#39;v\u0026#39;) direction = 1; if (field[i][j] == \u0026#39;\u0026lt;\u0026#39;) direction = 2; if (field[i][j] == \u0026#39;^\u0026#39;) direction = 3; for (int delta = 1;; delta++) { int nh = i + vx[direction] * delta, nw = j + vy[direction] * delta; if (nh \u0026lt; 0 || nw \u0026lt; 0 || nh \u0026gt;= H || nw \u0026gt;= W) break; if (field[nh][nw] == \u0026#39;#\u0026#39; || field[nh][nw] == \u0026#39;\u0026gt;\u0026#39; || field[nh][nw] == \u0026#39;v\u0026#39; || field[nh][nw] == \u0026#39;\u0026lt;\u0026#39; || field[nh][nw] == \u0026#39;^\u0026#39;) break; isNG[nh][nw] = true; } } } } オーバーキルというか無駄 # 1マスごとに見られる回数の上限に気づかなかった場合。\n縦のi列目の出現する壁や人のindexをまとめる二次元vectorを用意。横も同様。 上のものには最初は-1, 最後にはH or Wの盤兵を入れておく。 これらは単調増加なので、各人にフォーカスするとき、次にどこまで埋めればいいのかは二分探索で求められる。 埋める作業に関しては二次元imos法で行えば最後にまとめてでできる。 こうして、同様に視線が通ったマスの全てを洗い出せる。 オーバーキルだしこれを書いてる途中に1マスごとに何回までしか見られないのかはちゃんと気付くべき。\n"},{"id":4,"href":"/docs/editorial/DP/%E6%A1%81DP/Y-abc317-F/","title":"(Y-) abc317 F","section":"桁 Dp","content":" 問題リンク\n"},{"id":5,"href":"/docs/article/Weakly-Supervised-Learning/2017%E6%99%82%E7%82%B9%E3%81%AE%E5%90%84%E6%89%8B%E6%B3%95/","title":"2017時点の各手法","section":"Weakly Supervised Learning","content":" 元論文\nIntroduction # Weakly Supervised Learningでは3つの分野がある。\nIncomplete Supervision　学習データの一部にだけラベルがあって、他にない。これ、Semi Supervised Learningでは？？？ 大量の写真は集められるけど、アノテーションのコストもあって、一部しかアノテーションできないやつ。 Inexact Supervision　ラベル付けの粒度が荒く、細かい粒度のラベル付けはできない。 画像内の各オブジェクトについて、 Inaccurate Supervision　アノテーションにに一部間違いを混入されている 実際、これらは並列して起こることもよくある。簡単のため、この記事では二値分類を考える。\nIncomplete Supervision # 学習データの一部にだけラベルがあって、他にない問題。これ、Semi Incomplete Supervisionと混同してないか？\n定式化すると、$f : X \\to Y$の学習器で、データセットが$D$であり、(l+1のindexは間違えて表記してるが、l+1番目であるという意味)\n$$ D = {(\\mathbf{x}_1, y_1), (\\mathbf{x}_2, y_2), \\cdots, (\\mathbf{x}_l, y_l), \\mathbf{x}_l+1, \\cdots, \\mathbf{x}_m} $$\nつまり、$l$個のアノテーション済と、$m - l$個のラベルなしがある状態。\nこれを解決するテクは2つ。違いは下図。\nActive Learning # 専門家(人間)が、ラベル付けされてないうち、重要なデータ=(これがわかれば性能上がるぞ～～～のデータ)のみにラベルをつけて学習データにいれていく。\nどういうものがいいかは、 このスライドが参考になる。以下が基準の例。 情報量(不確かさ)をいかに減らせるか Uncertainty Sampling, Query-by-Committee 欠点は、不安定なことと、Queryを出すべきものを選ぶのが難しいところ。情報量の計算は大変なんよ。 入力パターンをいかにうまく減らせるか clustering methodでclusterを作ってそこから推測ってね。欠点は、Clusteringの結果に全依存しちゃうこと。 上の2つのハイブリッドもある。 コストは、OracleにQueryを出す回数そのものなので、これをいかに減らせるか。 データをうまく分類する仮説が存在してるなら、指数のオーダーで改善されていくという結果もある(元の13，14参考)。そういう仮説がないなら、active learningの下限はpassive learningの上限と一致するらしい。あまり役に立たないというのを示してるらしい。\nなお、困難なケースでも、Active Learningは繊細な設計に役に立つらしい。\nSemi-Supervised Learning # 少ないデータでまず分類器を学習させる。その分類器で高い確信度の結果を正しい結果と判定し、自動的にラベル付けをして、本番の学習データとして使っていく。\n微妙だと判断されたやつはつかわなければよし。 人間が介在しない。全自動化されてる。 一種として、transductive learningというのがある。 Bootstrapのように、今ある訓練データで学習器作って、ラベルなしのなかで確信度高いものにラベルをつける。→それを訓練データとして取り込んでもう1回学習器をつくる。という流れを何回も回す。 このサイトを参考にした。\n人の手が介在するやり方 # データが$n$クラスを代表する$n$個の混合ガウス分布が、それぞれ$a_i$の確率を持っているとする。\n$$ f(\\mathbf{x} | \\mathbf{\\theta}) = \\sum \\limits _{j = 1}^{n} a_j f(\\mathbf{x}, \\theta_j) $$\nそして、我々は特徴量をのベクトル$\\mathbf{x}_i$と、仮定した混合ガウスモデルの$g_i$から、最終的な予測ラベル$y_i$を得たい。\n$$ h(\\mathbf{x}_i) = \\argmax \\limits _{c \\in {Y, N}} \\sum \\limits _{j = 1}^{n} P(y_i = c | g_i = j, \\mathbf{x}_i) P(g_i | \\mathbf{x}_i) $$\n$$ P(g_i = j | {\\mathbf{x}}_i ) = \\frac{\\alpha _j f({\\mathbf{x}}_i | \\theta _j)}{\\sum \\nolimits _{k=1}^{n} \\alpha _k f({\\boldsymbol x}_i | \\theta _k) }. $$\nこれは、i番目の特徴量ベクトルと\n普通に判断はできないが、\nクラスター推測：データは出てきた元集団があり、そこからの抽出したデータは近傍に集まる。\n手法は以下の4つが有名。\nGenerative Method # ラベル付けされたものとされてないものは、似てるなら同じクラスに属すると判断する方法。EMアルゴリズムで推定できるらしい。\n高い精度を出すには、いい生成モデルを使う必要がある。\nGraph-based Method # 頂点が各データ、辺がそれぞれの関係(往々にして近さ)のグラフを作る。そして、例えば最小カットで2つに切り分けたサブグラフの中が同じクラスだと判定していく。(他にも手法ある)\n高い精度を出すには、いい感じにグラフを作る必要がある。また、明らかにこれは完全グラフを作るのでデータ量が増えるとこれは非現実的手法となる。\nLow-density Separation Method # 分離境界をデータ空間上の低密度な所にする、という手法。Semi-supervised Support Vector Machineが有名。\nDisagreement-based Method # 複数人の学習者を作り、協力してラベル付けをしていく。学習者のラベル付けの不一致が重要。\n1回の試行で、ラベル付けしてないデータから、それぞれが最も信頼できるものを選んで、その予測したラベルを正解として、自分以外の相手に渡して学習させる。こうやって何回も試行を繰り返す。これだとアンサンブル学習にもなるので効果も上がるっていうね。\n落とし穴 # 基本的にデータ量を増やせばガバガバアノテーションでもそこそこ正しいなら精度上がるが、逆に精度が下がることもある。\n半教師あり学習を使う時は、分類器の最悪の場合の性能をよくする=下振れを減らしたいとき。\nInexact supervision # ラベル付けの粒度が荒く、細かい粒度のラベル付けはできない。例えば、車とラベル付けした写真だが、車だけトリミングしたわけではなく、背景として道路標識、建物、信号、通行人なども移りこんでる。これを利用していくこと。\n$$ f: X \\to Y, D = { (X_i, y_i) } $$\n各$X_i$は、いろんな情報の詰め込みの集合になり、$X_i = {\\mathbf{x}_{i, 1}, \\cdots, \\mathbf{x}_im_i }$。$m_i$は添え字。これはつまり、$X_i = { 道路標識, 車, 信号, 通行人, 建物 }$のように。\nTweetみたいな雑多なものが色々混入するものとの相性は良さそうね。 by me. 他にも、画像処理、テキスト分類、スパム検出、医療診断、物体検出etcに応用できる。\n多くのマルチインスタンス学習のアルゴリズムがある。集合の中の決定の鍵となるアイテム(たち)を見つける研究もある。\nただし、集合内の各アイテムが異なるルールで分類されるケースだと学習はうまくいかないらしい。\n2021の新しいやつ論文もある。Labeled Data Generation with Inexact Supervision.\n2020の新しめの論文その2で、日本語の解説。数式多め。\nInaccurate Supervision # アノテーションにに一部間違いを混入されている。\nよくあるのが、ラベルノイズ。間違ったラベル付けを修正していくことをやっていく感じ。\n一例として、data-editing approachはグラフを作る。各点の近傍にある点に対して、異なるラベルを持つなら2つのノードを辺で結び、重みを与える。明らかに、まわりと距離近いのに違う！と判定されてるのは特異点か、ラベル付けミス。この場合、その点を学習データから削除するかラベルを変更することになる。ただ。高次元空間では信頼性低いし、疎空間の場合そもそも意味がない。\nクラウドソーシングでアノテーションの外注でも、間違いを混入される可能性がかなり高い。ワーカの質とタスクの難易度をモデル化できれば、アノテーションの質も高くなる。2017現在でEMアルゴリズムで確率モデルを作る研究がある。\nただ、ラベルの質よりも、学習したモデルの性能の方が大事みたい。つまりアノテーションの質に関して四の五の言う意味はないです。ここらへん中本くん成田くんの発表やね。\nただ、アノテーションするとき、Unsureは付け加えるべき。自信のないアノテーションが回避できるようにしないと。実際これで性能は上がる結果が研究にある。double or nothingの方法でやると、自信があるタスクだけつけさせることができる。\n結論 # 上述の3つは組み合わせることができるよ！\n他にも、time-delayed supervisionという強化学習での問題とかもこれの一種。\nまた、二値分類に注目したが、複数クラス分類や回帰問題にもこれは同じように反映できる。マルチクラスへの割り当てができる問題だと、更に難易度が上がる。\n"},{"id":6,"href":"/docs/article/graphic_design_ML/Content-aware-Generative-Modeling-of-Graphic-Design-Layouts/","title":"Content Aware Generative Modeling of Graphic Design Layouts","section":"Graphic Design","content":" 基本情報 # 著者 Xinru Zheng, Xiaotian Qiao, Ying Cao, Rynson W. H. Lau 題 一般的な文章に対するレイアウトの分析器 詳細 # Introduction # いい感じのレイアウトを提案するには写真、中身をちゃんと理解しないとだめやね。\n先行研究では、高レベルの意味論(semantic)だけで、レイアウトを構成していた。画像や文章の中身には踏み込んでなかった。(画像の知覚的重要度や、画像とテキスト要素の間の注意の遷移　についてだけらしい)\nこの研究では、文章や画像の中身に踏み込んで、レイアウトを構成する手法を提案する。ただし、雑誌に集中。\n確率的な生成フレームを作る。学習データは、画像文章付きの記事のレイアウトと、高レベルデザインの条件付き分布からなるらしい。 複雑なレイアウト分布をモデリングするGANも、デザインの構造的/カテゴリ的属性を符号化する意味 埋め込みネットワークを導入。\n特に人の手で訓練データ以外を与えずに学習させるよ、つまりデザインの規則とかに従って人が雛型の設計はいらないよ。研究のために雑誌のレイアウトのDBまで作ったよ。\n結果として、画像、デザインのカテゴリ、内容の要約を教えてくれれば、デザイン案を提示するようにしたよ。 それに加えて、事前にデザインのラフ案も入れることができて、それも考慮されるよ。\n深層生成モデルとは # 深層学習モデルを使って画像とかを生成するモデル。\n形状の生成では、Deep Belief NetworkやDeep Boltzmann Machinesがよく使われていた。だが、収束遅いし計算も大変。 最近は variational auto encoderや敵対的生成ネットワークGANがもっといいという結果に。ただ、VAEはぼやけた画像になりがちだが、GANではその課題はない。だが、GANには潜在的なパラメタを推定する力はないらしい。\n先行研究では2D/3D画像の生成で深層生成モデルを使ってたが、今回はGANのフレームワークで、レイアウトを深層生成モデルで出す。\nほぼ同時期の研究として、**LayoutGANという、GANベースのランダムな2Dのオブジェクトをいい感じに配置する研究(Layout-GAN: Generating Graphic Layouts with Wireframe Discriminator)**もある。ただこれはコンテンツの中身に踏み込んでるわけじゃない。\nDataset # 今回の研究のためにわざわざDB作ったよ。ground-truth(実物データ)のデータでがんばってやったよ。 既存のものは小さくて使いもんにあんまならなかった。3919ページ集めたよ。 カテゴリは、ファッション、食事、ニュース、科学、旅行、結婚。\n各ページについて、以下の6点について特徴量を出したよ。\n文章 画像 見出し 画像の上にある文章 画像の上の見出し 背景 文章には5つにははいらないが、文章でもないなぁってものも入れる。これを各ピクセルごとに、6つのうちのどれかと分類した。\nラベル付けの補助 # 手動だとさすがに大変なんで、一部だけ手動でやって、それをもってして自動的にラベル付けをするFully Convolutional Networkのモデルを1つ作って任せた。その結果を人の手で微修正した。\nキーワードの抽出 # 中身を見るには画像や文字の中身を見る必要がある。 画像はそのまま見るにしても、文字データは全ては冗長過ぎる。 ここでは、Googleの Rapid Automatic Keyword Extractionを利用した。このキーワードを使って、カテゴリ分けを行った。recipe, tasteならfoodのように。\nこのままだと混じりけが多いので、学習データのうち、上位100個を抽出してそこから類義語などを手動で消す、\n実際の入力では、RAKEでキーワードを抽出して、それらがどのジャンルなのか、によって判断する。\nレイアウトの表現 # 先行研究ではパラメタだったが、この研究では思い切って画像に。各ピクセルに上の6種類のラベルのいずれかをつける形に。 この画像ベースのものは、CNNで扱いやすいらしい。\nピクセル自体は細かくすれば結果はよくなるが、ここでは計算コストとの兼ね合いもあって $60 \\times 45$に。だいたいの雑誌ページは4:3だし。\nモデルの設計 # Future Work # 気になった参考文献とか先行研究とか # ガイドに従った色々考えたレイアウトの自動生成の先行研究 # Automatic Stylistic manga layout, 2012, ACM\nAttention-Directing Composition of Manga Elements, 2014, ACM\nテンプレートベースのレイアウトの自動生成 # 手動で作ったいくつかのデザインパターンへの誘導。\nWebページの内容を別のレイアウトに変換する。 # example-based retargeting for Web design, ACM\n漫画のレイアウトを統計的に学習して、自動生成する # 視覚的原理によって定義したenergy functionの最適化によって、ページのレイアウトの最適化の研究 # 事前に見てほしいルートに沿うようなレイアウトの自動生成 # "},{"id":7,"href":"/docs/article/graphic_design_ML/Learning-to-Generate-Posters-of-Scientific-Papers/","title":"Learning to Generate Posters of Scientific Papers","section":"Graphic Design","content":" 基本情報 # 著者 Yuting Qiang, Yanwei Fu, Yanwen Guo, Zhi-Hua Zhou, Leonid Sigal 題 科学論文のポスターの自動生成 発表年 2016 中身 # Introduction # 論文をポスターにするのは非常に重要なまとめ操作。だが、デザインは詳しくない人にとっては大変なこと。 この研究やるのに以下の問題があった。\n論文内容をどう抽出するか ポスターのレイアウトをどうするか 図や表をレイアウトに従いどう配置するか。 この論文では、data-drivenな自動生成手法を作ってみた。鍵は上の3点からして「内容の抽出とレイアウト構成」。\n内容の抽出は、TextRankを使ったよ。(筆者注　2019年時点ではGoogleのRAKEのほうがいいと思う)。となるとキモはレイアウト構成。\nレイアウト構成は、次の3ステップで作っている。\n簡単な確率的グラフィカルモデルを作り、それでパネルの属性を推論する。 パネルを表す木構造を導入する(設計した再帰的なアルゴリズムで使うため) 木構造とアルゴリズムから、別の確率的グラフィカルモデルを作り、実際にポスターにデザインを落とし込む。 確率的グラフィカルモデル # 関連研究 # General Graphic Design # テキストベースのレイアウトは、2003年にJacobらが研究した。 1ページでの画像的なレイアウト アルバムでのレイアウト インターフェースのデザイン 問題の定式化 # ポスターの集合$M$がある。 各ポスター$m \\in M$には、パネル$\\mathbf{P}_m$がある。(論文発表のパネルは、入れ子構造みたいなやつ) 各$p \\in \\mathbf{P}_m$には、graphical elementの$\\mathbf{G}_p$が存在してる。これは表や図などにあたる。 各$p \\in \\mathbf{P}_m$には、以下の要素もある。 テキストの長さ$l_p$。 全てのテキストのうち、このパネルでのテキストの割合$t_p$ 全ての画像のうち、このパネルでの画像の割合$g_p$ パネルのサイズ(積)$s_p$と、アスペクト比$r_p$ 各$\\mathbf{G}_p$には、以下の要素がある。 画像のサイズ(積)$s_g$と、アスペクト比$r_g$ 横軸でいうとどこにあたるのか、left, center, rightの3つ。Texで作る時にこれらを指定するので。$h_g$ パネルの横幅に対して、画像の横幅の比$u_g$ ↑の各要素を、各$p$ごとに決めて、$\\mathbf{G}_p$をも生成するのが仕事。\n学習データの$t_p, g_p, l_p, r_g, s_g$から、他を予測するモデルならいいと思うが、それでは$s_p$と$u_g$のような、パネルのサイズと画像の横幅比みたいな関係ありそうなパラメタの関係は見れなくなる。画像はある程度ないと小さくて見づらいから、パネルのサイズ関係なくあまり変わらない、みたいな。\nユーザーのやること # textRankでキーワードを抽出して、図や画像はユーザーが手動で入れるだけ。\nどう実装したか # パネルのパラメタの推定 # パネルサイズ$s_p$と、アスペクト比$r_p$は、テキストの割合$t_p$と画像の割合$g_p$に依存する、という仮説でこの研究は考える。\n$$ P(s_p, r_p | t_p, g_p) = \\Pi _ {p \\in P} P(s_p | t_p, g_p) P(r_p | t_p, g_p) $$\nこんな風に分解できる$s_p, r_p$はこの条件の下では独立と仮定する。\nそして、右辺はそれぞれ、線形ガウス分布に従うと仮定。\n$$ P(s_p | t_p, g_p) = N(s_p; \\mathbf{w_s} \\cdot (t_p, g_p, 1)^{T}, \\rho_s) $$ $$ P(r_p | t_p, g_p) = N(r_p; \\mathbf{w_r} \\cdot (t_p, g_p, 1)^{T}, \\rho_r) $$\n$\\mathbf{w_s}, \\mathbf{w_r}, \\rho_s, \\rho_r$は訓練データから最尤推定で得られる。\nパネルの配置の生成 # 従来はただ二、三列に配置するだけのつまらないものだった。この研究では、木構造をまず作る。\nRoot(「横」を,「0.25:0.75」に分割して, 謎の数字)\rこうやって2つに分割して、新たに出来た領域に対して\r1. 終端でこれ以上もう分割はしない。\r2. またRoot()をつなげて分割する。 ここで、レイアウトをするときに、可能な限り各パネルのアスペクト比$r_p$を崩したくない、と考える。(この論文の仮定はそうやってる)。\n$$ Loss(L, L\u0026rsquo;) = \\Sigma_{i=1}^{k} |r_{p_i} - r_{p_i}^{\u0026rsquo;}| $$\nこんな風に、Loss()を定義する。「\u0026rsquo;」の記号があるものは、レイアウトの変更後を指す。\nアルゴリズムとしては以下のようにする。どうやら、パネルの配置する順番自体を変更はしないようだ。\nページのサイズと、各パネルを(ページサイズs_p, アスペクト比r_p)の2つの値で持つ。 //なんかよくわからん if(pagenumber == 1){ ただ置くだけでいい。 } else { //[i, k - 1] for(int i = 1; i \u0026lt;= k - 1; i++){ double t = 1番目からi番目までのパネルサイズの和が占める、全てのパネルサイズの和に対する割合; double loss1 = PanelArrangement(i); } } "},{"id":8,"href":"/docs/article/order_thing/Learning-to-Order-Thing/","title":"Learning to Order Thing","section":"Order Thing","content":" 問題設定 # $a, b\\in X$とする。$a, b$に対して、$a \u0026lt; b$($b$は$a$より良い)か$a \u0026gt; b$($a$は$b$より良い)のように、優劣を伝えるクエリがあるとする。\n大量にその形のクエリを読み込んで、全体で$X$の各要素をランキングを作りたい。\n人が判断していると考えるので、$a \u0026lt; b$というクエリと$a \u0026gt; b$というクエリは両方存在しても矛盾とはみなさない。\n論文情報 # タイトル Learning to Order Things 刊行物 Journal of Artificial Intelligence Research 10(1999) 243-270 著者 William W. Cohen, Robert E. Schapire, Yoram Singer 論文の手法 # 設定 # $X$は比較対象の集合とする。 pref(a, b)を、$X \\times X \\rightarrow [0, 1]$の関数とする。 1に近いほど、$a \u0026gt; b$の傾向が強い。 0に近いほど、$b \u0026lt; a$の傾向が強い。 0.5に近ければ、どちらとも言えない。 pref(a, b)は、本質的にはいくつかの「好み関数」の線形合成である。下図参照。 これは、左と真ん中は、「ある順序(大概はわからないけど)に従って、作られるpref()」であり、それらを線形合成して新たなpref()を右図に作っている。\nまた、明確に順位を持っているときに、pref()を作ること自体は容易である。以下のようにすればいい。\npref(a, b)で、$a \u0026lt; b$なら、1、$a \u0026gt; b$なら0、$a = b$なら0.5 学習のやり方 # あらかじめ、各人の意見に重み$w_i$を設定する。\n毎回、何人かの人からそれぞれ各々の$X$内でのランキングを作る。そのランキングをもとに、重みつきでpref()を作る。つまり以下の式。\n$$ pref(a, b) = \\Sigma_{i = 1}^{N} w_i pref_{sub}(a, b) $$\nそして、後述の方法でpref()から、あらたにランキングを生成し、それをuserにfeedback。\nそして、今のprefを使い、ロスを以下の式で計算。\n$$ ロス = 1 - \\frac{1}{|F|} \\Sigma_{(a, b) \\in F} pref(a, b) $$\nこのロスをもって、$w_i$を更新して、一周が終わる。\n$$ w_i \\leftarrow \\frac{w_i \\beta ^{ロス}}{Z} $$\nこの$\\beta$はハイパーパラメタであって、$Z$はweightの和が1となる正規化のために割っている。\nランキングの推定方法 # 私が知りたかったやつ。\nAGREE()という関数を考える。これは順位で$a$が$b$より優れるときのpref(a, b)を全て足し合わせたもの。\n一般論をいうと、厳密なランキングを見つけるのはNP困難である。\nというわけで貪欲法\n"},{"id":9,"href":"/docs/article/Weakly-Supervised-Learning/PNU-Learning/","title":"PNU Learning(2017年)","section":"Weakly Supervised Learning","content":" 元論文\n参考資料　これありがたすぎる。\nIntroduction # 既存のPU学習では、一定の分布に従ってデータがあり、そこから偏らずに抽出した、という仮定が必要だった。 例えば Manifold仮説によれば、低次元のManifoldにデータが分布してるという。 現在、分布の仮説が正規化項？になって学習をいい方向にもっていくというかたちだ。だが、分布の仮説が違うなら、弱教師付き学習の結果が間違ったことになるので、やらない方がマシという結論になる。結果の裏付けもある。\n2014年のPU学習の分析では、分布の仮定なしにもちゃんと性能は出ると示した。\nまた、 2016年の研究で、PU LearningははっきりとPositiveとNegativeがわかるものを凌駕する場合もあることを考証し、それの条件を突き止めた。\nこんな中で、Positive、Negative、Unknown、全部混ぜ混ぜにしてみたのがこの研究。\n背景 # 問題の設定 # $\\mathbf{x} \\in \\mathbb{R}^n, y \\in {+1, -1}$。データはこのような二値分類のタスク。これらは、$p(\\mathbf{x}, y)$の分布に従う(そういう分布があるという前提)。\nまた、\n$p_P(\\mathbf{x}) := p(\\mathbf{x}|y = +1)$　これはPositiveのデータの分布。これに従った$n$個の独立なデータの集合を$\\Chi_P$とする。 同様に、$p_N(\\mathbf{x}) := p(\\mathbf{x}|y = -1)$　これはNegativeのデータの分布。これに従った$n$個の独立なデータの集合を$\\Chi_N$とする。 ラベルなしは、$p(\\mathbf{x}) := p(y = +1) p_P(\\mathbf{x}) + p(y = -1) p_N(\\mathbf{x})$に従う。これに従った$n$個の独立なデータの集合を$\\Chi_U$とする。 なお、$p(y = +1) + p(y = -1)=1$はもちろん成り立つ。二値分類なので。 次に、\n$g := \\mathbb{R}^d \\to \\mathbb{R}$という決定関数を考える。 符合が正ならPositive、それ以外ならNegativeとする。 $l := \\mathbb{R} \\to \\mathbb{R}$という損失関数を考える。 $m = y g(\\mathbf{x})$の広い$m$で、$l(m)$は小さい値を取る*、という性質を持つ。 つまり、損失関数は0に近い部分(判別しづらいと思われてるもの)以外は、小さい値=誤差小さいと判定される。　これまじ？わからんけど・ そして、損失関数から、リスク関数を定義する。$R_P(g) := \\mathbb{E}_{P_p}[l(g(\\mathbf{x}))]$　$p_P(\\mathbf{x})$=Positiveのデータの分布においての取り得る$\\mathbf{x}$についての、損失関数の期待値。。\n$R_P(g) := \\mathbb{E}_{p_P}[l(g(\\mathbf{x}))]$　$p_P(\\mathbf{x})$=Positiveのデータの分布においての取り得る$\\mathbf{x}$についての、損失関数の期待値。 $R_N(g) := \\mathbb{E}_{p_N}[l(-g(\\mathbf{x}))]$　$p_P(\\mathbf{x})$=Negativeのデータの分布においての取り得る$\\mathbf{x}$についての、損失関数の期待値。 $R_{U, P}(g) := \\mathbb{E}_{p_U}[l(g(\\mathbf{x}))]$　$p_P(\\mathbf{x})$=Unknownのデータの分布においての取り得る$\\mathbf{x}$についての、損失関数の期待値。 $R_{U, N}(g) := \\mathbb{E}_{p_U}[l(-g(\\mathbf{x}))]$　$p_P(\\mathbf{x})$=Unknownのデータの分布においての取り得る$\\mathbf{x}$についての、損失関数の期待値。 これから、各学習のリスク関数を求めるが、ここで$p(\\mathbf{x}, y)$の真の分布はわからないので、数式でそれを使わないようにする。(期待値で出てきたら、それ以上変形はストップ)\nPN分類。 # Positive VS Negative。弱教師付き学習以前の普通の学習やね。リスク関数は以下のように定義する。\n$$ R_{PN}(g) := p(y = +1) \\mathbb{E} _{p_P} [l(g(\\mathbf{x}))] + p(y = -1) \\mathbb{E} _{p_N} [l(g(\\mathbf{x}))] $$\n$$ = p(y = +1) R_P(g) + p(y = -1) R_N(g) $$\nここで損失関数$l(m) = \\max(0, 1 - m)$のヒンジ損失を使うならば、SVMのリスクと一致する。\nつまり、SVMで分類する限りの性能をPN分類は出せる。同じ事だしまあ、それはそうなんですよ。\nPU分類 # ここでは、Positiveとラベルなしのデータを互いに集める。 2007年の研究ではPU分類できたが、愚直な分類ではbiasがあるらしい。これを解決したのが、 2014年のdu Plessisの研究。どんなに悪くてもベストから$2 \\sqrt{2}$倍という評価を示した。\nPU分類ではNegativeについての期待値はないので、$R_{PN}(g)$のPN分類のリスク関数を、Negativeがない形に変形する。$p(\\mathbf{x}) := p(y = +1) p_P(\\mathbf{x}) + p(y = -1) p_N(\\mathbf{x})$によって変形。\n$$ p(y = -1) \\mathbb{E} _{p_N} [l(-g(\\mathbf{x}))] = \\mathbb{E} _{p_U} [l(-g(\\mathbf{x}))] - p(y = +1)\\mathbb{E} _{p_P} [l(-g(\\mathbf{x}))] $$\n$$ R_{PN}(g) = p(y = +1) R _P (g) + \\mathbb{E} _{p_U} [l(-g(\\mathbf{x}))] - p(y = +1)\\mathbb{E} _{p_P} [l(-g(\\mathbf{x}))] $$\n$$ = p(y = +1)(\\mathbb{E} _{p_P} [l(g(\\mathbf{x})) - l(-g(\\mathbf{x}))]) + R _{U, N} $$\nまた、ここでNegativeの代わりにラベルなしを使うので、$R_{PN} = R _{RU}$とリネームしておく。\n損失関数$l(m)$としてのヒンジ関数とランプ関数 # ここで、$\\mathbb{E} _{p_P} [l(g(\\mathbf{x})) - l(-g(\\mathbf{x}))] $ という量が問題に。 2014年のdu Plessisの研究で、なんでヒンジ関数はNGで、ランプ関数がいいのかを少し書く。\n上の量だと、$l(-g(\\mathbf{x}))$のマイナスさえなければ、お互いに打ち消せる。だが、打ち消せない以上、仕方ない。\nランプ関数 Non-Convexなら # $$ l(m) + l(-m) = 1 $$\nを満たすものとして、\n$$ l(m) = \\frac{1}{2} \\max(0, \\min(2, 1 - m)) $$\nというものがある。これを満たすとき、$l(m) - l(-m) = 2l(m) - 1$が成り立つ。これを代入すると、\n$$ \\mathbb{E} _{p_P} [l(g(\\mathbf{x})) - l(-g(\\mathbf{x}))] = \\mathbb{E} _{p_P}[2l(g(\\mathbf{x})) - 1] $$\n$$ R_{PU} = 2 p(y = +1) R_{P} + R _{U, N} - p(y = +1) $$\nとキレイに式変形できる。このことから、PN分類で使われるSVMと似たように、各クラスでの重み付きの損失関数の最小化を既存のSVMで解くという問題に帰着できる。たぶん下式のようなソフトマージンで\n$$ \\frac{1}{2} || \\mathbf{w} || + 2p(y = +1) \\sum _{i \\in P} l(g(\\mathbf{x}_i) + \\sum _{j \\in U} l(-g(\\mathbf{x}_j)) $$\nHinge関数 Convexたち # $$ l(m) - l(-m) = -m $$\nこれはヒンジ関数が満たす。この場合、\n$$ \\mathbb{E} _{p_P} [l(g(\\mathbf{x})) - l(-g(\\mathbf{x}))] = \\mathbb{E} _{p_P}[-g(\\mathbf{x})] $$\n$$ R_{PN} = R _{PU} = R _{U, N} + \\mathbb{E} _{p_P}[-g(\\mathbf{x})] $$\nとなる。この場合、$g$を訓練すればするほど、Positiveなクラスで学習させてるので、$mathbb{E} _{p_P}[-g(\\mathbf{x})] = mathbb{E} _{p_P}[-1] = -p(y = -1)$となる。これは、データの中でNegativeが多ければ多いほど、全体としての損失関数が減るということ。さすがにこれ、データの性質に依存し過ぎてうから、ヒンジ損失はまずくないか？by me\n論文としてはどっちも評価してみるらしいが、2014年でこっち良くないと言われてなかったか？\nNU分類 # さて、ここまでPUについて言った(というか2014年の論文のやつだが)が、NUについても同じ事をやればいい。つまり、\n$$ R_ {NU} = R _{U, P} + p(y = -1) \\mathbb{E} _{p_N} [l(-g(\\mathbf{x})) - l(g(\\mathbf{x})))] $$\nNon-Convexなら、\n$$ R_ {NU} = 2 p(y = -1) R_{N} + R _{U, P} - p(y = -1) $$\nConvexなら、\n$$ R_ {NU} = R _{U, P} + \\mathbb{E} _{p_N}[-g(\\mathbf{x})] $$\n本題 # PU、NU、PN学習についてここまで定式化してきたby先人。この論文では、\nPUNU学習 PNU学習 の2つを提案して、それぞれについて分析と実験も行った。\nPUNU学習 # 一番簡単なアイディアとして、PU学習とNU学習を単に組み合わせるだけ。PとU、NとUでやって、その結果を統合する。この時、損失関数も単純な線形合成になる。$\\gamma \\in [0, 1]$\n$$ R _{PUNU} = (1 - \\gamma) R _{PU} + \\gamma R _{NU} $$\n$\\theta_P = p(y = +1), \\theta_N = p(y = -1)$として、\nこの式に、Non-ConvexとConvexの違いを代入してみる。あまりにも長いので、 参考資料の部分で見て。\n実装 # SVMでやっぱりやる。上のように定義した2つのRについての学習を行う。\n学習器$g(\\mathbf{x})$は、$\\mathbf{\\phi}(\\mathbf{x}) = (\\phi_1(\\mathbf{x}), \\cdots, \\phi _b (\\mathbf{x}))$という$b$個の変換関数のベクトルとして、\n$$ g(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{\\phi (\\mathbf{x})} $$\nとする。これは、以下の値の最小化のソフトマージン。\n$$ \\hat{R}(g) + \\lambda || \\mathbf{w} || $$\nなお、実際の$R$はわからないので、ここではいくつかのサンプルから計算したempiricalな$R$を使う。\n理論解析 # 全体的な誤差の評価 # $$ g(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{\\phi (\\mathbf{x})} $$\nに対して、\n$$ "},{"id":10,"href":"/docs/article/Weakly-Supervised-Learning/Positive-Confidential-learning2017/","title":"Positive Confidential Learning(2017)","section":"Weakly Supervised Learning","content":" 元論文\n日本語解説はどうやらないみたい。\nこっちのGitHubレポジトリに3mの発表動画やスライドがある。テストコードも。\nIntroduction # Positiveデータしか持たないが、その中で各データに信頼度というパラメタがある。Unknown、Negativeはない。\n1クラス分類はあるが、ハイパーパラメタの調整ができない手法しかなかった。しかも、どんなにデータを増やしても、分布の境界を予測はできない。類似したものを見つけることができても、分類=境界線をいい感じに引く、ことはできない。\nこのPConfは、ハイパーパラメタを客観的に選ぶこともできる。これは、ERM(経験損失最小化 = 選んだ標本誤差の最小化)を使って実現されている。\nPU学習とも似てる手法ではあるが、PU学習で必要なPositiveのデータの事前確率の推定(2007年のPU学習では、$p(ラベル付き|データ)$を推定していた(結局あんま精度高くないよね)　これを指してるのだと思う)は不要。これを高精度で求めるのは苦労がかかるので、朗報。実は各データについての信頼度という情報には、事前確率、条件付確率、事後確率はすべて含まれている。\n問題の定式化 # データは$\\mathbf{x} \\in \\mathbb{R}^d$の形。ラベルは二値分類なので、$-1, 1$。これらは、未知の分布関数$p(\\mathbf{x}, y)$に従う。\n実現したいのは、$g(\\mathbf{}) : \\mathbb{R}^d \\to \\mathbb{R}$という二値分類器(出力が二値ではないのは、確からしさを残すため？ by me)\n例によって、損失がどれぐらい出てくるのかのリスク関数を定義する。\n$$ R(g) = \\mathbb{E} _{p(\\mathbf{x}, y)} [l(y g(\\mathbf{x}))] $$\n$l(m)$は損失関数。ラベルと予測器が同じ符合なら$l(m)$は0に、違う符合なら関数に渡されるのは負値で、より自信満々に間違えるほど大きな負値を渡すことになる。$l(m)$は大きなマイナスほど大きな値を返す。\n例によって、このリスク関数を最小化していく。\nここで、$p(\\mathbf{x}, y)$は未知なので、普通のERM(経験損失最小化)では、真の期待値を今あるテストデータの期待値で代用する。それはそう。しかし、PConfでは、違う。\nPConfでは、与えられたデータ群(と言ってもPositiveしかないが)に対して、$\\chi$という集合で、信頼度を定義する。\n$$ \\chi := (\\mathbf{x}_i, r_i) で、iは全てのデータ $$\n$r_i$は各データの信頼度\n$$ r_i = p(y = +1 | \\mathbf{x}_i) $$\n$\\mathbf{x}_i$はあるデータを全数つかわない場合、データ群から一様ランダムに抽出したものである。\nNegativeデータがないので、普通のERMの計算方法ではリスク関数を計算するできない。次のセクションでは信頼度で代わりに表示する方法を表す。\nPConf分類のやり方 # ERM(経験損失最小化)の枠組み # $\\pi _{+} = p(y=+1)$これは全データのうち、Positiveのデータの割合。あれ、Negativeとか見えないんじゃなかったっけ？by me\n$r(\\mathbf{x}) = p(y=+1 | \\mathbf{x})$ $r_i$はデータ$\\mathbf{x}_i$についての信頼度だった。これは、引数に渡したデータ$\\mathbf{x}$の信頼度。\n$\\mathbb{E} _{+}$は、$\\mathbb{E} _{p(\\mathbf{x} | y = +1)}$の場合、つまりPositiveのデータに限った場合の条件付期待値である。これらを使って、以下のように$R(g)$を表せる。\n$$ R(g) = \\pi _{+} \\mathbb{E} _{+} [ l(g(\\mathbf{x})) + \\frac{1 - r(\\mathbf{x})}{r(\\mathbf{x})} l(-g(\\mathbf{x}))] $$\n当然だが全部Positiveなので、$r(\\mathbf{x}) \\neq 0$である。\n式変形の意味 # 意味としては、以下の通り。(本来はAppendix Aにある)\n$$ R(g) = \\sum _{y = +1, -1} l(y g(\\mathbf{x})) p(\\mathbf{x}, y) d\\mathbf{x} = \\sum _{y = +1, -1} l(y g(\\mathbf{x})) p(\\mathbf{x}| y) p(y)d\\mathbf{x} $$\n$\\pi _{+} = p(y = +1)$、$\\pi _{-} = p(y = -1)$は積分の外に出せるので、\n$$ =\\pi _{+} \\mathbb{E} _{p(\\mathbf{x} | y = +1)} [l(yg(x))] + \\pi _{-} \\mathbb{E} _{p(\\mathbf{x} | y = -1)} [l(-yg(x))] $$\n第2項では、$y=-1$なので損失関数のなかには負の符号がつく**。ここで、一旦以下の値の変形を見る。\n$$ \\pi _{+} p(\\mathbf{x} | y = +1) + \\pi _{-} p(\\mathbf{x} | y = -1) $$\n$$ = p(\\mathbf{x}, y = +1) + p(\\mathbf{x}, y = -1) = p(\\mathbf{x}) $$\n$$ = \\frac{p(\\mathbf{x}, y = +1)}{p(p(y = +1 | \\mathbf{x}))} = \\frac{\\pi _{+} p(\\mathbf{x} | y = +1)}{r(\\mathbf{x})} $$\nここで、式の両辺に$\\pi _{+} p(\\mathbf{x} | y = +1)$が出てきてるので、$\\pi _{-} p(\\mathbf{x} | y = -1)$について解けば、\n$$ \\pi _{-} p(\\mathbf{x} | y = -1) = \\pi _{+} p(\\mathbf{x} | y = +1) (\\frac{1 - r(\\mathbf{x})}{r(\\mathbf{x})}) $$\nとえられる。これを代入すれば元の式が得られる。\n本筋の続き # $$ R(g) = \\pi _{+} \\mathbb{E} _{+} [ l(g(\\mathbf{x})) + \\frac{1 - r(\\mathbf{x})}{r(\\mathbf{x})} l(-g(\\mathbf{x}))] $$\n上のように上手いこと変形することで、Negativeのデータにおける期待値を消去してる。さらに言えばこの$R(g)$の最小化を考えるにあたり、未知の量$\\pi _{+}$は比例定数なので考えなくてもいい。\nでは、この$R(g)$は理想的な分布を使っていたわけだが、これを実際にあるデータの平均で期待値を代用すると、以下のような最適化問題を解く形になる。\n$$ \\min _{g} \\sum _{i = 1} ^ {n} [ l(g(\\mathbf{x}_i)) + \\frac{r_i}{1 - r_i} l(-g(\\mathbf{x}_i)) ] $$\n最適化問題だから分母消してもいいだろ←いいの？ # これに$r_i$を乗じた以下のもの\n$$ \\min _{g} \\sum _{i = 1} ^ {n} [ r_i l(g(\\mathbf{x}_i)) + (1 - r_i) l(-g(\\mathbf{x}_i)) ] $$\nこの式、更に簡約できる。\n$$ = \\min _{g} \\sum _{i = 1} ^ {n} p(y = +1 | \\mathbf{x}_i) l(g(\\mathbf{x}_i)) + p(y = -1 | \\mathbf{x}_i) l(-g(\\mathbf{x}_i)) $$\n$$ = \\min _{g} \\sum _{i = 1} ^ {n} \\sum _{y = -1, 1} p(y | \\mathbf{x} _i) l(y g(\\mathbf{x} _i)) $$\nこのように簡約できる。だが、実はこれは等価ではない。外側の$\\sum$が、$p(\\mathbf{x})$に対してだが、これが$p(\\mathbf{x} | y = +1)$ならば正しい。なぜなら、$r_i=p(y = +1 | \\mathbf{x}_i)$を掛けたので、\n$$ p(\\mathbf{x}_i | y = +1) = \\frac{r_i p(\\mathbf{x}_i)}{p(y = +1)} $$\n$$ r_i = \\frac{p(\\mathbf{x}_i | y = + 1) p(y = +1)}{p(\\mathbf{x}_i)} $$\nここで、$p(y = +1)$は定数？なので、まあ掛けないとあかん理由わかるね。\n本筋に戻る # 重点サンプリングとは　以下に簡単に書く。\n$$ \\mathbb{E} _{\\mu} [f(x)] = \\int f(x) \\mu(x) dx = \\int f(x) \\frac{\\mu(x)}{\\nu(x)} \\nu(x) dx $$\n$w(x) = \\frac{\\mu(x)}{\\nu(x)}$と言い換えておくと、この上の式は\n$$ \\int f(x) w(x) \\mu(x) dx = \\mathbb{E} _{\\nu} [ f(x)w(x) ] $$\nと言い換えられる。つまり、重み関数$w(x)$を掛ければ、別の分布についての期待値となる。この手法を重点サンプリング=Importance Samplingという。\n上の2つの式は、互いに重点サンプリングである、とも見れる。$p(\\mathbf{x})$と$p(\\mathbf{x} | y = +1)$についての期待値。\n性能評価 # 長すぎて略したい\u0026hellip;むずかしいし。\n結論として、\n$$ \\min _{g} \\sum _{i = 1} ^ {n} [ l(g(\\mathbf{x}_i)) + \\frac{r_i}{1 - r_i} l(-g(\\mathbf{x}_i)) ] $$\nは真のリスク関数に収束するが、\n$$ \\min _{g} \\sum _{i = 1} ^ {n} \\sum _{y = -1, 1} p(y | \\mathbf{x} _i) l(y g(\\mathbf{x} _i)) $$\nは真のリスク関数には収束しない。\n実装するには # $$ g(\\mathbf{x}) = \\mathbf{w} ^ T \\mathbf{\\Phi(\\mathbf{x})} $$\nこんな風に線形識別器$g(\\mathbf{x})$を作るとする。\nL2正規化をしたERMは、以下の最小化を行う。$\\lambda$は正規化定数であり、$R$は半正定値行列。\n$$ \\min _{\\mathbf{a}} \\frac{\\lambda}{2} \\mathbf{w} ^ T R \\mathbf{w} + \\sum _{i = 1} ^ {n} [ l( \\mathbf{w}^T \\mathbf{\\Phi(\\mathbf{x}_i)}) + \\frac{r_i}{1 - r_i} l(-\\mathbf{w}^T \\mathbf{\\Phi(\\mathbf{x}_i)}) ] $$\n正規化項はいつもの$|| \\mathbf{w} ||^2$ではないが、これは$R$が半正定値行列なので、コレスキー分解$R = C^T C$とすることができる。こうすると、$\\mathbf{w} ^ T R \\mathbf{w} = ((C\\mathbf{w}) ^T (C\\mathbf{w}))$と、別の線形空間に写像したうえでのL2正規化と見れる。\n損失関数は\n$l(z) = (z - 1)^2$ $l(z) = \\max(0, 1 - z)$ $l(z) = \\log(1 + e^{-z})$ これが一番よかった。 これは確率的勾配法とかを使えば最適化できる。\nこれ、**SVMで行けるんじゃね？？？？？**by me(たぶんやってる)\n実験 # どうやら線形モデルでも、DNNでも有効らしい。DNNで更に有効らしい。なんだこれは。\n線形モデルの合成実験 # 平均が$\\mathbf{\\mu} _{+}, \\mathbf{\\mu} _{-}$であり、共分散行列が$\\Sigma _{+} \\Sigma _{-}$であるデータを使う。\n提案した手法 提案した手法に$r_i$掛けて、違うよね？とした手法 回帰ベースの手法。信頼度事態を予測して、0.5以上か以下で判断する 1クラスSVMはGaussianカーネルを使う。性質上、正例データのみ使う。 完全にラベル付けされた例も、比較実験として使う。 提案手法、違うよね？手法、完全にラベル付けした手法は、$g(\\mathbf{x}) = \\mathbf{w} ^ T \\mathbf{x} + b$を使ったらしい。最適化ではエポック数5000、学習率0.001で行った。\nなお、$r(\\mathbf{x})$はどう計算されるんだ？に関しては「2つのガウス密度から解析的に計算される」そう。なにこれ？？？何も言ってないけど、2つのクラスの真の中心の距離の逆比っすかね？？？まあ、$r(\\mathbf{x}) = p(y = +1 | \\mathbf{x})$なので、$p(\\mathbf{x} | y = +1)$からベイズの定理で求めるのかね？\n結果としては、提案手法のPConfは、完全にラベル判明したものと同等の精度があると判明。\nなお、真の確信度わからずにガウス分布に従うノイズを加えると、性能が低下することにはするが、全然使える範囲でもあると。\nDNNにおいての手法適用 # Fashion-MNISTという服関連のデータセットでやった。1つのクラス(T-shirts Top)だけをPositive、他をNegativeにした。データは以下の4つに分ける。\n訓練 検証 テスト Positiveのデータに対して、$r(\\mathbf{x}_i)$を推定するための確率的分類器の学習用データ 本来は人がやるけど、ここはこれで代用や！ ロジスティック回帰で実装した。 ここらの前提はまだそれなりにある　把握しきれない。 3つの隠れ層でそれぞれ100個のニューロンがあり、最後に1つで総結合するニューロンを持つネットワークで、活性化関数はReLU関数を使った。\nあといろいろある。CIFAR-10についてもやってる。\n結果 # 提案したPconfはやはり素晴らしい性能だった。完全にラベル付けしたものの性能にも迫るぐらいね。\nほんへ終了\n"},{"id":11,"href":"/docs/article/Weakly-Supervised-Learning/Positive%E3%81%A7%E3%83%A9%E3%83%99%E3%83%AB%E6%9C%89%E3%82%8A%E7%84%A1%E3%81%97%E3%81%A7%E5%88%86%E5%B8%83%E3%81%8C%E9%81%95%E3%81%86%E6%99%82%E3%81%AEPU%E5%AD%A6%E7%BF%922019/","title":"Positiveでラベル有り無しで分布が違う時の PU学習(2019)","section":"Weakly Supervised Learning","content":" 元論文\nIntroduction # case controlシナリオという状況。\nラベルありPositiveはラベルなしとは別に抽出される。 ラベルなしは、全データの中から一部だけサンプリング この状況では、先行研究によれば、Positiveがどのようにラベル付けされるのかの仮定なくして、分類器を特定できない。なので、今までは完全にランダムに一様に選んだ＝Positiveのラベル付きとラベルなしは皆同じ分布に従うとしている。\nだが、現実ではそうはならんやろ、の例がどんどん多い。selection bias=選択バイアスが存在する。\n今までも対策があったが他の謎制約をつけることが条件だった。\nここでは、同じ$\\mathbf{x}$に対して、$p(o = +1 | \\mathbf{x}, y = +1)$が高ければ、$p(y=+1 | \\mathbf{x})$も高くなる。これは以下のような典型的なselection biasがある例でも、絶対に成り立つ。\nAnomaly Detectionの例 # ラベル付けされてないデータセットから、anomaly=異常値を検出する。ここで、異常値データが収集されるとき、データが異常であればあるほど、ラベル付けされる可能性が高くなる。つまり、同じanomalyの中でも、一様にランダムでラベル付けされてるわけではない！\n顔認識の例 # Positiveが明確に誰誰とわかる顔、Unlavelingが誰なのかが不明な顔。この時、例えPositiveであるものでも、はっきりと顔が映るのがPositiveで、ぼんやりならUnlavelingにされてしまう。\n選択バイアスがあるPU Learningでの問題設定 # $\\mathbf{x} \\in \\chi \\subset \\mathbb{R}^d$と学習データを定義し、それぞれ$y \\in -1, +1$のラベルを持つ二値分類である。$o \\in 0, +1$はラベルがない=0、ある=1。先人に従い、$p(\\mathbf{x}, y, o)$という未知の分布が存在するとする。\nサンプリングスキームには、censorシナリオ=one sampleとcase-controlシナリオ=two samplesがある。\nCensorシナリオ # censorシナリオでは、ラベル付け候補データは(当然Positive, Negative関係なく)周辺確率分布$p(\\mathbf{x})$に従いまず選ばれ、\nPositiveならば、$p(o=+1 | \\mathbf{x}, y = +1)$の確率でラベルをつける。 Negativeならば、前提条件として$p(o=+1 | \\mathbf{x}, y = -1) = 0$なので、ラベルをつけない。 Case-Controlシナリオ # case-controlシナリオでは、Positiveのデータは条件付き分布$p(\\mathbf{x} | y=+1)$から選ばれ、ラベルなしのデータは周辺確率分布$p(\\mathbf{x})$から選ばれる。\nこっちの方がわずかに一般的らしい。Case-Controlシナリオは、$p(\\mathbf{x}), p(\\mathbf{x} | o = +1)$によって生成されたサンプルへのアクセスを仮定するが、Censorシナリオは、$p(\\mathbf{x}), p(\\mathbf{x} | o = 0), p(\\mathbf{x} | o = +1)$を仮定するから大変らしい。 ここらへんあんまり理解できなかった\u0026hellip;\n本筋 # ということで、case controlシナリオに絞ると。\nPositiveのデータセット$[\\mathbf{x}_i] _{i = 1}^{n}$　$p(\\mathbf{x} | y = +1, o = +1)$　ラベル付き(当然Positiveである)という条件での$\\mathbf{x}$の分布に従って取得。 Unlabelingのデータセット$[\\mathbf{x ^\\prime}_i] _{i = 1}^{n ^{\\prime}}$　$p(\\mathbf{x})$　PNやラベル情報関係なく、取得。 ここで、「完全にランダムに選ぶ」ことを仮定してないことに注意。なので、$p(\\mathbf{x} | y = +1)$は$p(\\mathbf{x} | y + 1, o = +1)$と完全に一致するわけではない。これが違うとき、それこそが選択バイアスである。\n注 by me. $p(\\mathbf{x} | y = +1)$は、Positiveのデータの分布。$p(\\mathbf{x} | y = +1, o = +1)$は、Positiveのうち、ラベルがついてる時のデータの分布。これが異なるということは、まさにラベル付けされたPositiveと全体のPositiveの分布が違ってバイアスがあること。\n$\\pi = p(y = +1)$、つまりPositiveである確率はクラス事前確率というらしい。この$\\pi$は既知である前提でいく。定め方はそれなりに色々あるが、この問題設定では、クラス事前確率を理論的に保障しながら推定はできないらしい。\nでも、大丈夫。分からないとしてもアルゴリズムをマイナーチェンジをするだけで済む。\nつまり、「完全にランダムに選ぶ」という仮定から弱めた条件の下でちゃんと学習できるよ、と示してみた。\n特定戦略 # 2008年のPU Learningの初リリース時、仮定なしにPとUのデータからだけで$p(y = +1 | \\mathbf{x})$を得ることができなかった。なぜなら、真っ先に$p(o = 1 | \\mathbf{x}, y = 1) = p(o = 1 | y = 1) = 定数c$と、$\\mathbf{x}$関係なく一定確率でPositiveならラベル付けしてた、と仮定したからだ。よく読めばその後の$g(\\mathbf{x})$の推定は思いっきりこの仮定使ってる。\nCase-Controlシナリオでは、「完全にランダムに選ぶ」と仮定なら、$p(\\mathbf{x} | y = +1, o = 0) = p(\\mathbf{x}, y = +1, o = +1)$が成り立つ(上の段落の式の別表示ともいえるね)。この時、以下のように$p(y = + 1 | \\mathbf{x})$を推定できた。\n$$ p(y = + 1 | \\mathbf{x}) = \\frac{p(\\mathbf{x}, y = +1)}{p(\\mathbf{x})} = \\frac{p(\\mathbf{x} | y = +1) \\pi}{p(\\mathbf{x})} $$\n$$ = \\frac{ p(\\mathbf{x} | y = +1, o = +1) \\pi}{p(\\mathbf{x})} $$\nそして、ここの等式が、仮説$p(o = 1 | \\mathbf{x}, y = 1) = p(o = 1 | y = 1) = 定数c$、「完全にランダムに選ぶ」が使われて式変形してるところ。ここまで変形したかたちなら、入力から不変推定量で推定できる。\nというわけで、我々は$p(y = +1 | \\mathbf{x})$自体を推定する代わりに、$p(y = +1 | \\mathbf{x})$からいくつかの情報を抽出して分類器を学習させよう。これは Partial Identificationと呼ばれている。(Manski, 2008？)\nこの研究では、以下のような仮定に弱めてみる。\n任意の$\\mathbf{x} _i, \\mathbf{x} _j \\in \\chi$で、\n$$ p(y = +1 | \\mathbf{x} _i) \\leq p(y = +1 | \\mathbf{x} _j) \\Leftrightarrow p(o = +1 | \\mathbf{x} _i) \\leq p(o = +1 | \\mathbf{x} _j) $$\n元々は、両辺はイコールじゃないとあかんかった。この条件は、$\\mathbf{x} _i$が$\\mathbf{x} _j$よりもPositiveっぽくないなら、$\\mathbf{x} _i$の方が、$\\mathbf{x} _j$よりも、ラベル付けされるっぽくない。\nこれでは、リスク関数の不変推定量を経験的にデータから作れないが、それでもやってみたわ。\nこれ、普通の仮説じゃね？弱いかいうほど？こんな仮説でも許せばいいんだ？ by me\nPartial Identificationと分類の戦略 # 前述のとおり、選択バイアスがあるときは$\\pi$が与えられようが、$p(y = +1 | \\mathbf{x})$は推定できない。\n以下の定理1が成り立つ。\n$$ r(\\mathbf{x}) = \\frac{p(\\mathbf{x} | y = +1, o = +1)}{p(\\mathbf{x})} $$\n$$ p(y = +1 | \\mathbf{x} _i) \\leq p(y = +1 | \\mathbf{x} _j) \\Leftrightarrow r(\\mathbf{x} _i) \\leq r(\\mathbf{x} _j) $$\n定理1の証明 # 注: $p(a|b)$の時、$b$は全ての項についてるおまけみたいなもんだから、それ以外で見て判断していく。\n$$ p(o = +1 | \\mathbf{x}) = \\sum _{y \\in -1, 1} p(o = +1, y | \\mathbf{x}) $$\n周辺確率分布に展開する。\n$$ = p(o = +1 | \\mathbf{x}, y = +1)p(y = +1 | \\mathbf{x}) + p(o = +1 | \\mathbf{x}, y = -1)p(y = -1 | \\mathbf{x}) $$\n$p(o = +1 | \\mathbf{x}, y = -1) = 0$というのがPU Learningなので、式は以下のように簡約できる。この時、本来ならば上のように全パターンの可能性を網羅しないといけない。\n$$ p(o = +1 | \\mathbf{x}) = p(o = +1 | \\mathbf{x}, y = +1)p(y = +1 | \\mathbf{x}) $$\n次に$r(\\mathbf{x})$について\n2行目ではベイズの定理で変形を考えてみてる。 4行目では、 第1項 分子は$p(y = +1 | \\mathbf{x}) p(o = +1 | \\mathbf{x}, y = +1) = p(o = +1, y = +1 | \\mathbf{x})$　$\\mathbf{x}$を隠すと、条件付確率の定義の式そのもの。 分母と第3行目第1項の積は$p(o = +1 | \\mathbf{x}) p(y = +1 | \\mathbf{x}, o = +1) = p(y = +1, o = +1 | \\mathbf{x})$となる。　上の式と同じなので、確かに同値変形。 第2項と第3項では、それぞれ分子分母に$p(o = +1)$をかけている。 ここで、先ほどの$p(o = +1 | \\mathbf{x}) = p(o = +1 | \\mathbf{x}, y = +1)p(y = +1 | \\mathbf{x})$を使うと、第1項は1となり、第2項も$p(y = +1, o = +1)$となり、第3項も$p(\\mathbf{x}, o = +1)$となる。つまり、\n$$ r(\\mathbf{x}) = \\frac{p(\\mathbf{x}, o = +1)}{p(y = +1, o = +1)p(\\mathbf{x})} = \\frac{p(o = +1 | \\mathbf{x})}{p(y = +1, o = +1)} $$\nここで、$C=p(y = +1, o = +1)$とすると($\\mathbf{x}$について、これは変数ではないので)まさに$r(\\mathbf{x}) = C p(\\mathbf{x}, o = +1)$　これはまさに仮定\n$$ p(y = +1 | \\mathbf{x} _i) \\leq p(y = +1 | \\mathbf{x} _j) \\Leftrightarrow p(o = +1 | \\mathbf{x} _i) \\leq p(o = +1 | \\mathbf{x} _j) $$\nそのもの。\n本筋 # $$ r(\\mathbf{x}) = \\frac{p(\\mathbf{x} | y = +1, o = +1)}{p(\\mathbf{x})} $$\n$$ p(y = +1 | \\mathbf{x} _i) \\leq p(y = +1 | \\mathbf{x} _j) \\Leftrightarrow r(\\mathbf{x} _i) \\leq r(\\mathbf{x} _j) $$\nともあれ、以上が成り立つ。\nこのことは、$p(y = +1 | \\mathbf{x} _i)$を我々が推測できずとも、右辺の$r(\\mathbf{x})$で順序が正しいので、右辺の推測ができれば、左辺もある程度は不等式で挟み込める、ということ。\nそして、ここで$r$は経験的にデータから得られるので、$\\theta \\in \\mathbb{R}$として、\n$$ h(x) = sign(r(\\mathbf{x}) - \\theta) $$\nこの$h(\\mathbf{x})$こそが、識別器そのもの。必要十分になってる不等式は、$p(y = +1 | \\mathbf{x})$について、つまり我々が一番欲しかったもの。\n$\\theta$の決め方として、以下の$\\theta _{\\pi}$がおすすめ。\n$$ \\pi = p(y = +1) = \\int p(\\mathbf{x}) d \\mathbf{x} 積分する区間ではr(\\mathbf{x}) \\geq \\theta _{\\pi} $$\nちょうどすべての条件を満たす$r(\\mathbf{x})$についての期待値が、全体でのPositiveの割合になるように、上手く$\\theta _{\\pi}$にするのが理想的。\nprecisionとrecall # ここの記事で解説がある。\nprecisionは$\\frac{TP}{TP + FP}$　Positiveと言ったもののうち、本当にPositiveの割合。 recallは$\\frac{TP}{TP + FN}$　結果的にPositiveの量に対して、どれだけ正し買ったかの割合。 我々は**$h(\\mathbf{x}) = r(\\mathbf{x}) - \\theta$の$\\theta = \\theta _{\\pi}$とすれば、precision=recallとなる**、と証明できる。証明略。\n提案したアルゴリズム # ここまでの話をまとめると、$p(y = +1 | \\mathbf{x})$を予測するのは、選択バイアスがあると仮定するなら無理だが、代わりに不等式で同様に比較できる$r(\\mathbf{x}) = \\frac{p(\\mathbf{x} | y = +1, o = +1)}{p(\\mathbf{x})}$を、不変推定量からいい感じに得たい。流れとしては以下の感じ。\n入力 Positiveのデータセット$[\\mathbf{x} _i] _{i = 1} ^ {n}$ Negativeのデータセット$[\\mathbf{x} ^{\\prime}_i] _{i = 1} ^ {n^{\\prime}}$ Negativeのデータセット$[\\mathbf{x} ^{test}_i] _{i = 1} ^ {n^{test}}$ Positiveの割合$p(y = +1) = \\pi$　これは小規模なデータセットからの平均を不変推定量として推定できる。 $[\\mathbf{x} _i] _{i = 1} ^ {n}$と$[\\mathbf{x} ^{\\prime}_i] _{i = 1} ^ {n^{\\prime}}$を使って、分類器$f(\\mathbf{x})$を学習して、$r(\\mathbf{x})$を作る。最も、実際は経験的に不変推定量から$\\hat{r}(\\mathbf{x})$を作るけど。 $\\hat{r}(\\mathbf{x})$から、$\\theta_{\\pi}$を作る。最も以下略で$\\hat{\\theta_{\\pi}}$だが。 $\\hat{r}(\\mathbf{x}), \\hat{\\theta_{\\pi}}$から、$h(\\mathbf{x})$を作り、これを識別器にする。 以下では、この各ステップについてどのように推定するのかを書く。\n選択バイアスがないと仮定したリスク関数の最小化と # 選択バイアスがあると考えてるわけだが、この場合はリスク関数を上手く表現できないとのこと。仕方がないので、選択バイアスがないパターンのリスク関数を表す。これは 2015のdu Plessisら\nなお、ここで$\\mathbf{x}$ではなく$X$なのは、複数のベクトルをまとめて扱った和として考えていいだろう。\n$$ R(f) = \\pi \\mathbb{E} _{p(\\mathbf{x} | y = +1)} [l(f(X), y = +1) - l(f(X), y = -1)] + \\mathbb{E} _{p(\\mathbf{x})} [ l(f(X), y = -1) ] $$\nここで、選択バイアスが働いてるので、$p(\\mathbf{x} | y = +1)$について期待値を求めてる上式は使えない。我々は$p(\\mathbf{x} | y = +1, o = +1)$しか知らないからだ。だが、ここでも、仕方ないとして知ってる要素入れ替えたものを、ここでつかおう。\n$$ R(f)^{bias} = \\pi \\mathbb{E} _{p(\\mathbf{x} | y = +1, o = +1)} [l(f(X), y = +1) - l(f(X), y = -1)] + \\mathbb{E} _{p(\\mathbf{x})} [ l(f(X), y = -1) ] $$\nここで、損失関数としてはこの論文では\n$$ l(m, y = +1) = -\\log f(\\mathbf{x}), l(m, y = -1) = -\\log f(1 - f(\\mathbf{x})) $$\nとしている。これを代入すると、以下のかたちに。これを使ってるみたい。\n$$ R(f)^{bias} = \\pi \\mathbb{E} _{p(\\mathbf{x} | y = +1, o = +1)} [-\\log f(X) + \\log (1 - f(X))] + \\mathbb{E} _{p(\\mathbf{x})} [ \\log (1 - f(X)) ] $$\n性能評価のところは一旦割愛や\u0026hellip;\n実際のところ、最小化を考えるにあたり、以下のように正則化項$R(f)$をつけてる。\ndu Plessisらはこの式を経験的にやっても不偏推定量なので真の分類器の損失関数へと近づくと2015年示した(まだ読んでない)。\nだが 2017のKiryoらが示した通り、Deep Neural Networkでは上の式はあまり使えないとのこと。なぜなら、DNNのような表現能力が高すぎるネットワークでは過学習をするらしい。マイナスになる項を際限なくマイナスにしちゃうらしい。具体的には、$\\log(1 - f(X))$で、$f(X)$を$1 - \\epsilon$にいくらでも近づけることになり、マイナスへ無限に行っちゃうのだ。\nだから、以下のように$\\log(1 - f(X))$の部分で$\\max(0, \\cdot)$という操作を加えて常に正にする正規化？をしている。\nそして、$r()$\n$r(\\mathbf{x})$の推定 # $$ r(\\mathbf{x}) = \\frac{p(\\mathbf{x} | y = +1, o = +1)}{p(\\mathbf{x})} $$\nこれを得たい。これに関しては、2012年Sugiyamaらが方法をまとめている。最小二乗法を使ったLeast-squares importance fittingを使おう。数学的にも解析しやすいからね。\nまず、$s : \\chi \\to \\mathbb{R} ^{+}$を考え、$s \\in S$とする。この$s$を上手いこと選んで、$r(\\mathbf{x})$に近づけるようにする。\n$$ R_{DR}(s) = \\mathbb{E} _{p(\\mathbf{x})} [ (s(X) - r(X))^2 ] $$\n$$ = \\mathbb{E} _{p(\\mathbf{x})} [ s(X)^2 + r(X)^2 ] - 2 \\mathbb{E} _{p(\\mathbf{x})} [ s(X)r(X) ] $$\nここで、2つ目の項は\n$$ \\int s(X)r(X) p(\\mathbf{x}) d\\mathbf{x} = \\int s(X) \\frac{p(\\mathbf{x} | y = +1, o = +1)}{p(\\mathbf{x})}p(\\mathbf{x}) d\\mathbf{x} $$\n$$ = \\int s(X) p(\\mathbf{x} | y = +1, o = +1) d\\mathbf{x} = \\mathbb{E} _{p(\\mathbf{x} | y = +1, o = +1)} [s(X)] $$\nこれを使うと、\n$$ R_{DR}(s) = \\mathbb{E} _{p(\\mathbf{x})} [ s(X)^2 + r(X)^2 ] - 2 \\mathbb{E} _{p(\\mathbf{x} | y = +1, o = +1)} [s(X)] $$\nこの式を最小化する。この時、$\\mathbb{E} _{p(\\mathbf{x})} [ r(X)^2 ]$は定数とみなせるので、考えなくていい。実際に何かは知らないが、$s(X)$決める上ではどうしようもないからな。つまり、\n$$ \\hat{r}(\\mathbf{x}) = \\min _{s \\in S} \\frac{1}{2} \\mathbb{E} _{p(\\mathbf{x})} [ s(X)^2] - \\mathbb{E} _{p(\\mathbf{x} | y = +1, o = +1)} [s(X)] $$\n$\\theta _{\\pi}$の推測 # $$ \\pi = p(y = +1) = \\int p(\\mathbf{x}) d \\mathbf{x} 積分する区間ではr(\\mathbf{x}) \\geq \\theta _{\\pi} $$\n上式が、$\\theta _{\\pi}$の定義だった。$p(\\mathbf{x})$に従うテストデータの$[\\mathbf{x} ^{test}_i] _{i = 1} ^ {n^{test}}$を使って、経験的に推定すると、以下の式を満たす$\\theta _{\\pi}$が得られる。\n$$ n^{test} \\pi = \\sum _{i = 1}^{n^{test}} 1(\\mathrm{if} \\hat{r}(\\mathbf{x} _i) \\geq \\hat{\\theta _{\\pi}}) $$\nこれの決定自体は上の式に従えばいいが、明らかに単調性があるので実際は二分探索をすればいいね。\n実験 # 実験では、SVMで$s(\\mathbf{x})$もとい$\\hat{r}(\\mathbf{x})$を作った。つまり、\n$$ s(\\mathbf{x}) = \\mathbf{w} ^ T \\mathbf{\\Phi}(\\mathbf{x}) $$\nこのように識別器を定義できた。$\\Phi$はいつも通り、基底の変換やね。カーネル関数はガウシアンカーネルを使った。つまり、\n$$ \\mathbf{\\Phi} _{l} (\\mathbf{x}) = \\exp (- || \\mathbf{x} - \\mathbf{c}_l ||^2 / 2 \\sigma^2) $$\nただし、$\\mathbf{c}_l$は、$\\mathbf{x}_i, \\mathbf{x ^{\\prime}} _i$のすべて。\nそして、識別関数の$f(\\mathbf{x})$自体は以下のようにシグモイド関数で定めてみた。(これそもそも$s(\\mathbf{x})$がわかってるのに、わざわざこれを求める意味ある？)\nおわり\n"},{"id":12,"href":"/docs/article/Weakly-Supervised-Learning/PU-Learning/","title":"PU Learning(2007年)","section":"Weakly Supervised Learning","content":" 元論文\n参考にしたサイトたち\nhttps://mamo3gr.hatenablog.com/entry/2020/11/29/123147\nhttps://speakerdeck.com/hellorusk/pu-positive-unlabeled-learning?slide=3\n何なの？ # ラベルはいっぱいあるけどつけるの間に合わん。普通はPositiveとNegativeにつけられたデータで行うが、PU LearningはPositiveとUnknownで区分したデータで学習させる。\n生成モデルに基づく半教師付きの手法と違って、分布を仮定する必要はない。\n手法 # 仮定、説明 # サンプル$x \\in \\mathbb{R}$について、\n正答$y \\in 0, 1$があり、1ならPositive、0ならNegativeである。 ラベル$s \\in 0, 1$があり、1ならラベルあり、0ならラベルなし そして、未知だが$(x, y, s)$に対する不変の分布があるとする。\n仮定として、Positiveなものしかラベル付けされてない、しかも一部。つまりNegativeはUnknownである。$p(s = 1| x, y = 0) = 0$\nもう1つ仮定として、ラベル付けされるPositiveは、Positiveの全体からランダムに選ばれると仮定する。$p(s = 1 | x, y = 1) = p(s = 1 | y = 1) = 一定値c$。$x$に関係なく、取るという意味。\n理論的なおはなし # 条件付確率の分解をやる。「xであるときにy=1」$p(y = 1 | x)$と「xであって、y=1であるときにs=1」$p(s = | x, y = 1)$の積。依存関係はあるが関係はない。\n$$ p(s = 1 | x) = p((y = 1 \\And s = 1) | x) = p(y = 1 | x) p (s = 1 | x, y = 1) = p(y = 1 | x) \\cdot c $$\nつまり、\n$$ p(y = 1 | x) = \\frac{p(s = 1 | x)}{c} $$\nつまり、データに対してPositiveである確率は、\n$p(s = 1 | x)$　データ$x$に対して、それがラベル付きかどうか。これは通常の分類器で推定できるので問題なし。 全部のデータと実際のラベル付きデータはあるんで、その分布を仮定する手法ならなんでも。深層学習でもいい。 $c = p(s = 1 | y = 1)$　Positiveなラベルなら、印がつく確率。 がわかる or 推定できるなら、一番欲しい$p(y = 1 | x)$推定できる。\n定数$c$の推定方法 # というわけで、$c = p(s = 1 | y = 1)$を推定することができれば、勝ちです。\n手法1 # 前述のとおり、$p(s = 1 | x)$「データ$x$に対して、それがラベル付きかどうか」。これは学習器で学習できる。その学習器に入れた時の結果を$g(x)$と置く。\n$c = p(s = 1 | y = 1)$Positiveなラベルなら、印がつく確率。\nここで、訓練データと同じように抽出したテストデータ集合$V$を考える。その中で、ラベルの付いてる集合を$P$とする。\n$$ c = \\frac{1}{n} \\sum \\limits_{x \\in P} g(x) $$\nつまり、$V$のなかで実際にラベル付きのデータに対して、分類器でラベル付き=1か否か=0を**、$|P|=n$で割った平均。$|V|$ではない！。これは実際にラベル付きのデータに対してどれぐらい正確に予測できてるか、ということを示す。\nそして、分類器が正しく訓練データについて$p(s = 1 | x)$を学習できてるなら、これはテストデータ(そしては訓練データ以外の全体のデータ)に対して、真のPositiveのデータのうち、ラベル付けされてるサンプルの割合になる。\nつまり、ちゃんと訓練データに対して学習をさせたから(もちろんその訓練データの中でまた訓練とテストに分けるけど)、同じ分布(未知だけど)に従ってPositiveのものにラベルがつくと仮定してる以上、すでにラベルついてるデータに対して予測器でラベルつく割合を見つけられれば、それは全体のPositiveの者に対して、ラベル付きであるの割合だとなる。\n実際はだいたいそうならない。ちゃんと同じ分布に従うかな？でもこの手法はそれなりに正しいんですよこれ。\n手法2 # 先ほどは、$p(y = 1 | x) = \\frac{p(s = 1 | x)}{c}$で$p(y = 1 | x)$を求めた。\n今度は、まず$p(s = 1 | x)$を学習してみる。次に\nラベル付きはそのまま。 ラベルなしは、ラベルあり=重み$w(x)$とラベルなし=重み$1 - w(x)$という2つの点に複製する。そして、もう一度、$p(s = 1 | x)$を重みつきで学習する。 なお、重みは、$w(x) = p(y = 1 | x, s = 0)$とする。\nこんな風に、一度方法1から予測した$c$から計算できる。\nそして、これらの重みをそれぞれつけなおしたものから、もう一度$p(s = 1 | x)$を学習させるのだ。\n実際の学習のフェーズ # 実際、識別器はSVMのソフトマージンで実装される。だが、SVMのソフトマージンでの定式化での損失関数はヒンジ損失$z$を用いると、\n$$ \\frac{1}{2} || \\mathbf{w} || + C_p \\sum _{i \\in P} z_i + C_U \\sum _{j \\in U} z_j $$\nこの$C_P, C_U$は経験則で決めるしかないらしい。この手法をこの論文がbiased-SVMと提案している。\n$C_U = 0.01, 0.03, 0.05, \\cdots, 0.61$ $\\frac{C_P}{C_U} = 10, 20, \\cdots, 200$ $C_P$に大きく重みを寄せる。 評価 # 手法2の方が実験的に精度が良い。\nどっちも、(論文は2だけ離散の分類器は無理らしいけど、全部離散は無理では？？？)分類器が離散だとNGらしい。\nまた、理論的に正しく境界を決めるには、誤差関数をnon-convex loss(SVMのhinge lossはNG)にしないとならないという難点がある。\n"},{"id":13,"href":"/docs/article/Weakly-Supervised-Learning/%E5%88%86%E5%B8%83%E4%BB%AE%E5%AE%9A%E4%B8%8D%E8%A6%81PU-Learning2014/","title":"PU LearningでなんでSVMのヒンジ関数は精度悪いか(2014)","section":"Weakly Supervised Learning","content":" 元論文\n参考したスライド\n参考にした記事\n親戚論文の中国語解説　とっても良い\nこれも東大杉山研じゃないか、たまげたな。\n前にやったPU Learningは、強い分布に対しての仮定が必要だった。つまり、PositiveなDataのうちから一様に抽出して、ラベル付きになっているという強い分布への仮定が必要だった。今回のやつはそこへのカウンターも兼ねた理論的な証明。\n問題設定 # Positive(+1)とNegative(-1)の誤分類を最小化する関数は以下のように書ける。これは、誤分類の割合を表す関数。\nまず、あるデータ$X$について、ラベル付けする識別器$f(X) \\in 1, -1$があるとする。また、\n$R_{-1}(f) = P_{-1} (f(X) \\neq -1)$　本来Negative=-1なのに、識別器にPositive=1に分類されてしまう確率。 $R_{1}(f) = P_{1} (f(X) \\neq 1)$　本来Positive=1なのに、識別器にNegative=-1に分類されてしまう確率 $\\pi$は全sampleのうちのpositiveの割合であり、$\\frac{n_{pos}}{n_{pos} + n_{neg}}$で推定する。 $$ R(f) = \\pi R_1(f) + (1-\\pi) R_{-1}(f) $$\nまた、ここから更にcost-sensitive=重み付き分類は、上記の式にcostの$c_1, c_{-1}$をつけたもの。これは以下のもの\n$$ R(f) = \\pi c_1 R_1(f) + (1-\\pi) c_{-1} R_{-1}(f) $$\nPU分類 # 上の式はPositive=1とNegative=-1だったが、PU分類ではUnknownにNegativeが全部入ってるし、Positiveも一部ある。$\\pi$は前述のとおり、全sampleでのPositiveな割合。(Positiveなラベルしかつけないというけどさすがにsampleを見る限りNegativeが何個あったかも把握はしておくので、πが求まる。これがラベルなしのデータでも同じ割合であると推定)この時、↓の式のように、ラベル付けされてないものの確率を求めることができる。\n$$ P_X = \\pi P_1 + (1-\\pi) P_{-1} $$\n次に、$R_X(f)$=分類器f(X)が、$P_X$に対してその中でPositiveの確率として、求めてみる。つまり$\\pi$では？と思うが、$\\pi$は全体のPositiveな真の割合であり、$R_X(f)$は推定してるといえる。 これ、ある訓練データから一部をPositiveとしてラベル付けしておくかたちだが、多分ラベルなしは、訓練データでのNegativeを除く。排除しないと、割合が$\\pi$で推定できないから(Negativeが予想以上に混入するので)。\n$$ R_X (f) = P_X (f(X) = 1) = \\pi P_1(f(X) = 1) + (1 - \\pi) P_{-1} (f(X) = 1) $$\n$$ = \\pi(1 - R_1(f)) + (1 - \\pi) R_{-1}(f) $$\nとなる。この$R_X$、ラベルなしのクラス$P_X$のうち、識別器にpositiveと認識される確率である(再掲)だが、これを使って$R(f)$も表してみる。なんせPU分類には$R_{-1}$は分からないから、できるだけ消したい。重みは一旦なしで見る。\n$$ R(f) = \\pi R_1(f) + (1-\\pi) R_{-1}(f) $$\n$$ = \\pi R_1(f) + R_X(f) - \\pi(1 - R_1(f))　= 2\\pi R_1(f) + R_X(f) - \\pi $$\nそして、$\\mu$を$P_x$に対して$P_1$が占める割合とする。今までは$\\pi$で、Negativeも入れた中でのSampleのPositiveの割合だった。しかし、$\\mu$は、ラベル付き=Positiveとラベルなし(Negativeの割合は使わない！)ということ。\nこの$\\mu$という量を使って、$R(f)$を無理やり式変形してみる。\n$$ R(f) = \\frac{2 \\pi}{\\mu} \\cdot \\mu R_1(f) + \\frac{1}{1 - \\mu} \\cdot (1 - \\mu) R_X(f) - \\pi $$\nこのように、はじめにいった重み付きのPN分類に帰着できるとわかるね。\n損失関数について # ヒンジ損失関数を考える。\n$$ \\max (0, 1 - y_i(\\mathbf{w}^T \\mathbf{x}_i + \\mathbf{\\theta})) $$\nランプ、ReLUの損失関数を考える。\n$$ \\max (0, \\min(2, 1 - y_i(\\mathbf{w}^T \\mathbf{x}_i + \\mathbf{\\theta}))) $$\nヒンジ関数ではどうなるか。 # $$ R(f) = 2\\pi R_1(f) + R_X(f) - \\pi $$\nこれに、ヒンジ関数を適用する。\n$$ R(f) = 2 \\pi P_{1} (f(X) \\neq 1) + $$\n続く\n結果論、ヒンジ関数はダメだけど、ReLUがいいって。\nPU分類の誤差 in 同一分布じゃない # 注意：他の論文を読んでたら、ここでは前提として「Positiveでラベル付けされてるものとされてないものは同じ分布に従う」とあるらしい。じゃあこれはなんだ\u0026hellip;?\nPU分類は、同一分布に従う必要がある。つまり、ラベル付けでPositiveになってるのは、Positive全体のデータからランダムに抽出させないとアカン。 では、同一分布に従ってない時(=ランダムに抽出してない？)はどうする？その時の誤差はどうなるのか、を解析したのがこの論文。\n関数$f(\\mathbf{x})$を次のように定める。\n$$ f(\\mathbf{x}) = \\sum _{i = 1}^{n} \\alpha_i k(\\mathbf{x}_i, \\mathbf{x}) + \\sum _{j = 1}^{n ^\\prime} \\alpha_j ^\\prime k(\\mathbf{x}_j^\\prime, \\mathbf{x}) $$\n$\\alpha_1, \\cdots, \\alpha_n, \\alpha_1^\\prime, \\cdots, \\alpha_{n^\\prime}^\\prime$は実数。 $\\mathbf{x}_1, \\cdots, \\mathbf{x}_n$は、$p(\\mathbf{x} | y = +1)$。Positiveの分布に従い、ランダムに$\\mathbf{x}_i$を$n$個抽出した。 $\\mathbf{x}_1^\\prime, \\cdots, \\mathbf{x} _{n^{ \\prime }} ^ \\prime$は、$p(\\mathbf{x})$。つまり、これはPositive、Negative関係なくランダムに$\\mathbf{x}_i^\\prime$を$n^\\prime$個抽出した。 つまり、$n$個のPositiveと、$n^\\prime$個のラベルなしがあるとして、それらと引数$\\mathbf{x}$のカーネル内積の一定係数の線形結合。この関数での誤差を考えてみる。\nこれをごにょごにょすると(中略\u0026hellip;)\n一定分布に従わないという最悪な状況でも、誤差は\n$$ O(\\frac{1}{\\sqrt{n}} + \\frac{1}{\\sqrt{n^\\prime}}) $$\nのオーダーになる。これは$n$個が独立同分布に従って得られ、$n^\\prime$個がまた別の独立同分布に従ってる場合に最適である。\nもし、いずれも同じ独立同分布に従ってるなら、\n$$ O(\\frac{1}{\\sqrt{n + n^\\prime}}) $$\nだが、これは非現実的。同じ独立同分布に従ってるなら、PU Learningする意味ないので。\nただ、これを見る限り、どれほど分布が違っていようが、完全に一致の分布から$n$と$n^\\prime$を取ってるのと比べて、たかだか$2 \\sqrt{2}$倍までしか悪くならない！\n$$ \\frac{1}{\\sqrt{n}} + \\frac{1}{\\sqrt{n ^ \\prime}} = = \\frac{\\sqrt{n} + \\sqrt{n ^ \\prime}}{\\sqrt{n n^\\prime}} $$\n"},{"id":14,"href":"/docs/article/sns/%E7%82%8E%E4%B8%8A%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6/","title":"Twitterの炎上について","section":"sns","content":"Twitterの炎上についての定義を各論文から集めてみた。 多分画一なものはないから、まずはそれらを参考に炎上の定義を考えたほうがいいな。\n感情分析＋「お前」「不愉快」などの悪意ワード # 「ユーザーに着目したSNS上の攻撃とそのメカニズムに関する分析」 # タイトル 著者 ユーザーに着目したSNS上の攻撃とそのメカニズムに関する分析 Tomoka Segawa, Kimitaka Asatani, Ichiro Sakata 論文の要約 # SNSでの攻撃は趣味が近いユーザー同士と、遠いユーザー同士の2つがある。 近いユーザー同士は、一部の趣味で起こる場合が多い 全コミュニティのわずか1%で、74%の争いが起きてる。 Ribeiroら、Characterizing and detecting hateful users on twitter　より 遠いユーザー同士は、政治思想が強い、普段からネガティブな発言をすると、普段関係なくても叩きに行く。 たたきに加わる人たちは、普段会話などをしてる間柄。 攻撃的ツイートは、GoogleのBERTで自然言語処理する。 ひっかけるキーワードは「お前」、「不愉快」など Semantic Textual Similarityを求める。手動でラベル付けした攻撃的ツイートを、cos類似度が大きくなるようにBERTのfine tuningできるらしい。 ユーザのフォロー関係のグラフは、Louvain法を使ってクラスタ分けできる。 そうやって得た各コミュニティでの攻撃件数、被攻撃件数etcを集める。 感情の判断は、ML-Askを使ったみたい。 かなり時代進化してきたしここは置き換えられそうね。 結果は、Accuracyは高くないが、数件以上の攻撃的投稿する人に関しては高い。 同一コミュニティ内の攻撃は、必ずしも普段からネガティブな投稿をする人だらけで攻撃してるわけじゃない。 ロジハラみたいな感じ\u0026hellip;？ 遠いコミュニティは露出が多いと叩かれる。同一コミュニティはそうでもなかったり。 攻撃者だけを抽出すると、明らかにそれらだけでつるんでいた。 この攻撃性を強化する研究とか、見ていいよね。(エコーチェンバー) 例えば、宮迫闇営業の時は、2つに分類した攻撃的発信の混合だとわかるね。 オンラインチャットにおける誹謗中傷コメントの発信を未然に防止する機能 # タイトル 著者 オンラインチャットにおける誹謗中傷コメントの発信を未然に防止する機能 Masaki Ito, Wataru Sunayama 悪意ワードは事前に埋め込む。各ワードごとに、重大さ、というスコアを固定する。\nうーん、これ埋め込みでさあ、ある程度学習されることで自動で振れない\u0026hellip;? By me.\ndamage 3: 相手の存在を否定する単語、関係を軽視する単語\r死ね　消えろ　邪魔　殺すetc\rdamage 2: 相手の人格や容姿を攻撃する単語\rハゲ　ブス　デブ　意地汚いetc\rdamage 1:　相手を貶める単語\r馬鹿　ドジ　雑魚 論文の要約 # 自動で悪口かどうかの判別はできるが、文脈を想定していくのはできない。1回では傷つかないけどやりとりしていくと傷つく場合はある。 (参考)SNSで悪口を含む投稿に取り下げを促す　藤堂悠杜ら、SNS 上の悪口を含む投稿に対する取り下げを促すフィードバック文の自動生成方法の検討 各ワードに重みをつける。各人にHPという概念をつくる。 各発信のうち、ヤバワードの重みの和を攻撃力とする。 発信のうちの相手からのダメージを軽減するワードもあり、それを防御力とする。攻撃力を相殺する感じ。 相手のHPが一定以下になるような発言をしようとした瞬間に、システム介入する感じ。 発信するメッセージに以下の種類のものがある。 共感系　例:今送ろうとしているメッセージに問題がないか、もう一度見直してみてはどうでしょうか？　HP高めの時に デメリット提示系　相手を傷つけるメッセージを送ると、お互いの関係が壊れる可能性がありますよ。　HP低めの時 禁止系　今後のメッセ――ジの送信ができなくなります。　HP瀕死の時 一言で言うと遊戯王、ポケモン　なんだお前(驚愕) 結果として、共感系のメッセージを出すのが一番有効みたい。 人間関係と誹謗中傷検出によるオンラインハラスメント対策 # タイトル 著者 人間関係と誹謗中傷検出によるオンラインハラスメント対策 Tiesong Shang, Juan Zhou, Hideyuki Takada 誹謗中傷に当たる言葉を特定するのは難しい。なぜならtypo、スラングなど文が崩れているから。　←ほんと？？？2023年ならできそう。\n研究では、bag-of-opinionモデルというものを使った。要約参照して。\n論文の要約 # メッセージの内容で誹謗中傷であると判断し、加害者と被害者を見つける。 その加害者と被害者中心に、ソーシャルグラフを作る。 そこで受けてる誹謗中傷と嫌がらせのメッセージが、全体のメッセージに締める総数を計算して、それを重症度とする。 これ、1度誹謗中傷受けただけでも辛くないか\u0026hellip;\u0026hellip;？ Baidu Tiebaでやってる。中国の2ch(検閲ゴリゴリついてる) 事前に誹謗中傷を表す単語やパターンを定義して、bag-of-opinionモデルというのを使う。 (参考文献)　Lizhen Quら、 The Bag-of-Opinions Method for Reviewing Rating Prediction from Sparse Text Patterns 複数語に渡ると予想されるフレーズに点数をつける場合、N-gramにするしかないが、学習に必要なデータは多すぎる。 以下の3つの要素によってOpinionが成る。 root word 同じ文で使われた修飾語 1つ以上の否定語 コーパスからリッジ回帰(重回帰分析+L2正則化)で、Opinionごとにスコアを振る感じ。 誹謗中傷する人の多くは、誹謗中傷を普段からはしない人。逆にわずか1%の人が10%もの誹謗中傷をしている。という結果も。 E. Wulczyn, Ex machina: Personal attacks seen at scale 結果はなんか、書いてないんだが\u0026hellip;\u0026hellip; BERTを用いたSNS上における攻撃的文章訂正システム # タイトル 著者 BERTを用いたSNS上における攻撃的文章訂正システム 吉田 基信、松本 和幸、吉田 稔、北 研二 論文の要約 # 先行研究では、SVMで炎上するorしないを判断して、炎上すると判断した単語を、あらかじめ学習させたword to vecで訂正する。だが、日本語として意味のない文を出しちゃうという問題が。 大西真輝ら、 ツイート炎上抑制のための包括的システムの構築 提案したシステムは以下の4歩からなる。 入力テキストの前処理 TweetLというライブラリで正規化を行う。(半角全角の統一　ハッシュタグやメンションの除去など) 攻撃的文章であるかどうかの判定。(学習済のものをfine tuningしておく。データを1600件学習させ400件をテストに使う。スパムメッセージは除外。ラベルとして、安全、攻撃的、スパムの3つをアノテーションする) 攻撃的ならば、単語置き換え　BERTのTransformerの第12層のAttentionの重みが一定以上の場合、(あらかじめ学習させた)置き換えする。 また、事前に登録した危険単語の場合、問答無用でBERTで類義語に置き換えする。 元の文章との類似度表示　埋め込みベクトル特有の内積を利用したcos類似度を使う。 実験結果として、安全攻撃的スパムの分類精度自体は76%から63%と高くない。 単語置き換えでも、全く意味が違う文に変換されてしまった。 「マーチはＦラン、低学歴だわ」→「マーチは当然、不可能だわ」 精度が足りなかったという結論。理由は fine-tuningで使われた1600個の内容が限定的で数が少ない。 変換する用語の品詞を考えてない そもそもBERTは優等生なので、誹謗中傷事態を学習してない。 似てないとされてる文は短文が多い。長文だと前後の文脈から推測できるってやつか？ "},{"id":15,"href":"/docs/article/SVM/%E3%82%AB%E3%83%BC%E3%83%8D%E3%83%AB%E6%B3%95/","title":"カーネル法","section":"Svm","content":"こっちの サイトがわかりやすい。\nこっちもいい。\nデータを非線形変換して、特徴空間に写像して、そのうえでSVMなどで分類する。この手法がカーネル法であり、非線形変換する関数をカーネル関数と呼ぶ\n例えば、$(X, Y, Z)$を、$(X, Y, Z, X^2, Y^2, Z^2, XY, YZ, ZX, \\cdots)$のように拡張する。このベキ展開は実際にも使われてる手法。こんな風にデータから新しいラベルを作って、それが有効に説明してるように頑張るのを、特徴量エンジニアリングという。\nただし、明らかに大変ですよね。次元が明らかに爆発するもんね。\nカーネルによる内積の計算 # データ空間の$\\Omega$から、特徴空間$H$まで移す特徴写像の$\\Phi : \\Omega \\to H$を考える。この$\\Phi$は非線形な写像となる。\n$H$の上では線形であるとやりやすいので、ベクトルとみなした時の内積が定義できると嬉しい。ただ、このままだと、バカデカい次元数になったときに、内積の計算も一苦労ですよね。\nこれを、うまいこと$H$での内積を計算することなく、データ空間$\\Omega$での$\\mathbf{X}_i, \\mathbf{X}_j$について、何かしらの関数=カーネル関数で計算したするのを代用する、というのがカーネルトリック。再生核ヒルベルト空間でさえあればね。\n記号としては$k(\\mathbf{X}_i, \\mathbf{X}_j)$を、特徴空間$H$の上の内積$\u0026lt;\\Phi(\\mathbf{x}_i), \\Phi(\\mathbf{x}_j)\u0026gt;$の代用とする感じ。\n正定値カーネル # カーネル関数に対して以下が成り立つなら正定値カーネル。\n$k(x, y) = k(y, x)$ $\\forall c_i, c_j \\in \\mathrm{R}, \\forall x_i, x_j \\in \\chi \\sum _{i = 1, j = 1}^{n} c_i c_j k(x_i, x_j) \\geq 0$ 2つ目の条件は、(i, j)成分を$k(x_i, x_j)$として、グラム行列となる。グラム行列なので、$A^{*} A$で書けたりもするし、当然半正定値である。理論元がエルミート行列なので、複素世界に行ったら$c_j$は共役となることをお忘れなく。\n正定値カーネルはこういう性質を持つ。\n$\\forall x, k(x, x) \\geq 0$ $|k(x, y)|^2 \\leq k(x, x) k(y, y)$ $\\chi$の部分集合の$\\chi_s$として、そこに制限しても正定値のカーネルとなる。 正定値カーネルは次のように拡張することもできる。\n非負の定数関数は明らかにカーネル関数。 $f$は任意の関数、$k$はカーネル関数。この時、$\\hat{k}(x, y) = f(x) k(x, y) \\bar{f(y)}$もカーネル関数。 これ自体、正定値の式に簡単に変形できるので。つまりこれも簡単にグラム行列に。 そして、カーネルトリックを保証する最大の強さは、\n$x \\to \\Phi(x)$なら、$\\Phi(x) ^ T \\Phi(y) = k(x, y)$ならば、正定値のカーネルである。つまり、何かしらの写像$\\Phi(x)$に対して必ず該当のカーネル関数は存在する。\n再生核ヒルベルト空間 # ここでも見てな。\n普通のヒルベルト空間は以下の条件を満たす。 こちらがわかりやすい。\n三角不等式と線形性が成り立つ 完備性がある(コーシー列が常に収束)　上のことでもあるけど 内積が存在している 例えばユークリッド空間がいい例 どうやら$H$はこれを満たすものらしい。$\\forall x \\in \\chi$で、$k_x \\in H$が存在し、\n$$ \\forall f \\in H, \u0026lt;f, k(\\cdot, x)\u0026gt;_H = f(x) $$\nこれを再生性という。再生核ヒルベルト空間上の内積は$\u0026lt;\\cdot, \\cdot\u0026gt;_H$。\nこの中で、$k(y, x) = k_x (y)$となるカーネル$k$を再生核という。\n正直ようわからんが、任意の関数に対してカーネル関数との内積を取ったら、それが$f(x)$っぽいってことかね。\n再生核ヒルベルト空間の再生核$k$は、必ず正定値カーネルであり、ある再生核ヒルベルト空間に対して一意に決まる。\nまあ色々あるがいったん飛ばそう。\nカーネル法の実際 # 主成分分析を考える。普通に主成分分析するのと、ガウシアンカーネルに変換した後でやる。\nガウシアンカーネルの場合、$\\sigma$が小さいと、細長い結果に。大きいと、線形の結果に近づく。\nちなみに、主成分以外はノイズとみなすことで、主成分分析はノイズ除去にも使えたりする。\n"},{"id":16,"href":"/docs/article/SVM/%E3%82%BD%E3%83%95%E3%83%88%E3%83%9E%E3%83%BC%E3%82%B8%E3%83%B3/","title":"ソフトマージン","section":"Svm","content":" こっちのQiitaはわかりやすい。\nこっちのブログも補助として見た。\n誤差項はやっぱΣらしい。\nソフトマージン # $$ \\min_{\\mathbf{w, \\theta}} || \\mathbf{w} || $$\n$$ 条件：y_i(\\mathbf{w}^T \\mathbf{x_i} + \\mathbf{\\theta}) \\geq 0 $$\nハードマージンは、完全に分割できる超平面じゃないとアカン、というところだった。しかし、ソフトマージンでは、教師データに誤分類が入ることに対してもOKとするけど、代わりにペナルティを与えるというもの。\n上のように多少のズレでも許す。\n$$ \\min_{\\mathbf{w, \\theta}} \\lambda || \\mathbf{w} || + \\sum _{i = 1} ^ {n} L(\\mathbf{y_i}, f(\\mathbf{w}, \\mathbf{x}_i)) $$\n$$ 条件1: y_i(\\mathbf{w}^T \\mathbf{x_i} + \\mathbf{\\theta}) \\geq 1 - L(y_i, f(\\mathbf{w}, x_i)) $$\n$$ 条件2: L(y_i, f(\\mathbf{w}, x_i)) \\geq 0 $$\n$\\lambda || \\mathbf{w} ||$は正則化項、$\\sum _{i = 1} ^ {n} L(\\mathbf{y_i}, f(\\mathbf{w}, \\mathbf{x}_i)$は、誤差項。 学習で誤差項を合わせる感じだけど、誤差にあまりに過学習させないように、当初の$|| \\mathbf{w} ||$が存在する。\n損失関数 # では、損失関数$L$についてどうするのか、ということになる。\n一番雑なのは、成功なら0、失敗したら1、という0-1損失関数。だけど、これではギリギリ成功、余裕を持って失敗と切り分けることはできない。 また、ハードマージンと同様、実際の最適解を求めるには、勾配法でやるので、微分可能が勿論望ましい。\nこのうち、\n①は、ハードもソフトも分類できてるもの。これ考えなくていい。 ②は、ハードもソフトも分類できてないもの。 ③は、ハードでは誤分類だが、ソフトで改善される。 ④は、ハードもソフトも分類できるが、ソフトではマージン内(なくてもセーフだが)。サポートベクトルじゃないのに\u0026hellip; 2から4まで考える必要がある。これを満たすのは、ヒンジ損失関数というもの。\n$$ \\max (0, 1 - y_i(\\mathbf{w}^T \\mathbf{x}_i + \\mathbf{\\theta})) $$\nこれを求めるのは例によって勾配法。虹計画法や確率的勾配降下法を使う。\n"},{"id":17,"href":"/docs/article/SVM/%E3%83%8F%E3%83%BC%E3%83%89%E3%83%9E%E3%83%BC%E3%82%B8%E3%83%B3/","title":"ハードマージン","section":"Svm","content":" ハードマージン # このQiitaわかりやすい。\nこっちも補足してみてる。\n最適化問題に落としこむ部分では こっちが一番わかりやすいかな。\nこの記事の冒頭のリンクでは、SVMが識別器$f(X)$だとして、損失関数はConvexだとうまくいかないと言ってた。\nSVMでは、ハードマージン(完璧に2つのグループに分類できる　具体的には2つのグループの最近点=サポートベクトル、はいずれも正しく分類されてる前提)をまず考える。この時、ヘッセの公式で、超平面$\\mathbf{w}^T \\mathbf{x} + \\mathbf{\\theta} = \\mathbf{0}$との距離は、各$\\mathbf{x}_i$について\n$$ \\frac{|\\mathbf{w}^T \\mathbf{x}_i + \\mathbf{\\theta|}}{|| \\mathbf{w} ||} $$\nとなる。この時、SVMが目的は$\\mathbf{w, \\theta}$を動かして、\n$$ d_{max} = \\max_{\\mathbf{w, \\theta}} [\\min_{i = 1, \\cdots, n} \\frac{|\\mathbf{w}^T \\mathbf{x}_i + \\mathbf{\\theta}|}{|| \\mathbf{w} ||} ] $$\n$$ = \\max_{\\mathbf{w, \\theta}} [\\frac{1}{|| \\mathbf{w} ||} \\min_{i = 1, \\cdots, n} |\\mathbf{w}^T \\mathbf{x}_i + \\mathbf{\\theta}| ] $$\nを得る事。ただ、超平面はスケール倍しても変わらないため、$\\min_{i = 1, \\cdots, n} |\\mathbf{w}^T \\mathbf{x}_i + \\mathbf{\\theta}| = 1$となるように、$\\mathbf{w, \\theta}$を正規化する(こういう条件を付けられる)。この条件下で、$\\frac{1}{|| \\mathbf{w} ||}$を最大化することになる。\nまた、ハードマージンの原理上、全ての点について正しく分類させないといけないので、各点$\\mathbf{x}_i$について、ラベル$y_i \\in -1, 1$について、以下が成り立つべし。\n$y_i = 1$なら、$\\mathbf{w}^T \\mathbf{x_i} + \\mathbf{\\theta} \\geq 0$ $y_i = -1$なら、$\\mathbf{w}^T \\mathbf{x_i} + \\mathbf{\\theta} \u0026lt; 0$ これを改めて清書すると、ハードマージンの最適化問題は、($||\\mathbf{w}||$は逆数を取った)\n$$ \\min_{\\mathbf{w, \\theta}} || \\mathbf{w} || $$\n$$ 条件：y_i(\\mathbf{w}^T \\mathbf{x_i} + \\mathbf{\\theta}) \\geq 0 $$\nこれを解くには、ラグランジュの未定乗数法を使う。 KKT条件という形式の問題なので、それをやってみると、以下の関数の\n$$ \\hat{\\mathbf{\\alpha}} = \\argmax_{\\mathbf{\\alpha}} [\\sum _{i = 1} ^ {n} - \\frac{1}{2} \\sum _{i = 1}^{n} \\sum _{j = 1}^{n} \\alpha_i \\alpha_j y^{(i)} y^{(j)} \\mathbf{x}^{(i)T} \\mathbf{x}^{(j)}] $$\n$$ 条件: \\alpha_i \\geq 0, \\sum _{i = 1} ^ {n} \\alpha_i y^{(j)} = 0 $$\nこのサイトでは式変形とかが詳しい。\nこれ自体は勾配法で最適解を求めることになる。\nimport matplotlib.pyplot as plt import numpy as np from sklearn import svm from sklearn.datasets import make_blobs from sklearn.preprocessing import minmax_scale # データセットを作成する。 X, y = make_blobs(n_samples=40, centers=2, random_state=6) # データセットを描画する。 fig, ax = plt.subplots(facecolor=\u0026#34;w\u0026#34;) ax.scatter(X[:, 0], X[:, 1], c=y, s=20, cmap=\u0026#34;Paired\u0026#34;) # 学習する。 clf = svm.LinearSVC(C=1000) clf.fit(X, y) # 分類平面及びマージンを描画する。 XX, YY = np.meshgrid(np.linspace(*ax.get_xlim(), 100), np.linspace(*ax.get_ylim(), 100)) xy = np.column_stack([XX.ravel(), YY.ravel()]) Z = clf.decision_function(xy).reshape(XX.shape) ax.contour( XX, YY, Z, colors=\u0026#34;k\u0026#34;, levels=[-1, 0, 1], alpha=0.5, linestyles=[\u0026#34;--\u0026#34;, \u0026#34;-\u0026#34;, \u0026#34;--\u0026#34;] ) plt.show() "},{"id":18,"href":"/docs/article/Weakly-Supervised-Learning/Positive%E3%81%A7%E3%83%A9%E3%83%99%E3%83%AB%E6%9C%89%E3%82%8A%E7%84%A1%E3%81%97%E3%81%A7%E5%88%86%E5%B8%83%E3%81%8C%E9%81%95%E3%81%86%E6%99%82%E3%81%AEPU%E5%AD%A6%E7%BF%922019/PU-Learning%E3%81%AE%E3%82%AF%E3%83%A9%E3%82%B9%E3%81%AE%E5%AE%9F%E8%A3%85/","title":"バイアスつきPU Learningクラスの実装","section":"Positiveでラベル有り無しで分布が違う時の PU学習(2019)","content":" こちらのGitHubのレポジトリのコードについて読み込んでみた。\npusb_liner_kernel.py # import部分 # import numpy as np from scipy import optimize import chainer from chainer import cuda, Function, gradient_check, Variable from chainer import optimizers, serializers, utils from chainer import Link, Chain, ChainList import chainer.functions as F import chainer.links as L numpy、scipyの最適化を使う。chainerはDNNの訓練と評価を行うための深層学習フレームワーク。汎用的なDNN作成を支援してるっぽい。\nPU#__init__() # def __init__(self, pi): self.pi = pi self.loss_func = lambda g: self.loss(g) あらかじめ$\\pi = p(y = +1)$だけ与えておく。\n識別器$g(\\mathbf{x})$を受け取って損失を計算するように、loss_func()を定義。もっともloss()も 定義されてるものだがそれを使う。\nPU#loss() # def loss(self, g): g = np.log(1+np.exp(-g)) return g 損失関数を定義。意味してるのは、\n$$ \\log (1 + e^{g}) $$\nうまいことなめらかなReLU関数みたいに。論文中では\n$$ l(f(\\mathbf{x}), y = +1) = -\\log g(\\mathbf{x}) $$\nと書いてあったけど真数マイナスにならんの？と思ったが、どうやら$f(\\mathbf{x})$は$[a, 1 - a], a \\in (0, 1/2)$の定義である。識別器、0か1かを出すの\u0026hellip;\u0026hellip;?謎。0がNegativeで1がPositiveなの？\nと思ったやん。これ最後に$- \\theta_{\\pi}$をかけて、マイナスならNegative、プラスならPositiveをするみたい。\nPU#pu() # def pu(self, x, b, t, reg): xp = x[t == 1] xu = x[t == 0] n1 = len(xp) n0 = len(xu) gp = np.dot(xp, b) gu = np.dot(xu, b) loss_u = self.loss_func(-gu) J1 = -(self.pi/n1)*np.sum(gp) J0 = (1/n0)*np.sum(loss_u) J = J1+J0+reg*np.dot(b,b) return J x　与えられるDataFrameの訓練データ。tが1ならPositive、0ならUnlabeled。 b　は与えられる引数。puを最小化するのに最終的に最適なbがゆくゆくは収束してほしい( 別のメソッドでやる)。下の式で言うと$g$を$\\mathbf{x}^T \\mathbf{b}$で実現させてるみたい？ つまり、ここでは$g$ははっきりとSVMであると言える。と思う。 t　はTarget。つまり各データに対して、0か1か。 reg　は正則化定数。ここではL2正則化をかけている。 x[t == 1]は、1である場合がTrueでそれ以外がFalse。それをxに噛ませて、該当のindexのところだけ抽出してる感じ。\nこれはPU Learning(オリジナル)の損失関数\n$$ R(g) = \\pi \\mathbb{E} _{p(\\mathbf{x} | y = +1)} [l(g(X), y = +1) - l(g(X), y = -1)] + \\mathbb{E} _{p(\\mathbf{x})} [ l(g(X), y = -1) ] $$\nについて計算しているようだ。論文で言うと(5)。\nJ1 # J1の項は$p(\\mathbf{x} | y = +1)$についての期待値のもの、すなわち\n$$ \\pi \\mathbb{E} _{p(\\mathbf{x} | y = +1)} [l(g(X), y = +1) - l(g(X), y = -1)] $$\nを求めているようだ。実際は不偏推定量で計算しているから、すべてのxpのデータについて、理想のbの係数ベクトルの積？の和を個数で割っている感じ。どうやら具体的な損失関数は別でやってるっぽい？\nJ0 # J0の項は$p(\\mathbf{x})$についての期待値のもの。\nどうやらUnlabeledのデータになんか$\\mathbf{b}$で内積を取って、それをなぜか先ほど定義した\\log (1 + e^{g})に入れてReLUから平滑化？の変換を施してからJ0を足している。謎。\nPU#prob() # def prob(self, x, b): x = self.x g = np.dot(x, b) prob = 1/(1+np.exp(-g)) return prob どこでも使われてなかったもの。\nデータの$\\mathbf{x}$と識別器のパラメタ行列の$\\mathbf{b}$を受け取ったら、積を取るともっともらしさみたいなのが出るから、これをシグモイド関数に入れて確率に変換している。\nPU#optimize() # def optimize(self, x, t, x_test): x_train, x_test, lda_chosen = self.kernel_cv(x, t, x_test) res = self.minimize(x_train, t, lda_chosen) return res, x_test x　訓練データ。 kernel_cvでテストデータに分割してもらう。 t　訓練データに対してラベルデータ。0 ro 1 x_test　テストのデータ。この関数では何もしない(kernel_cvで分割の参考にさせてるだけ) x_train, x_testはkernel_cvで分割してもらってる。\nlda_chosenは正規化項の係数。これもkernel_cvで求まるらしい。\n中身はラッパーで、 self.minimizeをやっているだけ。\nPU#minimize() # def minimize(self, x, t, reg): b0 = np.zeros(x.shape[1]) func = lambda b: self.pu(x, b, t, reg) grad = lambda b: self.gradient(x, b, t, reg) self.result = optimize.minimize(func, b0, jac=grad, method=\u0026#34;BFGS\u0026#34;) self.result = self.result.x return self.result x　訓練データ。 kernel_cvでテストデータに分割してもらう。 t　訓練データに対してラベルデータ。0 ro 1 初期解は0埋めしている。funcはscipyの最適化のtargetの関数にしている、ただラップしてるだけ。\ngradient定義してる関数を参照。これをラップしたものをgradient関数にしてるらしい。自動微分させない感じ？\nscipyのoptimize.minimizeでは、ヤコビアンを。BFGSは典型的な方法。\n最後にresultの計算して得た最適な係数たちを返して終わり。\nPU#gradient() # def gradient(self, x, b, t, reg): xp = x[t == 1] xu = x[t == 0] n1 = len(xp) n0 = len(xu)s g = np.dot(xu, b) z = 1/(1+np.exp(-g)) dg = np.sum(xp, axis=0)/n1 grad = -self.pi*dg + np.dot(z.T, xu)/n0 + reg * b return grad これは pu()と同様に最初はPositive=1とUnlaveled=0に分割している。\nまあやってることはpu()の微分だと思うけど。元々の式は以下のものに正則化項を入れていた。\n$$ R(g) = \\pi \\mathbb{E} _{p(\\mathbf{x} | y = +1)} [l(g(X), y = +1) - l(g(X), y = -1)] + \\mathbb{E} _{p(\\mathbf{x})} [ l(g(X), y = -1) ] $$\nこれの式は、不偏推定量で記述した上の形になれば、\n$$ -\\pi \\frac{1}{n_1} \\sum _{i = 1}^{n_1} \\mathbf{x} _{P,i} ^ T \\mathbf{b} + \\frac{1}{n_0} \\sum _{j = 1}^{n_0} \\mathbf{x} _{U, j} ^ T \\mathbf{b} + r \\mathbf{b} ^ T \\mathbf{b} $$\nこれはどうやら$f(\\mathbf{x}) = \\mathbf{x} ^ T \\mathbf{b}$だとしたときみたい。$\\mathbf{b}$で微分するので、\n$$ -\\pi \\frac{1}{n_1} \\sum _{i = 1}^{n_1} \\mathbf{x} _{P,i} + \\frac{1}{n_0} \\sum _{j = 1}^{n_0} \\mathbf{x} _{U, j} + 2r \\mathbf{b} $$\nなんか、ソースコードでは正規化項に2倍かかってないんだよな？間違えてやってないかね？\nなお、実際のところ、$\\log (1 + e^{x})$の変換してるところはそこは配慮せなあかん。それが上のコード。\nPU#test() # def test(self, x, b, t, quant=True, pi=False): theta = 0 f = np.dot(x, b) if quant is True: temp = np.copy(f) temp = np.sort(temp) theta = temp[np.int(np.floor(len(x)*(1-pi)))] pred = np.zeros(len(x)) pred[f \u0026gt; theta] = 1 acc = np.mean(pred == t) return acc 与えられた$\\mathbf{x}$と、識別器の係数ベクトル$\\mathbf{b}$について演算して、実際に検証する。\nquant is Trueならば、論文での\n$$ n^{test} \\pi = \\sum _{i = 1}^{n^{test}} 1(\\mathrm{if} \\hat{r}(\\mathbf{x} _i) \\geq \\hat{\\theta _{\\pi}}) $$\nをもとに、$\\theta$を逆算している。\n最後の比較はバイアスの$\\theta$を引いてプラスならPositive、それ以外ならNegativeで。\nPU#dist() # def dist(self, x, T=None, num_basis=False): (d,n) = x.shape # check input argument # set the kernel bases if num_basis is False: num_basis = 300 idx = np.random.permutation(n)[0:num_basis] C = x[:, idx] # calculate the squared distances XC_dist = CalcDistanceSquared(x, C) TC_dist = CalcDistanceSquared(T, C) CC_dist = CalcDistanceSquared(C, C) return XC_dist, TC_dist, CC_dist, n, num_basis これはkernel_cv内で使われているので、本筋はそちらを見たほうがいい。\nT　TODO 訓練データは$d$次元のもので、$n$個存在している。num_basisがない場合、デフォルトでベース300個とする。\n次にnum_basis個だけランダムに訓練データのidxを選択する。\nそして、XC, TC, CCの距離をそれぞれ計算して、返す。\n距離関数はユークリッド距離である。\nCalcDistanceSquared() # def CalcDistanceSquared(X, C): Xsum = np.sum(X**2, axis=0).T Csum = np.sum(C**2, axis=0) XC_dist = Xsum[np.newaxis, :] + Csum[:, np.newaxis] - 2*np.dot(C.T, X) return XC_dist 与えられた訓練データ集合について、$X$と$C$の全ての要素の間のユークリッド距離の和を計算する。\nPU#kernel_cv() # 非常に長いので、複数に分割する。\ndef kernel_cv(self, x_train, t, x_test, folds=5, num_basis=False, sigma_list=None, lda_list=None): x_train, x_test = x_train.T, x_test.T XC_dist, TC_dist, CC_dist, n, num_basis = self.dist(x_train, x_test, num_basis) x_train, t　訓練データとそのPUのラベル。 x_test　テストデータ。 folds　交叉検証の時の分割数。デフォルトは5。 まず、先ほど定義した distでランダムに数点を選択させて、Cとする。そこから\n訓練データとCの距離 テストデータとCの距離 C同士の距離 を求める。\n# setup the cross validation cv_fold = np.arange(folds) # normal range behaves strange with == sign cv_split0 = np.floor(np.arange(n)*folds/n) cv_index = cv_split0[np.random.permutation(n)] 次に、交叉検証を行う。cv_split0 = np.floor(np.arange(n)*folds/n)これによって、cv_split0は[0, 0, ..., 1, 1, ..., 2, 2, ...]のような配列に。\nそして、cv_indexはつまり、cv_split0をシャッフルしたものである。\n# set the sigma list and lambda list if sigma_list==None: sigma_list = np.array([0.01, 0.05, 0.1, 0.5, 1, 2, 5, 10, 20]) if lda_list==None: lda_list = np.array([0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1., 5.]) score_cv = np.zeros((len(sigma_list), len(lda_list))) ガウシアンカーネル(カーネル関数が$\\exp(|| \\mathbf{x} - \\mathbf{c}_l || ^ 2 / (2 \\sigma ^2))$)の分散について、いろいろ候補を用意している。\nldaはTODO。\nfor sigma_idx, sigma in enumerate(sigma_list): # pre-sum to speed up calculation h_cv = [] t_cv = [] for k in cv_fold: # 各k=テストにする分割。 # XC_distについて、exp(- || x_i - c ||^2 / (2σ^2))となるが、cは与えられたデータで、あらかじめPU#dist()でランダムに訓練データから300個抽出した感じみたい。 h_cv.append(np.exp(-XC_dist[:, cv_index==k]/(2*sigma**2))) # 今回のk=テストにする分割のデータの正解ラベル t_cv.append(t[cv_index==k]) for k in range(folds): # calculate the h vectors for training and test # kがテストにする分割ID。 # hte, tteにはテストデータのガウシアンカーネルの値と正解ラベル。 # htr, ttrには訓練データの同上のデータが。 count = 0 for j in range(folds): if j == k: hte = h_cv[j].T tte = t_cv[j] else: if count == 0: htr = h_cv[j].T ttr = t_cv[j] count += 1 else: htr = np.append(htr, h_cv[j].T, axis=0) ttr = np.append(ttr, t_cv[j], axis=0) # htrは本来のhtrと全て1の列を結合。 # hteは本来のhteと同上。 # つまり、いずれも特徴の方。 one = np.ones((len(htr), 1)) htr = np.concatenate([htr, one], axis=1) one = np.ones((len(hte), 1)) hte = np.concatenate([hte, one], axis=1) # ldaは正規化係数。正規化係数も交叉検証する。 for lda_idx, lda in enumerate(lda_list): # 交叉検証して、実際にminimizeしてみる。 res = self.minimize(htr, ttr, lda) # calculate the solution and cross-validation value score = self.pu(hte, res, tte, lda) score_cv[sigma_idx, lda_idx] = score_cv[sigma_idx, lda_idx] + score 大きなループのなかで、次のことを行っている。\n# get the minimum # 交叉検証の結果一番よかったものを選ぶ。 (sigma_idx_chosen, lda_idx_chosen) = np.unravel_index(np.argmin(score_cv), score_cv.shape) sigma_chosen = sigma_list[sigma_idx_chosen] lda_chosen = lda_list[lda_idx_chosen] x_train = np.exp(-XC_dist/(2*sigma_chosen**2)).T x_test = np.exp(-TC_dist/(2*sigma_chosen**2)).T one = np.ones((len(x_train),1)) x_train = np.concatenate([x_train, one], axis=1) one = np.ones((len(x_test),1)) x_test = np.concatenate([x_test, one], axis=1) return x_train, x_test, lda_chosen PU#liner_cv() # つかわれてない。\ndef linear_cv(x0, x1, folds=5, lda_list=None): if lda_list==None: lda_list = np.array([0.001, 0.01, 0.1, 1.]) scores = [] for lda in lda_list: func = lambda b: self.linear_uu(b, x0, x1) res = self.minimize(func, b0) score = func(res) scores.append(score) scores = np.array(scores) lda = lda_list[scores.argmin()] return lda ガウシアンカーネルを使わず、普通にminimize()させた感じ。\n"},{"id":19,"href":"/docs/article/Weakly-Supervised-Learning/Positive%E3%81%A7%E3%83%A9%E3%83%99%E3%83%AB%E6%9C%89%E3%82%8A%E7%84%A1%E3%81%97%E3%81%A7%E5%88%86%E5%B8%83%E3%81%8C%E9%81%95%E3%81%86%E6%99%82%E3%81%AEPU%E5%AD%A6%E7%BF%922019/%E5%AE%9F%E9%A8%93%E3%81%AE%E3%82%B3%E3%83%BC%E3%83%89%E3%81%AE%E8%A7%A3%E6%9E%90/","title":"バイアスつきPU Learningクラスの実装","section":"Positiveでラベル有り無しで分布が違う時の PU学習(2019)","content":" こちらのGitHubのレポジトリのコードについて読み込んでみた。\npusb_liner_kernel.py # import部分 # import numpy as np from scipy import optimize import chainer from chainer import cuda, Function, gradient_check, Variable from chainer import optimizers, serializers, utils from chainer import Link, Chain, ChainList import chainer.functions as F import chainer.links as L numpy、scipyの最適化を使う。chainerはDNNの訓練と評価を行うための深層学習フレームワーク。汎用的なDNN作成を支援してるっぽい。\nPU#__init__() # def __init__(self, pi): self.pi = pi self.loss_func = lambda g: self.loss(g) あらかじめ$\\pi = p(y = +1)$だけ与えておく。\n識別器$g(\\mathbf{x})$を受け取って損失を計算するように、loss_func()を定義。もっともloss()も 定義されてるものだがそれを使う。\nPU#loss() # def loss(self, g): g = np.log(1+np.exp(-g)) return g 損失関数を定義。意味してるのは、\n$$ \\log (1 + e^{g}) $$\nうまいことなめらかなReLU関数みたいに。論文中では\n$$ l(f(\\mathbf{x}), y = +1) = -\\log g(\\mathbf{x}) $$\nと書いてあったけど真数マイナスにならんの？と思ったが、どうやら$f(\\mathbf{x})$は$[a, 1 - a], a \\in (0, 1/2)$の定義である。識別器、0か1かを出すの\u0026hellip;\u0026hellip;?謎。0がNegativeで1がPositiveなの？\nと思ったやん。これ最後に$- \\theta_{\\pi}$をかけて、マイナスならNegative、プラスならPositiveをするみたい。\nPU#pu() # def pu(self, x, b, t, reg): xp = x[t == 1] xu = x[t == 0] n1 = len(xp) n0 = len(xu) gp = np.dot(xp, b) gu = np.dot(xu, b) loss_u = self.loss_func(-gu) J1 = -(self.pi/n1)*np.sum(gp) J0 = (1/n0)*np.sum(loss_u) J = J1+J0+reg*np.dot(b,b) return J x　与えられるDataFrameの訓練データ。tが1ならPositive、0ならUnlabeled。 b　は与えられる引数。puを最小化するのに最終的に最適なbがゆくゆくは収束してほしい( 別のメソッドでやる)。下の式で言うと$g$を$\\mathbf{x}^T \\mathbf{b}$で実現させてるみたい？ つまり、ここでは$g$ははっきりとSVMであると言える。と思う。 t　はTarget。つまり各データに対して、0か1か。 reg　は正則化定数。ここではL2正則化をかけている。 x[t == 1]は、1である場合がTrueでそれ以外がFalse。それをxに噛ませて、該当のindexのところだけ抽出してる感じ。\nこれはPU Learning(オリジナル)の損失関数\n$$ R(g) = \\pi \\mathbb{E} _{p(\\mathbf{x} | y = +1)} [l(g(X), y = +1) - l(g(X), y = -1)] + \\mathbb{E} _{p(\\mathbf{x})} [ l(g(X), y = -1) ] $$\nについて計算しているようだ。論文で言うと(5)。\nJ1 # J1の項は$p(\\mathbf{x} | y = +1)$についての期待値のもの、すなわち\n$$ \\pi \\mathbb{E} _{p(\\mathbf{x} | y = +1)} [l(g(X), y = +1) - l(g(X), y = -1)] $$\nを求めているようだ。実際は不偏推定量で計算しているから、すべてのxpのデータについて、理想のbの係数ベクトルの積？の和を個数で割っている感じ。どうやら具体的な損失関数は別でやってるっぽい？\nJ0 # J0の項は$p(\\mathbf{x})$についての期待値のもの。\nどうやらUnlabeledのデータになんか$\\mathbf{b}$で内積を取って、それをなぜか先ほど定義した\\log (1 + e^{g})に入れてReLUから平滑化？の変換を施してからJ0を足している。謎。\nPU#prob() # def prob(self, x, b): x = self.x g = np.dot(x, b) prob = 1/(1+np.exp(-g)) return prob どこでも使われてなかったもの。\nデータの$\\mathbf{x}$と識別器のパラメタ行列の$\\mathbf{b}$を受け取ったら、積を取るともっともらしさみたいなのが出るから、これをシグモイド関数に入れて確率に変換している。\nPU#optimize() # def optimize(self, x, t, x_test): x_train, x_test, lda_chosen = self.kernel_cv(x, t, x_test) res = self.minimize(x_train, t, lda_chosen) return res, x_test x　訓練データ。 kernel_cvでテストデータに分割してもらう。 t　訓練データに対してラベルデータ。0 ro 1 x_test　テストのデータ。この関数では何もしない(kernel_cvで分割の参考にさせてるだけ) x_train, x_testはkernel_cvで分割してもらってる。\nlda_chosenは正規化項の係数。これもkernel_cvで求まるらしい。\n中身はラッパーで、 self.minimizeをやっているだけ。\nPU#minimize() # def minimize(self, x, t, reg): b0 = np.zeros(x.shape[1]) func = lambda b: self.pu(x, b, t, reg) grad = lambda b: self.gradient(x, b, t, reg) self.result = optimize.minimize(func, b0, jac=grad, method=\u0026#34;BFGS\u0026#34;) self.result = self.result.x return self.result x　訓練データ。 kernel_cvでテストデータに分割してもらう。 t　訓練データに対してラベルデータ。0 ro 1 初期解は0埋めしている。funcはscipyの最適化のtargetの関数にしている、ただラップしてるだけ。\ngradient定義してる関数を参照。これをラップしたものをgradient関数にしてるらしい。自動微分させない感じ？\nscipyのoptimize.minimizeでは、ヤコビアンを。BFGSは典型的な方法。\n最後にresultの計算して得た最適な係数たちを返して終わり。\nPU#gradient() # def gradient(self, x, b, t, reg): xp = x[t == 1] xu = x[t == 0] n1 = len(xp) n0 = len(xu)s g = np.dot(xu, b) z = 1/(1+np.exp(-g)) dg = np.sum(xp, axis=0)/n1 grad = -self.pi*dg + np.dot(z.T, xu)/n0 + reg * b return grad これは pu()と同様に最初はPositive=1とUnlaveled=0に分割している。\nまあやってることはpu()の微分だと思うけど。元々の式は以下のものに正則化項を入れていた。\n$$ R(g) = \\pi \\mathbb{E} _{p(\\mathbf{x} | y = +1)} [l(g(X), y = +1) - l(g(X), y = -1)] + \\mathbb{E} _{p(\\mathbf{x})} [ l(g(X), y = -1) ] $$\nこれの式は、不偏推定量で記述した上の形になれば、\n$$ -\\pi \\frac{1}{n_1} \\sum _{i = 1}^{n_1} \\mathbf{x} _{P,i} ^ T \\mathbf{b} + \\frac{1}{n_0} \\sum _{j = 1}^{n_0} \\mathbf{x} _{U, j} ^ T \\mathbf{b} + r \\mathbf{b} ^ T \\mathbf{b} $$\nこれはどうやら$f(\\mathbf{x}) = \\mathbf{x} ^ T \\mathbf{b}$だとしたときみたい。$\\mathbf{b}$で微分するので、\n$$ -\\pi \\frac{1}{n_1} \\sum _{i = 1}^{n_1} \\mathbf{x} _{P,i} + \\frac{1}{n_0} \\sum _{j = 1}^{n_0} \\mathbf{x} _{U, j} + 2r \\mathbf{b} $$\nなんか、ソースコードでは正規化項に2倍かかってないんだよな？間違えてやってないかね？\nなお、実際のところ、$\\log (1 + e^{x})$の変換してるところはそこは配慮せなあかん。それが上のコード。\nPU#test() # def test(self, x, b, t, quant=True, pi=False): theta = 0 f = np.dot(x, b) if quant is True: temp = np.copy(f) temp = np.sort(temp) theta = temp[np.int(np.floor(len(x)*(1-pi)))] pred = np.zeros(len(x)) pred[f \u0026gt; theta] = 1 acc = np.mean(pred == t) return acc 与えられた$\\mathbf{x}$と、識別器の係数ベクトル$\\mathbf{b}$について演算して、実際に検証する。\nquant is Trueならば、論文での\n$$ n^{test} \\pi = \\sum _{i = 1}^{n^{test}} 1(\\mathrm{if} \\hat{r}(\\mathbf{x} _i) \\geq \\hat{\\theta _{\\pi}}) $$\nをもとに、$\\theta$を逆算している。\n最後の比較はバイアスの$\\theta$を引いてプラスならPositive、それ以外ならNegativeで。\nPU#dist() # def dist(self, x, T=None, num_basis=False): (d,n) = x.shape # check input argument # set the kernel bases if num_basis is False: num_basis = 300 idx = np.random.permutation(n)[0:num_basis] C = x[:, idx] # calculate the squared distances XC_dist = CalcDistanceSquared(x, C) TC_dist = CalcDistanceSquared(T, C) CC_dist = CalcDistanceSquared(C, C) return XC_dist, TC_dist, CC_dist, n, num_basis これはkernel_cv内で使われているので、本筋はそちらを見たほうがいい。\nT　TODO 訓練データは$d$次元のもので、$n$個存在している。num_basisがない場合、デフォルトでベース300個とする。\n次にnum_basis個だけランダムに訓練データのidxを選択する。\nそして、XC, TC, CCの距離をそれぞれ計算して、返す。\n距離関数はユークリッド距離である。\nCalcDistanceSquared() # def CalcDistanceSquared(X, C): Xsum = np.sum(X**2, axis=0).T Csum = np.sum(C**2, axis=0) XC_dist = Xsum[np.newaxis, :] + Csum[:, np.newaxis] - 2*np.dot(C.T, X) return XC_dist 与えられた訓練データ集合について、$X$と$C$の全ての要素の間のユークリッド距離の和を計算する。\nPU#kernel_cv() # 非常に長いので、複数に分割する。\ndef kernel_cv(self, x_train, t, x_test, folds=5, num_basis=False, sigma_list=None, lda_list=None): x_train, x_test = x_train.T, x_test.T XC_dist, TC_dist, CC_dist, n, num_basis = self.dist(x_train, x_test, num_basis) x_train, t　訓練データとそのPUのラベル。 x_test　テストデータ。 folds　交叉検証の時の分割数。デフォルトは5。 まず、先ほど定義した distでランダムに数点を選択させて、Cとする。そこから\n訓練データとCの距離 テストデータとCの距離 C同士の距離 を求める。\n# setup the cross validation cv_fold = np.arange(folds) # normal range behaves strange with == sign cv_split0 = np.floor(np.arange(n)*folds/n) cv_index = cv_split0[np.random.permutation(n)] 次に、交叉検証を行う。cv_split0 = np.floor(np.arange(n)*folds/n)これによって、cv_split0は[0, 0, ..., 1, 1, ..., 2, 2, ...]のような配列に。\nそして、cv_indexはつまり、cv_split0をシャッフルしたものである。\n# set the sigma list and lambda list if sigma_list==None: sigma_list = np.array([0.01, 0.05, 0.1, 0.5, 1, 2, 5, 10, 20]) if lda_list==None: lda_list = np.array([0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1., 5.]) score_cv = np.zeros((len(sigma_list), len(lda_list))) ガウシアンカーネル(カーネル関数が$\\exp(|| \\mathbf{x} - \\mathbf{c}_l || ^ 2 / (2 \\sigma ^2))$)の分散について、いろいろ候補を用意している。\nldaはTODO。\nfor sigma_idx, sigma in enumerate(sigma_list): # pre-sum to speed up calculation h_cv = [] t_cv = [] for k in cv_fold: # 各k=テストにする分割。 # XC_distについて、exp(- || x_i - c ||^2 / (2σ^2))となるが、cは与えられたデータで、あらかじめPU#dist()でランダムに訓練データから300個抽出した感じみたい。 h_cv.append(np.exp(-XC_dist[:, cv_index==k]/(2*sigma**2))) # 今回のk=テストにする分割のデータの正解ラベル t_cv.append(t[cv_index==k]) for k in range(folds): # calculate the h vectors for training and test # kがテストにする分割ID。 # hte, tteにはテストデータのガウシアンカーネルの値と正解ラベル。 # htr, ttrには訓練データの同上のデータが。 count = 0 for j in range(folds): if j == k: hte = h_cv[j].T tte = t_cv[j] else: if count == 0: htr = h_cv[j].T ttr = t_cv[j] count += 1 else: htr = np.append(htr, h_cv[j].T, axis=0) ttr = np.append(ttr, t_cv[j], axis=0) # htrは本来のhtrと全て1の列を結合。 # hteは本来のhteと同上。 # つまり、いずれも特徴の方。 one = np.ones((len(htr), 1)) htr = np.concatenate([htr, one], axis=1) one = np.ones((len(hte), 1)) hte = np.concatenate([hte, one], axis=1) # ldaは正規化係数。正規化係数も交叉検証する。 for lda_idx, lda in enumerate(lda_list): # 交叉検証して、実際にminimizeしてみる。 res = self.minimize(htr, ttr, lda) # calculate the solution and cross-validation value score = self.pu(hte, res, tte, lda) score_cv[sigma_idx, lda_idx] = score_cv[sigma_idx, lda_idx] + score 大きなループのなかで、次のことを行っている。\n# get the minimum # 交叉検証の結果一番よかったものを選ぶ。 (sigma_idx_chosen, lda_idx_chosen) = np.unravel_index(np.argmin(score_cv), score_cv.shape) sigma_chosen = sigma_list[sigma_idx_chosen] lda_chosen = lda_list[lda_idx_chosen] x_train = np.exp(-XC_dist/(2*sigma_chosen**2)).T x_test = np.exp(-TC_dist/(2*sigma_chosen**2)).T one = np.ones((len(x_train),1)) x_train = np.concatenate([x_train, one], axis=1) one = np.ones((len(x_test),1)) x_test = np.concatenate([x_test, one], axis=1) return x_train, x_test, lda_chosen PU#liner_cv() # つかわれてない。\ndef linear_cv(x0, x1, folds=5, lda_list=None): if lda_list==None: lda_list = np.array([0.001, 0.01, 0.1, 1.]) scores = [] for lda in lda_list: func = lambda b: self.linear_uu(b, x0, x1) res = self.minimize(func, b0) score = func(res) scores.append(score) scores = np.array(scores) lda = lda_list[scores.argmin()] return lda ガウシアンカーネルを使わず、普通にminimize()させた感じ。\n"},{"id":20,"href":"/docs/article/sns/%E3%83%95%E3%82%A1%E3%82%AF%E3%83%88%E3%83%81%E3%82%A7%E3%83%83%E3%82%AF%E3%81%A7%E3%81%AE%E5%BC%B1%E6%95%99%E5%B8%AB%E3%81%A4%E3%81%8D%E5%AD%A6%E7%BF%92/","title":"ファクトチェックでの弱教師つき学習","section":"sns","content":" Weakly Supervised Learning for Fake News Detection on Twitter # 論文はこちら\n著者：Stefan Helmstetter, Heiko Paulheim\n4ページしかない(迫真)\nIntroduction # 人の手でファクトチェックやるコストがあまりに高すぎる。汚れたデータのままで学習してみるよ。\nfake news自体のTweetを使うより、発信ユーザの信頼できるorできないを判断する野を目標にするよ。信頼できるユーザが少しfakeを流すとしてもそれは仕方ないと判断。\n手法は、**弱教師つき学習(Weakly Supervised Learning)**を使う。\n精度高かったよ。F1スコアで0.9にもなった。\n関連研究 # 今までの研究はみな\nデータ量が少なさすぎる。 局限する分野が狭すぎる。 各タスクに限定したもので、人の手でアノテーションしたものしか訓練データにならない。 という欠点があった。この研究では、わずかの信頼できる、できないというアノテーションから、大きな未アノテーションのデータへ適用できる。\nデータセット # 大きなデータセットと、小さな手動でラベル付けしたデータセットを用意したよ。\n大きなデータセット # まずは、信頼できる発信者と信頼できない発信者を集めることから始めた。\nここで、ツイートごとにfake or not fakeでラベル付けする。fake newsの発信者であっても、虚実入り混じる発信をすることに注意。\n65のfake newsの発信者を集めた。信頼できるのは10人ほど集めた。当然これらは、積極的にTwitterに発信することが前提である。データセットのバランスを合わせるのも忘れずに。\nTwitterでそれらのアカウントの2月から6月分のTweetを集めた。合わせて401414件あり、110787件がfakeと、ラベル付けされた。 fake newsのアカウントは15%のfakeと40%の真実、残りはニュースじゃないTweetであった。\n重要な事実として、嘘つきも、真実の割合が嘘に比べて3倍ぐらい多いのである。なので普通に判断させるとfake newsのソースも信頼できる、と扱われる。\nでも仕方ないんだ。アノテーションに手間かけられないしガバガバなのは許せ。by me.\n小さなデータセット # 絶対的基準として、Politifactからの116ツイートを使った。これはプロの記者がつけたもの。fake newsに対して、絶対的な正答例として使うことに。\nこれらのツイートしたアカウントと、大きなデータセットで追跡したアカウントは必ずしも一致しないことに注意。\nネガティブな例を生成するとかなんとか。ようわからん。\n類似度判定自体は、cos類似度やTF-IDFだそうで。\nこれトレーニングに使わなかったの\u0026hellip;? by me.\n評価シナリオ # ツイートだけ見て評価する or ツイートとプロフィール両方見ての判断。\n手法 # まずは各ツイートをベクトルに変換。そしていろんな手法でそのベクトルから特徴量を抽出する。特徴量には5種類あると考えた。\nユーザーレベルの特徴 # ユーザーのフォロワー数などの、TwitterAPIで取得できる限りのものを集めた。ツイート頻度、Tweet/Retweetの割合など。計53個集めたよ。\nツイートレベルの特徴 # これもAPIで取得できる限りのもの。？と！の出現頻度、Tweetの日時、Tweet数、語数を使った。69の特徴量を使った。\nただし、経過した時間依存のfavとRT数は使用しなかった。\nテキストの特徴 # TF-IDFを使ったbag of wordモデルと、Doc2Vecのモデル両方を使った。(注：今BARDあるから後者でよくね？)gensimという埋め込み生成を使った。ラベルなし学習でできるし。\nトピックの特徴 # トピックはかなり重要なので、Tweet文から抽出させるようにして、別途やることにした。遅延ディリクレ配置LDAモデルを訓練した。トピック数を10から200まで10刻みで動かした。\n感情の特徴 # SentiWordNetで各ツイートのpositive, negative, neutral wordの数を測定した。さらにTextBlobでtweetの客観性を計算した。\nこれらから8つの特徴量を追加した。\n次元圧縮 # さすがにこれらは多すぎるので、落とします。各特徴量のジニ係数？を計算し、一定以下の者は全て消した。\n学習アルゴリズムとパラメタ最適化 # Naive Bayes、Decision Tree, SVM, Nueral networkの4つを基本的な分類器とした。アンサンブルの手法のRandom ForestとXGBoostも行った。\n実験結果 # まずは、偽ニュースのソースについての特定をやってみた。クロス評価を行った。ユーザー情報を追加したほうが、明らかにファクトチェックではいい成績を残す。一番よかったのは、XGBoostでF1値がツイートだけは0.78、ユーザとツイート合わせては0.94と高かった。\n次に、偽ニュースのツイートについての特定をやってみた。こっちの方がまあ、難しいわけです。こっちはニューラルネットワークの方が性能良かった！\n結論 # 大きな不正確なアノテーションのデータでもいい結果が残せるで。しかもかなり良い結果ちゃうか？\n"},{"id":21,"href":"/docs/article/Weakly-Supervised-Learning/Positive%E3%81%A7%E3%83%A9%E3%83%99%E3%83%AB%E6%9C%89%E3%82%8A%E7%84%A1%E3%81%97%E3%81%A7%E5%88%86%E5%B8%83%E3%81%8C%E9%81%95%E3%81%86%E6%99%82%E3%81%AEPU%E5%AD%A6%E7%BF%922019/%E3%83%A1%E3%82%A4%E3%83%B3%E3%82%B3%E3%83%BC%E3%83%89/","title":"メインコード","section":"Positiveでラベル有り無しで分布が違う時の PU学習(2019)","content":" こちらのソースコード。\nimport # import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.linear_model import LogisticRegression from dataset_linear import make_data from pusb_linear_kernel import PU from densratio import densratio 色々import\nexperiment # def experiment(datatype, udata): # priors = [0.2, 0.4, 0.6, 0.8] ite = 100 pdata = 400 num_basis = 300 seed = 2018 est_error_pu = np.zeros((len(udata), len(priors), ite)) est_error_pubp = np.zeros((len(udata), len(priors), ite)) est_error_dr = np.zeros((len(udata), len(priors), ite)) # for i in range(len(udata)): u = udata[i] for j in range(len(priors)): pi = priors[j] for k in range(ite): np.random.seed(seed) #PN classification x, t = make_data(datatype=datatype) x = x/np.max(x, axis=0) one = np.ones((len(x),1)) x_pn = np.concatenate([x, one], axis=1) classifier = LogisticRegression(C=0.01, penalty=\u0026#39;l2\u0026#39;) classifier.fit(x_pn, t) perm = np.random.permutation(len(x)) x_train = x[perm[:-3000]] t_train = t[perm[:-3000]] x_test = x[perm[-3000:]] t_test = t[perm[-3000:]] xp = x_train[t_train==1] one = np.ones((len(xp),1)) xp_temp = np.concatenate([xp, one], axis=1) xp_prob = classifier.predict_proba(xp_temp)[:,1] #xp_prob /= np.mean(xp_prob) xp_prob = xp_prob**20 xp_prob /= np.max(xp_prob) rand = np.random.uniform(size=len(xp)) temp = xp[xp_prob \u0026gt; rand] while (len(temp) \u0026lt; pdata): rand = np.random.uniform(size=len(xp)) temp = np.concatenate([temp, xp[xp_prob \u0026gt; rand]], axis=0) xp = temp perm = np.random.permutation(len(xp)) xp = xp[perm[:pdata]] updata = np.int(u*pi) undata = u - updata xp_temp = x_train[t_train==1] xn_temp = x_train[t_train==0] perm = np.random.permutation(len(xp_temp)) xp_temp = xp_temp[perm[:updata]] perm = np.random.permutation(len(xn_temp)) xn_temp = xn_temp[perm[:undata]] xu = np.concatenate([xp_temp, xn_temp], axis=0) x = np.concatenate([xp, xu], axis=0) tp = np.ones(len(xp)) tu = np.zeros(len(xu)) t = np.concatenate([tp, tu], axis=0) updata = np.int(1000*pi) undata = 1000 - updata xp_test = x_test[t_test == 1] perm = np.random.permutation(len(xp_test)) xp_test = xp_test[perm[:updata]] xn_test = x_test[t_test == 0] perm = np.random.permutation(len(xn_test)) xn_test = xn_test[perm[:undata]] x_test = np.concatenate([xp_test, xn_test], axis=0) tp = np.ones(len(xp_test)) tu = np.zeros(len(xn_test)) t_test = np.concatenate([tp, tu], axis=0) pu = PU(pi=pi) x_train = x res, x_test_kernel = pu.optimize(x, t, x_test) acc1 = pu.test(x_test_kernel, res, t_test, quant=False) acc2 = pu.test(x_test_kernel, res, t_test, quant=True, pi=pi) result = densratio(x_train[t==1], x_train[t==0]) r = result.compute_density_ratio(x_test) temp = np.copy(r) temp = np.sort(temp) theta = temp[np.int(np.floor(len(x_test)*(1-pi)))] pred = np.zeros(len(x_test)) pred[r \u0026gt; theta] = 1 acc3 = np.mean(pred == t_test) est_error_pu[i, j, k] = acc1 est_error_pubp[i, j, k] = acc2 est_error_dr[i, j, k] = acc3 seed += 1 print(acc1) print(acc2) print(acc3) est_error_pu_mean = np.mean(est_error_pu, axis=2) est_error_pubp_mean = np.mean(est_error_pubp, axis=2) est_error_dr_mean = np.mean(est_error_dr, axis=2) est_error_pu_std = np.std(est_error_pu, axis=2) est_error_pubp_std = np.std(est_error_pubp, axis=2) est_error_dr_std = np.std(est_error_dr, axis=2) return est_error_pu_mean, est_error_pubp_mean, est_error_pu_std, est_error_pubp_std, est_error_dr_mean, est_error_dr_std "},{"id":22,"href":"/docs/article/sns/%E5%BC%B1%E6%95%99%E5%B8%AB%E5%AD%A6%E7%BF%92%E3%81%A7%E3%83%AA%E3%83%97%E3%83%A9%E3%82%A4%E3%81%8B%E3%82%89%E7%82%8E%E4%B8%8A%E5%88%86%E6%9E%90/","title":"弱教師学習でリプライから炎上分析","section":"sns","content":"タイトル: Analysis of Information Spreading by Social Media Based on Emotion and Empathy 著者: Kazuyuki Matsumoto, Minoru Yoshida and Kenji Kita\nIntroduction # バズや炎上ツイートについて、リプライを分析してみた。絵文字とかいろいろあるでしょ？\n手法 # 感情表現、顔文字、意味ベクトル、Latent Dirichlet Allocation、情報エントロピー、性格分析の手法を使って、著名人のツイート、バズツイート、炎上ツイートの分析をしてみる。\n日本語の感情表現 # 日本語はいろいろな感情表現がある。Japanese Appraisal Evaluation Expression Dictionaryを参考に、実装してみた。\n例として、\nPositive: うっとり、浮かれる、楽しむ Negative: 憤る、うっぷん、悲しむ 顔文字 # 日本語特有の顔文字。顔文字について、アノテーションとして検出して意味を分析した。↓図のように。\n絵文字 # 顔文字と同様に、感情としてアノテーションすべき。\nSemantic Vectors(Wiki2vec) # 辞書で意味を理解するのは大事であるが、Twitterなどで固有名詞やスラングがあり、そのままでは向いてない。\nこの研究では、 2018年時点のWikipediaをもとに200次元のWordToVecを作った。\nまた、BERTも使っている。15%の言葉をMASKにして、BERTに予測させるらしい。BERTにfine-tuningを加えることで、特定タスクへの適用への応用ができる。\nトピック分析 # LDAという手法を用いて、トピックを分析する。 Gensimでよくね？\n情報理論 # 単語の出現率自体で、離散のエントロピーを求められる。\n性格分析 # 性格診断をしたというTweetを投稿した人にまず絞る。\n書いた投稿にたいしての他のユーザからの返信から性格を分析する。これは松本らが提案したニューラルネットワークがあるらしい。\n分析結果 # 感情のポジネガによる分析 # 有名人ツイート　ツイート者にたいして好意的な評価が多い。 炎上ツイート　ネガティブな評価が多い。 バズツイート　意外にも、炎上ツイートと同じだということ。 ただ、細かく見ると、炎上ツイートはバズツイートと比べて広がりに欠ける形か。多いのはjoy \u0026gt;\u0026gt; hate \u0026gt; loveの順番で変わらない。\n顔文字の分析 # バズるツイート、バズらないツイート、炎上するツイート、顔文字の出現頻度は違えども、使用する傾向は大体似てる。やっぱり、joyの時だけ顔文字使うね。遥か突き放されての次点が、anxiety, neutral。\nあと、使和ない人はどんな場面でも使わないので、顔文字はやっぱり分析の特徴量になりえないかな。\n絵文字の分析 # どのタイプでも、joy \u0026gt;\u0026gt; sorrow \u0026gt; love\u0026gt; その他の形だった。\n全体的に、Non Buzzのツイートで大量に使われたが、FlamingやBuzzではつかわれなかった。\nTo Sen: replaceで絵文字を付け加えるとflamingとか、Buzzを抑えられる説？\n情報量の分析 # バズらないツイートの平均エントロピーが低かったのは、長い返信がないから。これは分類のいい指標となる。\n意味ベクトルの分析 # 返信について、BERT、Wiki2Vecの意味ベクトルをまずは取得し、別々、そしてつなげた768+100=868次元のベクトルにする。そして各ラベルについてk-meansを行って、t-distributed stochastic neighbor embeddingというもので高次元から二次元に落とし込んで、図にして分類してみた。\nBERTでは、炎上、バズ、非バズに差はなかった。\nwiki2vecと、868次元vectorでは、バズ、非バズは区別できないが、明らかに炎上ツイートだけは区別できた。BERTは汎用性の高い分散表現を識別するように訓練されてるので、炎上ツイートへの返信はほとんど固有名詞が多い可能性がある。\nトピックについて # 今回は10と20で試した。前述のLDAを実施した。 perplexityという指標があり、低いほど、より正確な確率モデルである。\n炎上とバズ、非バズのツイートについて、トピックでは大きな違いはない。\n性格推定について # ニューラルネットワークを用いて返信のセットを推定すると、、5種類の自我状態の度合いを示すベクトルとして得られる。このベクトルを自我状態ベクトルと呼ぶことにする。自我状態ベクトルの形状に従って、性格パターンが29種類に分類される。\nバズらないツイートでは性格にばらつきは大きく参考にならない。ただ、炎上とバズでは、あんまよい結果にならなかった？微妙。\n討論 # 顔文字や絵文字だけで意見はやはりわからにあので、言語情報と非言語情報の両方を分析するべきだな。\nただ、問題として返信数について捉えてるが、バズらない、炎上しないと返信が増えない。逆に増えてしまった場合すべてをカバーできない。フォロー、フォロワー関連と返信文の内容との相関を見るべきだな。\n結論 # 返信によって、バズ、非バズ、炎上を分類できるという可能性を見た。\n炎上とバズは、情報の拡散では似ているが、お互い区別せねばならない。ツイートの種類ごとに特定の条件を考慮し、両社の違いを分析するのがfuture workだね。\n"},{"id":23,"href":"/docs/editorial/DP/%E6%A1%81DP/","title":"桁 Dp","section":"Dp","content":" 実装の雛型 # 以下の部分だけで、$O(\\log N) \\times 20$かかる。ここに色々なフラグとかが付く。 具体的にはvを0から9と10通り試しているが、dpテーブルにはその次元は冗長である。\n条件部分のj | (v \u0026lt; digit)が意味してるのは、\nj == 0の未満フラグがない時、キープするにはv == digitでなければならない。論理和を取るので、v != digitである。そして、前提条件でj == 0 \u0026amp;\u0026amp; v \u0026gt; digitを排除してるので、v \u0026lt; digitにした。 j == 1の未満フラグがあるとき、もうすでにどの数字でもよい。遷移も1に移り続ける。 dp.resize(N.size() + 1, vector\u0026lt;ll\u0026gt;(2, 0)); //dp[i][j] := 上からi(1-idx)桁目で、jは未満なら1、イコールなら0 dp[0][0] = 1; for (int i = 0; i \u0026lt; N.size(); i++) { int digit = N[i] - \u0026#39;0\u0026#39;; for (int j = 0; j \u0026lt; 2; j++) { for (int v = 0; v \u0026lt;= 9; v++) { if (j == 0 \u0026amp;\u0026amp; v \u0026gt; digit)break; dp[i + 1][j | (v \u0026lt; digit)] += dp[i][j]; } } } ans = dp[N.size()][0] + dp[N.size()][1]; //これは「0」も含む、N以下の値の全て。 "},{"id":24,"href":"/docs/editorial/","title":"競プロの問題解説","section":"Docs","content":"問題のprefixに、難易度をつけている。\n色 表記 茶 Br(own) 緑 G(reen) 水 C(yan) 青 Bl(ue) 黄 Y(ellow) 橙 O(range) 赤 R(ed) 各色の中で、だいたい三等分して、\n下位なら- 中位なら無印 上位なら+ をつける。\n例えば、Difficultyが1300なら、(C-)ABC123Dとする。\n"},{"id":25,"href":"/docs/read_book/Machine-Learning/%E3%83%99%E3%82%A4%E3%82%BA%E6%8E%A8%E8%AB%96%E3%81%AB%E3%82%88%E3%82%8B%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E7%B7%91%E3%83%99%E3%82%A4%E3%82%BA/section-1/","title":"第一章","section":"ベイズ推論による機械学習(緑ベイズ)","content":" 機械学習とは # 機械学習とは、いい感じに予測する学習器を作ること。代表的なタスクは以下の通り。\n回帰 # いい感じのデータ点を当てはめる式を作る。線形回帰ならば$y_n = \\mathbf{w} ^ \\mathbf{x}_n + \\epsilon_n$ (1.1)。$\\epsilon_n$はノイズで、ある分布に従う。\n線形回帰のうち、$\\mathbf{x}_n = (1, x_n, x_n^2, \\cdots ) ^ T $のものを多項式回帰という。\n回帰のグラフィカルモデル # 回帰の同時分布$p(\\mathbf{y}, \\mathbf{w}, \\mathbf{X})$は以下のように考えられる。 すべての$y_i$は独立なので、$\\mathbf{y}$の場合はそれらの総乗に。また、明らかに$\\mathbf{y}$がないとき、$\\mathbf{X}$と$\\mathbf{w}$はお互いに独立なので、$p(\\mathbf{w}, \\mathbf{X}) = p(\\mathbf{w}) p(\\mathbf{X})$が成り立つ。\n$$ p(\\mathbf{y}, \\mathbf{w}, \\mathbf{X}) = p(\\mathbf{w}) \\prod _{i = 1}^{N} p(y_n | \\mathbf{w}, \\mathbf{x}_i) p(\\mathbf{x}_i) $$\nしたがって、回帰のグラフィカルモデルは(x_n) -\u0026gt; y_n \u0026lt;- (w)である。 また、$\\mathbf{w}$についての事後分布、$p(\\mathbf{w} | \\mathbf{X}, \\mathbf{y})$は$\\mathbf{w}$の変数以外は定数とみなせば、\n$$ p(\\mathbf{w} | \\mathbf{X}, \\mathbf{y}) \\propto p(\\mathbf{w}) \\prod _{i = 1}^{N} p(y_n | \\mathbf{w}, \\mathbf{x}_i) $$\nこの式になる。この事後分布を計算することを学習と指す。 実際の計算では、右辺を計算したのち、これが$p(\\mathbf{w} | \\mathbf{X}, \\mathbf{y})$であると意味を持てるように、$\\mathbf{w}$で積分したら1になるような比例定数を乗じることになる。\n予測分布 # $p(\\mathbf{w} | \\mathbf{X}, \\mathbf{y})$を計算することが、非ベイズ統計での$\\mathbf{w}$を直接求めることに当たる。$\\mathbf{w}$も不確定だから分布で表す感じ。\nこの$p(\\mathbf{w} | \\mathbf{X}, \\mathbf{y})$を用いることで、新たな入力値$\\mathbf{x}_l$に対する出力の予測$y_l$の分布を計算できる(というかこれが学習の目的)。$\\mathbf{w}$の同時確率まで拡張して式変形してみると以下のようになる。\n$$ p(y_l | \\mathbf{x}_l, \\mathbf{X}, \\mathbf{y}) = \\int p(y_l | \\mathbf{x}_l, \\mathbf{w}, \\mathbf{X}, \\mathbf{y}) d \\mathbf{w} = \\int p(y_l | \\mathbf{x}_l, \\mathbf{w}) p(\\mathbf{w} | \\mathbf{Y}, \\mathbf{X}) d \\mathbf{w} $$\n識別的モデル # 参考文献(wiki)\nこの回帰と次の分類は、データ$\\mathbf{x}$にたいする予測$y$についての尤もらしさに着目している。つまり、$p(y | \\mathbf{x})$に着目。\n分類 # 二値分類など。連続値から離散値にする。よく使われるのは、\n$$ \\mathrm{Sig}(x) = \\frac{1}{1 + e^{-x}} $$\nこれを使うことで、$(-\\infty, +\\infty)$を$(0, 1)$に変換できる。これが確率らしきものなので、確率と考えてもいい。\nこれを多クラス分類に拡張すると、ソフトマックス関数を使えばいい。 入力は$\\mathbf{a} = (a_1, a_2, \\cdots, a_K) ^ T$という$K$次元のベクトルで、それぞれが各クラスを代表する値。クラス$k$である確率は\n$$ \\mathrm{SM}_k(\\mathbf{a}) = \\frac{e^{- a_k}}{\\sum _{i = 1}^{K} e^ {- a_i}} $$\nこうすることで、$\\sum _{i = 1} ^ K \\mathrm{SM}_i (\\mathbf{a}) = 1$を満たす確率らしきものに変換できる。\nクラスタリング # 与えられたデータの近くのグループをまとめる感じ。\nクラスタリングのグラフィカルモデル # 観測データ$\\mathbf{X}$と、それらに対するクラスの割り当て$\\mathbf{S} = (\\mathbf{s}_1, \\cdots, \\mathbf{s}_N)$($\\mathbf{s}_i$はそれぞれone-hotベクトルと想定)があるとする。各クラスターごとのパラメタを$\\mathbf{\\Theta} = (\\mathbf{\\theta}_1, \\cdots, \\mathbf{\\theta}_N)$とする。 この時、$p(\\mathbf{X}, \\mathbf{S}, \\mathbf{\\Theta})$は、\n$$ p(\\mathbf{X}, \\mathbf{S}, \\mathbf{\\Theta}) = p(\\mathbf{\\Theta}) p(\\mathbf{X} | \\mathbf{S}, \\mathbf{\\Theta}) p(\\mathbf{S}) = p(\\mathbf{\\Theta}) \\prod_{i = 1}^{N} p(\\mathbf{x}_i | \\mathbf{s}_i, \\mathbf{\\Theta}) p(\\mathbf{s}_i) $$\nこれが対応するグラフィカルモデルは(x) -\u0026gt; (y) \u0026lt;- (W)。\nある事前分布$p(\\mathbf{\\Theta})$に従って$\\mathbf{\\Theta}$を決めて、そこから各データの$\\mathbf{x}_n$の所属$\\mathbf{s}_n$を元に、$p(\\mathbf{x}_n | \\mathbf{s}_n, \\mathbf{\\Theta})$で各データが生成されるという前提を取っている。\nここで、$\\mathbf{s}_n$は分類したカテゴリを示すが、$\\mathbf{x}_n$を生成する時にこれは未観測の変数であると考えられる=隠れ変数。\n次元削減 # $D \\times N$の行列$\\mathbf{Y}$を、$M \\times D$の行列$\\mathbf{W}$と$M \\times N$の行列$\\mathbf{X}$で近似する分解手法。\n$$ \\mathbf{Y} \\approx \\mathbf{W} ^ T \\mathbf{X} $$\nここで、$D, N \u0026raquo; M$となると、本来$O(DN)$必要なメモリが$O(M(D + N))$に圧縮できる。完全な復元は無理であるが、近似できればいいと考えれば有効な削減方法。\n次元削減のグラフィカルモデル # 基本的に回帰と同じ。\n$$ p(\\mathbf{Y}, \\mathbf{X}, \\mathbf{W}) = p(\\mathbf{Y} | \\mathbf{X}, \\mathbf{W}) p(\\mathbf{X}) p(\\mathbf{W}) = p(\\mathbf{W}) \\prod _{i = 1} ^ {N} p(\\mathbf{y}_i | \\mathbf{x}_i, \\mathbf{W}) p(\\mathbf{x}_i) $$\nグラフィカルモデルは(X) -\u0026gt; (Y) \u0026lt;- W。\nこの式から、$p(\\mathbf{W} | \\mathbf{Y})$と$p(\\mathbf{X} | \\mathbf{X})$を計算したりする。\n生成モデル # 参考文献(wiki)\nわかりやすい参考文献\n先ほどのクラスタリングもそうだが、これらは全ての観測データ$\\mathbf{X}$の背後にある生成仮定を記述している＝生成モデル。 全ての変数に関しての同時分布を作ってる。\n回帰や分類における$p(y | \\mathbf{x})$だけ求めればいいのと違い、同時分布も考えるので$p(\\mathbf{x})$まで求められるようになる。次式のように\n$$ p(y | \\mathbf{x}) = \\frac{p(y) p(y | \\mathbf{x})}{p(\\mathbf{x})} $$\n機械学習のアプローチ # ツールボックス # 既知の機械学習の手法について、何かしらの基準に従って性能が良い手法を選んで識別器を作るという考え。教師データでラベル付きのものを使うので、教師アリ学習となる。\n複雑な数学の知識無くても使えるが、本当に上手くfitする手法があるとは限らない。\nモデリング # データに関するモデルがあると仮定して、事前にそれを構築する。そのパラメタを学習データから訓練する。数学バリバリ使う。\n柔軟性は高いが訓練は大変で数学も大変。\n独立と条件付確率 # $y_1, y_2$が独立なら、同じ前提条件の下(条件なしでも、条件が何でも)、必ずこれが成り立つ。\n$$ p(y_1 | x) p(y_2 | x) = p(y_1, y_2 | x) $$\nベイズの定理 # $$ p(x | y) = \\frac{p(y | x) p(x)}{p(y)} = \\frac{p(y | x) p(x)}{\\int p(x, y) dx} $$\n事前分布、事後分布 # 事前分布は、条件$x$の分布→$p(x)$\n事後分布とは、結果$y$がわかっているときの、前提条件$x$の分布→$p(x | y)$。\n事後で情報がわかることによって、この2つは全然違う分布によくなったりする。\n逐次推論 # 毎回の観測が独立であるとする。\n$y_1$が得られた時の$x$の事後分布$p(x | y_1)$は、ベイズの定理によって、$p(y_1)$を定数と考えると、$p(y_1 | x) p(x)$に比例する。\nここで、新たに観測した$y_2$が得られたとして、$p(x | y_1)$から、アップデートした事後分布の$p(x | y_1, y_2)$を得たい。\n$$ p(x | y_1, y_2) \\propto p(x, y_1, y_2) = p(y_1 | x) p(y_2 | x) p(x) \\propto p(y_2 | x) p(x | y_1) $$\n毎回の観測が独立なので、$p(x, y_1, y_2) = p(x, y_1) p(x, y_2) = p(y_1 | x) p(y_2 | x) p(x)$が成り立つ。\nこのように、既知の事後分布$p(y_1 | x)$から分布のアップデートできる。これは追加学習、逐次学習、オンライン学習という。\nこれを一般化する。観測データが$N$個でそれぞれ独立ならば、$\\mathbf{y} = (y_1, \\cdots, y_N)$だとすると、同時分布は\n$$ p(x ,\\mathbf{y}) = p(x) \\Pi _{i = 1}^{N} p(y_i | x) $$\nによって、\n$$ p(x | \\mathbf{y}) \\propto p(x , \\mathbf{y}) = p(x) \\Pi_ {i = 1}^{N} p(y_i | x) = p(y_N | x) p(x | y_1, \\cdots, y_{N - 1}) $$\nとなる。\nグラフィカルモデル # DAGを用いて表現。$p(x, y) = p(y | x) p(x)$という関係性だとする。つまり、xについての事前分布と、yを知った後のxの事後分布である。\nこの時、グラフに$x \\to y$という辺を加える。\nもっとの複雑な例として、\n$$ p(x_1, x_2, x_3, x_4, x_5, x_6) = p(x_1) p(x_2 | x_1) p(x_3 | x_1) p(x_4 | x_2, x_3) p(x_5 | x_2, x_3) p(x_6 | x_4, x_5) $$\n同様に、p(終点, 始点)のルールでグラフを描くと\nこのようにDAGにすることで独立なものを簡単に判別できる。この例は全て$x_1$から来ているので独立はないが。\nhead to tail型 # $$ p(x, y, z) = p(x)p(y | x)p(z | y) $$\n普通の条件付確率のこの連鎖のような展開は、グラフィカルモデルでは一本のパスとなる。\n更に式変形してみる。\n$$ p(x, z | y) = \\frac{p(x, y, z)}{p(y)} = \\frac{p(x) p(y|x) p(z|y)}{p(y)} = p(x | y) p(z | y) $$\nこれが意味するのは、$y$という条件の下で、$x, z$が独立=条件付独立。 グラフに換算すると、一本のパスの上で、任意の距離2の2点$a, b$があって、間に挟んでいるもの$c$があるなら$c$という条件下では、$a, b$は条件付独立。\ntail to tail型 # $$ p(x, y, z) = p(y)p(x | y) p(z | y) $$\nこのような依存関係の時、満たすべき条件は明らかに$y$という前提で条件付独立である。\n面白いことに、前のhead to tail型とは構成が違う中、いずれも$y$という前提条件があるなら$x, z$が独立している。 どっちのグラフとしても、$y$を取り除けば$x, z$には関係がないということからどっちも同じとわかるだろう。\nhead to head型 # $p(x, y, z) = p(y | x, z) p(x) p(z)$\n明らかに、これはグラフで見ても$y$が有ろうがなかろうが$x, z$は独立している。\n逆に、$p(x, z | y)$という$y$を観測した前提においては、\n$$ p(x, z | y) = \\frac{p(x, y, z)}{p(y)} = \\frac{p(x) p(z) p(y | x, z)}{p(y)} $$\nここまでしか変形できずに、独立ではないとわかる。つまり、本来$x, z$は独立だが、$y$という条件を付けてしまうとお互いに関係が生じて独立じゃなくなるということである。\nマルコフブランケット # $x$について着目し、その確率を求めたいとする。\n$a \\to x$なので、$a$の向こうがどんな形で繋がろうが、head to tailかtail to tailなので、$a$に対して、$x$とその向こうはお互いに独立である。つまり、確率計算では$a$だけ見ればよい。\n$x \\to e, c \\to e$のような形では、head to headになるので、$e$という条件では$c$と$x$が関係を持つ。\n逆に言えば、上のグラフより外の依存関係は不要である。この周辺をマルコフブランケットという。つまり、$x$の親、子とその親である。\nベイズ学習のアプローチ # モデルの構築。観測データの$\\mathbf{d}$と、観測されてない変数$\\mathbf{x}$に関して、同時分布$p(\\mathbf{d}, \\mathbf{x})$を構築する。 よさげな分布(ガウス分布、各種離散分布とかを天下り的にこれだ！と思って決めておく) 事後分布$p(\\mathbf{x} | \\mathbf{d}) = \\frac{p(\\mathbf{x}, \\mathbf{d})}{p(\\mathbf{d})}$を解析的または近似的に得る。 $p(\\mathbf{d})$はモデルの中で観測データが出現する確率であり、周辺尤度という。これもあらかじめ計算しておく。 結果として、$p(\\mathbf{x} | \\mathbf{d})$が求められる。 ガウス分布、β分布等には再生性があり、$p(\\mathbf{x} | \\mathbf{d})$も同様なタイプの分布になるが、一般的にはそもそも$p(\\mathbf{x} | \\mathbf{d})$はキレイな分布にならない、解析的に計算できない場合も多い。\n解決策の1つとして、サンプリングがある。もう1つは厳密な$p(\\mathbf{x} | \\mathbf{C})$は計算できないなら、似てるような形が簡単な式を代わりに使うというもの。\n不確実性に基づく意思決定 # 確率推論はあくまで確率だけを出している。それを元に意思決定するのはまた別の仕事(確率的に大差ないなら意思決定して本当にいいの？？？)\nベイズ学習のメリットとデメリット # メリット\n一貫して事後分布の推論問題に帰着できる。 不確かさを含んだ推論ができる。 利用可能な知識を取り入れやすい。 過学習しづらい。 欠点\n数理的な知識を結構要求する。 計算コストが高い。 "},{"id":26,"href":"/docs/read_book/Machine-Learning/%E3%83%99%E3%82%A4%E3%82%BA%E6%8E%A8%E8%AB%96%E3%81%AB%E3%82%88%E3%82%8B%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E7%B7%91%E3%83%99%E3%82%A4%E3%82%BA/section-2/","title":"第二章","section":"ベイズ推論による機械学習(緑ベイズ)","content":" 期待値 # $$ \\mathbb{E} [f(\\mathbf{x})] = \\int f(\\mathbf{x}) p(\\mathbf{x}) d\\mathbf{x} $$\nと定義される。純粋に$p(\\mathbf{x})$の取る平均は$\\mathbb{E} [\\mathbf{x}] = \\int \\mathbf{x} p(\\mathbf{x}) d\\mathbf{x}$\n期待値は独立かそうじゃないか関わらず、線形性を持つ。\n分散は、 $$ \\mathbb{E} [(\\mathbf{x} - \\hat{\\mathbf{x}}) ^ T (\\mathbf{x} - \\hat{\\mathbf{x}})] = \\mathbb{E} [\\mathbf{x} ^ T \\mathbf{x}] - \\mathbb{E} [\\mathbf{x} ^ T] \\mathbb{E} [\\mathbb{x}] $$\n2変数以上の期待値 # $$ \\mathbb{E} _{p(\\mathbf{x}, \\mathbf{y})} [\\mathbf{x} ^ T \\mathbf{y}] $$\nを計算する。もし$\\mathbf{x}, \\mathbf{y}$がお互いに独立であるなら、$p(\\mathbf{x}, \\mathbf{y}) = p(\\mathbf{x}) p(\\mathbf{y})$であり、\n$$ \\int \\int \\mathbf{x} ^ T \\mathbf{y} p(\\mathbf{x, y})d\\mathbf{x} d\\mathbf{y} = \\int \\mathbf{x} ^ T p(\\mathbf{x}) d\\mathbf{x} + \\int \\mathbf{y} p(\\mathbf{y}) d \\mathbf{y} = \\mathbb{E} [\\mathbf{x} ^ T] \\mathbb{E} [\\mathbf{y}] $$\nが成り立つ。 しかし、独立ではない場合は、上の積分を丁寧に行うしかない。これは条件付期待値。\n$$ \\int \\int \\mathbf{x} ^ T \\mathbf{y} p(\\mathbf{x, y})d\\mathbf{x} d\\mathbf{y} = \\mathbb{E}_{\\mathbf{y}} [ \\mathbb{E} _{\\mathbf{x | y}} [\\mathbf{x} ^ T \\mathbf{y}] ] $$\nエントロピー # エントロピー$H(\\mathbf{x})$を定める。大きいほど乱雑で、0が一番整っている。ここで$\\log 0 = 0$と定義。\n$$ H(\\mathbf{x}) = -\\mathbb{E} [\\log p(\\mathbf{x})] = -\\int p(\\mathbf{x}) \\log p(\\mathbf{x}) d \\mathbf{x} $$\nKLダイバージェンス # 2つの分布$p(\\mathbf{x}), q(\\mathbf{x})$について、疑似的に分布の距離っぽいなにか(距離の公理は満たさない！！厳密にやるなら最適輸送)。これをKLダイバージェンス。\n$$ \\mathrm{KL} [p(\\mathbf{x}) || q(\\mathbf{x})] = \\mathbb{E}_{p(\\mathbb{x})} [\\log p(\\mathbf{x})] - \\mathbb{E} _{p(\\mathbb{x})} [\\log q(\\mathbf{x})] = -\\int q(\\mathbf{x}) \\log \\frac{q(\\mathbf{x})}{p(\\mathbf{x})} d\\mathbf{x} $$\nサンプリングによる期待値の推定 # 分散はそうでもないけど期待値はサンプリングの平均は不偏推定量なんで、$\\frac{1}{n} \\sum _{i = 1} ^ {n} f(\\mathbf{x}_i)$\n離散確率分布 # 事象2つ 事象3つ以上 1回試行 ベルヌーイ カテゴリ 複数回試行 二項 多項 ベルヌーイ分布 # 1回のコイン投げで、表の確率が$\\mu$、裏が$1 - \\mu$の時、$x$を取る確率は以下のようにまとめられる。 $x = 0$なら$1 - \\mu$, $x = 1$なら$\\mu$になるのをまとめている。\n$$ \\mathrm{Bern}(x | \\mu) = \\mu ^ x (1 - \\mu) ^ {1 - x}, x \\in {0, 1} $$\nなお、エントロピーは$- \\mathbb{E} [\\log p(\\mathbf{x})] = -(1 - \\mu)\\log (1 - \\mu) - \\mu \\log \\mu$となり、この形は明らかに$\\mu = 0.5$で最大値を取る。\n二項分布 # 複数回のベルヌーイ分布をやる。$M$回の試行を繰り返し、表が出るのは$\\mu$の確率である。 この時、$x$回表が出る確率は以下のようになる。\n$$ \\mathrm{Bin}(x | M, \\mu) = _M C _x \\mu ^ x (1 - \\mu) ^ {M - x} $$\n明らかに上式は$M = 1$において、ベルヌーイ分布と同じようになる。$x = 0$ならば表が出ない、$x = 1$ならば表が出る。$M \u0026gt; 1$だと$x$は回数という意味だが、$x = 1$である限りベルヌーイ分布と同じ。\n主な期待値は以下のようになる。\n$$ \\mathbb{E} [x] = M \\mu \\\\ \\mathbb{E} [x ^ 2] = M \\mu ((M - 1) \\mu + 1) $$\nカテゴリ分布 # ベルヌーイ分布は表と裏の2通りしかないが、これを3通り以上の状態に拡張する。 $\\mathbf{s}$はカテゴリを示すone-hotベクトル。$\\mathbf{\\pi}$は各カテゴリにおける出現確率。 以下の式では総乗であるが、$s_i = 1$は一つだけなので、そこの$\\pi_i$だけ出てそれ以外は$1$となる。掛け合わせると$\\pi_i$が出てくる。\n$$ \\mathrm{Cat} (\\mathbf{s}, \\mathbf{\\pi}) = \\prod_{i = 1} ^ {N} \\pi_i ^ {s_i} $$\n同様にエントロピーを計算すると、\n$$ -\\mathbb{E} [\\log \\prod_{i = 1} ^ {N} \\pi _i ^ {s_i}] = - \\sum _{i = 1} ^ {N} \\pi _i \\log \\pi _i $$\n多項分布 # 二項分布においてカテゴリを3つ以上にも拡張したもの。3つ以上の種類の$N$個の球を1列に並べる並び方が$\\frac{N!}{a!b!c!\\cdots}, a + b + c + \\cdots = N$であるので、同様に分布の式も以下のようになる。$\\mathbf{x}$は各カテゴリにおける出現数。\n$$ \\mathrm{Mult}(\\mathbf{x} | \\mathbf{\\pi}, M) = M! \\prod _{i = 1} ^ {N} \\frac{\\pi_i ^ {x_i}}{x_i !} $$\nこれの期待値は、\n$$ \\mathbb{E} [ x_i ] = M \\pi_i \\\\ \\mathbb{E} [ x_i x_j ] = M \\pi_i ((M - 1)\\pi_i + 1) \\:\\: \\mathrm{if} j = k \\\\ = M(M - 1) \\pi_i \\pi_j \\: \\: \\mathrm{otherwise} $$\nポアソン分布 # 非負の整数$x$について、以下の確率で生成する。\n$$ \\mathrm{Poi} (x | \\lambda) = \\frac{\\lambda ^ x}{x!} e^ {-\\lambda} \\\\ \\log \\mathrm{Poi} (x | \\lambda) = x \\log \\lambda - \\log x! - \\lambda $$\n$x$が大きくなると明らかに確率は下がるが、完全には0にならないのが二項分布とかとの違い。\n$$ \\mathbb{E} [x] = \\lambda \\\\ \\mathbb{E} [x^2] = \\lambda (\\lambda + 1) $$\n連続確率分布 # β分布 # $x \\in (0, 1)$の値を生成する分布。$\\Gamma(a) = (a - 1)!$のガンマ関数(非自然数も定義しているが)　下のガンマ関数部は正規化項。 意味していることは表の確率が$x$コインを投げて表$a - 1$回、裏$b - 1$回が出る分布。\n$$ \\mathrm{Beta}(x | a, b) = \\frac{\\Gamma(a + b)}{\\Gamma(a) \\Gamma(b)} x ^ {a - 1} (1 - x) ^ {b - 1} \\\\ \\log \\mathrm{Beta}(x | a, b) = (a - 1) \\log x + (b - 1) \\log (1 - x) + \\log \\frac{\\Gamma(a + b)}{\\Gamma(a) \\Gamma(b)} $$\n期待値は以下の通り。$\\psi(x) = \\frac{d}{dx} \\log \\Gamma(x) = \\frac{\\Gamma ^ {\\prime}(x)}{\\Gamma(x)}$ というdigamma関数だとすると、\n$$ \\mathbb{E} [x] = \\frac{a}{a + b} \\\\ \\mathbb{E} [\\log x] = \\psi(a) - \\psi(a + b) = \\mathbb{E} [\\log (1 - x)] \\\\ $$\nなお、同じ$a = 1, b = 2$と$a = 3, b = 9$でも、期待値は同じだけど、後者の方がとがっている分布になる。試行回数が増えてより自信をもって確実に言えるみたいなもの。\nβ分布は、ベルヌーイ分布と二項分布の共役事前分布である。\nディリクレ分布 # β分布は連続かつ、$x$か$1 - x$の二択であったが、これを三択以上に拡張したもの。 同様に**$x \\in (0, 1)$の値を生成する分布**である。\n$$ \\mathrm{Dir}(\\mathbf{x} | \\mathbf{\\alpha}) = \\frac{\\Gamma(\\sum _{i = 1}^{K} \\alpha_i)}{\\prod _{i = 1}^{K} \\Gamma(\\alpha_i)} \\prod _{i = 1}^{N} c_i ^ {\\alpha_i - 1} \\\\ \\log \\mathrm{Dir}(\\mathbf{x} | \\mathbf{\\alpha}) = \\sum _{i = 1} ^ K (\\alpha_i - 1) \\log x_i + \\log \\frac{\\Gamma(\\sum _{i = 1}^{K} \\alpha_i)}{\\prod _{i = 1}^{K} \\Gamma(\\alpha_i)} $$\n期待値関連はβ分布と似ている。\n$$ \\mathbb{E} [x_k] = \\frac{\\alpha_k}{\\sum _{i = 1} ^ {K} \\alpha_i } \\\\ \\mathbb{E} [\\log x_k] = \\psi(\\alpha_k) - \\psi(\\sum _{i = 1}^{K} \\alpha_i) $$\nディリクレ分布は、β分布からして、カテゴリ分布と多項分布の共役事前分布。\nγ分布 # 正の実数$x$を生成する。\n$$ \\mathrm{Gam}(x | a, b) = \\frac{b ^ a}{\\Gamma(a)} x ^ {a - 1} e ^ {-b \\lambda} \\\\ \\log \\mathrm{Gam}(x | a, b) = \\log (a - 1)x - b \\lambda + \\log \\frac{b ^ a}{\\Gamma(a)} $$\n期待値は以下の通り。\n$$ \\mathrm{E} [x] = \\frac{a}{b} \\\\ \\mathrm{E} [\\log x] = \\psi(a) - \\log b $$\nガンマ分布はポアソン分布と1次元ガウス分布の分散の逆数の共役事前分布である。\nガウス分布 # 期待値$mu$, 分散$\\sigma$のパラメタを持つ。\n$$ \\mathcal{N} (x | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp( - \\frac{(x - \\mu) ^ 2}{2 \\sigma^2}) \\\\ \\log \\mathcal{N} (x | \\mu, \\sigma^2) = -\\frac{1}{2} (\\frac{(x - \\mu) ^ 2}{2 \\sigma^2} + 2\\log \\sigma + \\log 2 \\pi) $$\n期待値として、\n$$ \\mathbb{E} [x] = \\mu \\\\ \\mathbb{E} [x^2] = \\mu^2 + \\sigma^2 $$\nエントロピーは以下の通り。\n$$ -\\mathbb{E} [ \\log p(x) ] = \\frac{1}{2} \\mathbb{E} [ \\frac{x^2 - 2 \\mu x + \\mu^2}{\\sigma ^ 2} + 2 \\log \\sigma + \\log 2 \\pi ] \\\\ = \\frac{1}{2} \\mathbb{E}[\\frac{\\mu ^ 2 + \\sigma ^ 2 - 2\\mu^2 + \\mu^2}{\\sigma ^ 2} + 2 \\log \\sigma + \\log 2 \\pi] = \\frac{1}{2} (1 + 2 \\log \\sigma + \\log 2 \\pi) $$\n2つのガウス分布$\\mathcal{N} (\\mu_1, \\sigma_1 ^ 2)$と$\\mathcal{N} (\\mu_2, \\sigma_2 ^ 2)$のKLダイバージェンスは、\n$$ \\mathbb{E}_{p} [ \\log p(x) ] - \\mathbb{E} _{p} [ \\log q(x) ] = -\\frac{1}{2} (\\frac{(\\mu_1 - \\mu_2) ^ 2 + \\sigma_2 ^ 2}{\\sigma_1 ^ 2}) + 2\\log \\frac{\\sigma_1}{\\sigma_2} - 1 $$\n多次元ガウス分布 # 先ほどのは1変数であったが、これを多次元にしたもの。$\\mathbf{\\mu}$は期待値のベクトル、$\\mathbf{\\Sigma}$は今日分散行列。まあ正定値で対称ですね。 $D$は次元数。\n$$ \\mathcal{N} (\\mathbf{x} | \\mathbf{\\mu}, \\mathbf{\\Sigma}) = \\frac{1}{\\sqrt{2\\pi} ^ D |\\Sigma|} \\exp(-\\frac{1}{2} (\\mathbf{x} - \\mathbf{\\mu}) ^ T \\mathbf{\\Sigma} ^ {-1} (\\mathbf{x} - \\mathbf{\\mu})) \\\\ \\log \\mathcal{N} (\\mathbf{x} | \\mathbf{\\mu}, \\mathbf{\\Sigma}) = -\\frac{1}{2} ((\\mathbf{x} - \\mathbf{\\mu}) ^ T \\mathbf{\\Sigma} ^ {-1} (\\mathbf{x} - \\mathbf{\\mu}) + \\log |\\mathbf{\\Sigma}| + D \\log 2 \\pi) $$\n上式において、$\\mathbf{\\Sigma}$は対角行列ならば、お互いの共分散が0なので$D$個の独立したガウス分布に分けられる。 共分散が0でなくても、シルベスター標準形に直すことで$D$この独立したガウス分布、ともやはりみなせる。\n期待値は以下の通り。二次元の$x x ^ T$は行列を作る。\n$$ \\mathbb{E} [\\mathbf{x}] = \\mathbf{\\mu} \\\\ \\mathbb{E} [\\mathbf{x} \\mathbf{x} ^ T] = \\mathbf{\\mu} \\mathbf{\\mu} ^ T + \\mathbf{\\Sigma} $$\nエントロピーは以下の通り。期待値の部分は$\\mathbf{x} ^ T D \\mathbf{x}$で対角行列Dなので、実質的にはtraceそのもの。\n$$ -\\mathbb{E} [\\log p(\\mathbf{x})] = \\frac{1}{2} (\\mathbb{E} [(\\mathbf{x} - \\mathbf{\\mu}) ^ T \\mathbf{\\Sigma} ^ {-1} (\\mathbf{x} - \\mathbf{\\mu})] + \\log |\\mathbf{\\Sigma}| + D \\log 2\\pi) \\\\ \\mathbb{E} [(\\mathbf{x} - \\mathbf{\\mu}) ^ T \\mathbf{\\Sigma} ^ {-1} (\\mathbf{x} - \\mathbf{\\mu})] = \\mathbb{E} [\\mathrm{Tr}((\\mathbf{x} - \\mathbf{\\mu}) ^ T \\mathbf{\\Sigma} ^ {-1} (\\mathbf{x} - \\mathbf{\\mu}))] \\\\ = \\mathbb{E} [\\mathrm{Tr}((\\mathbf{x} - \\mathbf{\\mu}) (\\mathbf{x} - \\mathbf{\\mu}) ^ T) \\mathbf{\\Sigma} ^ {-1} ] = \\mathbb{E} [\\mathrm{Tr}(\\mathbf{x} \\mathbf{x} ^ T - \\mathbf{x} \\mathbf{\\mu} ^ T - \\mathbf{\\mu} \\mathbf{x} ^ T + \\mathbf{\\mu} \\mathbf{\\mu} ^ T) \\mathbf{\\Sigma} ^ {-1} ] \\\\ = \\mathbb{E} [\\mathrm{Tr}(\\mathbf{\\mu} \\mathbf{\\mu} ^ T + \\mathbf{\\Sigma} - 2\\mathbf{\\mu} \\mathbf{\\mu} ^ T + \\mathbf{\\mu} \\mathbf{\\mu} ^ T) \\mathbf{\\Sigma} ^ {-1} ] = \\mathrm{Tr}(\\mathbb{E} [I]) = D $$\nよって、\n$$ -\\mathbb{E} [\\log p(\\mathbf{x})] = \\frac{1}{2} (\\log |\\mathbf{\\Sigma}| + D(\\log 2 \\pi + 1)) $$\n同様に、KLダイバージェンスは、\n$$ \\mathbb{E}_p [\\log p(\\mathbf{x})] - \\mathbb{E}_p [\\log q(\\mathbf{x})] = \\\\ \\frac{1}{2} (\\mathrm{Tr}[((\\mathbf{\\mu_1} - \\mathbf{\\mu_2}) (\\mathbf{\\mu_1} - \\mathbf{\\mu_2}) ^ T + \\mathbf{\\Sigma}_2) \\mathbf{\\Sigma}_1 ^ {-1}] + \\log \\frac{|\\mathbf{\\Sigma_1}|}{|\\mathbf{\\Sigma_2}|} - D) $$\nウィシャート分布 # $D \\times D$の正定値行列を生成する分布。なので、先ほどの多次元ガウス分布の分散の逆行列(=精度行列)の生成に使える。$\\nu$は自由度という量であり、$\\nu \u0026gt; D - 1$。$\\mathbf{W}$は$D \\times D$の正定値行列であるパラメタ。\n$$ \\mathcal{W} (\\mathbf{X} | \\nu, \\mathbf{W}) = (正則化項) |\\mathbf{X}| ^ {\\frac{\\nu - D - 1}{2}} \\exp(\\frac{1}{2} \\mathrm{Tr}(\\mathbf{W} ^ {-1} \\mathbf{X})) \\\\ \\log \\mathcal{W} (\\mathbf{X} | \\nu, \\mathbf{W}) = \\frac{\\nu - D - 1}{2} \\log |\\mathbf{X}| - \\frac{1}{2} {Tr}(\\mathbf{W} ^ {-1} \\mathbf{X}) + \\log (正則化項) $$\n期待値は以下の通り。\n$$ \\mathbb{E} [\\mathbf{X}] = \\nu \\mathbf{W} $$\n"},{"id":27,"href":"/docs/article/Weakly-Supervised-Learning/%E8%A3%9C%E3%83%A9%E3%83%99%E3%83%AB%E5%AD%A6%E7%BF%922017/","title":"補ラベル学習(2017)","section":"Weakly Supervised Learning","content":" Introduction # 「○○ではない」という補ラベルで学習はできないだろうか？\n既存では、\nPartial Label　いくつかのクラスの中の1つなのはわかるが、どれなのかは不明。 Multi Labels Setup　各データは1つのクラスのみならず、複数のクラスを持つ。 そしてこの補ラベル学習は、普通のラベル付けされた学習と組み合わせることもできると。\n既存のmulti-class分類についてのおさらい # 例によって定式化する。\nデータは$\\mathbf{x} \\in \\mathbb{R}^n$、labelは$y \\in 1, 2, \\cdots, K$。いつも通り、未知の分布$p(\\mathbf{x}, y)$に従う。\n最終目的は、$f(\\mathbf{x} : \\mathbb{R}^d \\to 1, 2, \\cdots, K)$の識別器$f(\\mathbf{x})$を作る事。$f(\\mathbf{x})$の決定の中身は以下のように書くとする。\n$$ f(\\mathbf{x}) = \\argmax _{y \\in 1, \\cdots, K} g_y (\\mathbf{x}) $$\n$g_y (\\mathbf{x})$は、$\\mathbf{x}$がクラス$y$であるか否かの二値分類器。(二値分類器なら、いくつかの1があったらそれらがすべてラベルってこと？連続した$[0, 1]$を出すのじゃなかったのか？by me)つまり、全てのクラスにおいて、もっともらしさを計算して一番もっともらしいものを答えとしている。\n例によって、リスク関数$R(f)$を以下のように、損失関数$l(f(\\mathbf{x}), y)$で定義(二値分類ではないので、1つの引数にまとめるのはできない)。この$l(m)$は、$m$が小さいほど大きな値を取る。\n$$ R(f) = \\mathbb{E} _{p(\\mathbf{x}, y)} [ L(f(\\mathbf{x}), y) ] $$\n損失関数 # 以下の2つの損失関数について着目する。いずれも普通の損失関数$l(m)$からの線形合成からなる。\nOne Versus All Loss # $$ L _{OVA} (f(\\mathbf{x}), y) = l(g_y (\\mathbf{x})) + \\frac{1}{K - 1} \\sum _{y \\prime \\neq y } l(- g _{y \\prime} (\\mathbf{x})) $$\nいかに自信満々にクラス$y$であれはあるほど、このロスは小さくなる。逆に、その決定したクラスと他のクラスとの差があまりない場合、2つ目の項でそれなりに損失関数が大きくなってしまう。\nPairwise Comparison # $$ L _{PC} (f(\\mathbf{x}), y) = \\sum _{y \\prime \\neq y} l(g_y (\\mathbf{x}) - g _{y \\prime} (\\mathbf{x})) $$\n上と同じように自信満々にクラス決定できれば強い。このロスの場合、0.2, 0.1, 0,1, 0,1, 0.1, 0.1, 0.1, 0.1, 0.1のような分布だと、上の方法より良く出る。\nどうやって補ラベルからやるのか？ # まずは普通の分類とは別に、**補ラベル(〇〇ではない！)**だけで分類器をどう作るかを書く。\n$\\bar{p}(\\mathbf{x}, \\bar{y})$が以下の分布に従ってるとする。定義からしたら、それはそうなんだけど。\n$$ \\bar{p}(\\mathbf{x}, \\bar{y}) = \\frac{1}{K - 1} \\sum _{y \\neq \\bar{y}} p(\\mathbf{x}, y) $$\nつまり、○○以外のすべてのラベルに対しての確率の平均。\nリスク関数の表示 # 補ラベルに対して、損失関数$\\bar{L}(f(\\mathbf{x}), y)$を考える。これが意味のするものは、予測結果$f(\\mathbf{x})$と、補ラベル=$y$じゃない、との相違度。\nこんなに風に表せる。\n$$ R(f) = (K - 1) \\mathbb{E} _{\\bar{p} (\\mathbf{x}, \\bar{y})} [\\bar{L} (f(\\mathbf{x}), \\bar{y})] - M_1 + M_2 $$\n$M1, M2$の増減はあれど、$K-1$倍された、補ラベル$\\bar{p}(\\mathbf{x}, \\bar{y})$全体においての、損失関数の期待値。\n$$ M_1 = \\sum _{\\bar{y} = 1}^{K} \\bar{L}(f(\\mathbf{x}), \\bar{y}) $$\n$M_1$は、すべてのありえる補ラベルに対して、予測結果との損失関数$\\bar{L}$の和である。\n$$ M_2 = \\bar{L}(f(\\mathbf{x}), y) + L(f(\\mathbf{x}), y) $$\n$M_2$は、普通の損失関数と、補ラベルの損失関数の和、みたいな。和が定数という仮定に従うなら、$\\mathbf{x}$がクラス$y$に属してる可能性が高ければ高いほど、$\\mathbf{x}$がクラス$y$に属してない可能性が低い。\nここで、$M_1, M_2$は定数であると仮定する。実際はそうじゃない場合も多々あるが、ここで考えて評価するには仮定する！\n変形 # $$ (K - 1) \\mathbb{E} _{\\bar{p} (\\mathbf{x}, \\bar{y})} [\\bar{L} (f(\\mathbf{x}), \\bar{y})] = (K - 1) \\int \\sum _{\\bar{y} = 1} ^ {K} \\bar{L} (f(\\mathbf{x}), \\bar{y}) p(\\mathbf{x}, \\bar{y}) d\\mathbf{x} $$\n前に定義した、$\\bar{p}(\\mathbf{x}, \\bar{y})$を代入する。\n$$ = (K - 1) \\int \\sum _{\\bar{y} = 1} ^ {K} \\bar{L} (f(\\mathbf{x}), \\bar{y}) (\\frac{1}{K - 1} \\sum _{y \\neq \\bar{y}} p(\\mathbf{x}, y)) d\\mathbf{x} $$\nここで、Σの2つの順番を、次のようにしてもいい。全体をイテレーションしてることには変わりがないからだ。\n$$ = \\int \\sum _{y = 1} ^ {K} \\sum _{\\bar{y} \\neq y} \\bar{L} (f(\\mathbf{x}), \\bar{y}) p(\\mathbf{x}, y) d\\mathbf{x} $$\nよく見ると$p(\\mathbf{x}, y)$についての期待値のかたちである。\n$$ = \\mathbb{E} _{p(\\mathbf{x}, y)} [ \\sum _{\\bar{y} \\neq y} \\bar{L} (f(\\mathbf{x}), \\bar{y}) ] = \\mathbb{E} _{p(\\mathbf{x}, y)} [ M_1 - \\bar{L}(f(\\mathbf{x}), y) ] $$\n$M_1$には$\\mathbf{x}$が入ってるけど、定数と仮定するので、\n$$ = M_1 - \\mathbb{E} _{p(\\mathbf{x}, y)} [\\bar{L}(f(\\mathbf{x}), y) ] $$\nそして、ここで、\n$$ (K - 1) \\mathbb{E} _{\\bar{p}(\\mathbf{x}, \\bar{y})} [ \\bar{L}(f(\\mathbf{x}), \\bar{y}) ] - \\mathbb{E} _{\\bar{p}(\\mathbf{x}, \\bar{y})}[L(f(\\mathbf{x}), y)] $$\n次に、突然だが、引いてみる。\n$$ = M_1 - \\mathbb{E} _{p(\\mathbf{x}, y)} [ \\bar{L}(f(\\mathbf{x}), y) + L(f(\\mathbf{x}), y)] = M_1 - \\mathbb{E} _{p(\\mathbf{x}, y)} [ M_2 ] = M_1 - M_2 $$\nここで、$R(f) = \\mathbb{E} _{p(\\mathbf{x}, y)} [ L(f(\\mathbf{x}), y) ]$であることから、\n$$ R(f) = \\mathbb{E} _{p(\\mathbf{x}, y)} [ L(f(\\mathbf{x}), y) ] = M_2 - \\mathbb{E} _{p(\\mathbf{x}, y)} [ \\bar{L}(f(\\mathbf{x}), y) ] $$\n$$ = M_2 - (M_1 - (K - 1) \\mathbb{E} _{\\bar{p} (\\mathbf{x}, \\bar{y})} [\\bar{L} (f(\\mathbf{x}), \\bar{y})]) $$\nこのように、式変形できるのだ。\n実際の使われ方 # $$ R(f) = (K - 1) \\mathbb{E} _{\\bar{p} (\\mathbf{x}, \\bar{y})} [\\bar{L} (f(\\mathbf{x}), \\bar{y})] - M_1 + M_2 $$\n実際にはこの式を経験的な値で代用をするしかない。\n$$ \\hat{R}(f) = (K - 1)\\frac{1}{N} \\bar{L} \\sum _{i = 1} ^ N (f(\\mathbf{x} _i), \\bar{y_i}) $$\n損失関数の実際の使われ方 # 先ほど定義したOVA損失とPC損失を、経験損失で書き直してみる。補ラベルなので、$l(m)$の符号は逆である。\n$$ \\bar{L}_{OVA} (f(\\mathbf{x}), \\bar{y})= \\frac{1}{N - 1} \\sum _{y \\neq \\bar{y}} l(g _{y} (\\mathbf{x}_i)) + l( -g _{\\bar{y}} (\\mathbf{x_i})) $$\n$$ \\bar{L}_{PC} (f(\\mathbf{x}), y) = \\sum _{y \\neq \\bar{y}} l(g _{y}(\\mathbf{x}_i) - g _{\\bar{y}}(\\mathbf{x}_i)) $$\nあれ、PCのlの中身の符号はお互い逆では？？？ようわからんのだ。\n点対称$l(z) + l(-z)$ # もし、$l(z) + l(-z) = 1$と点対称ならば、\n$$ \\bar{L} _{OVA} (f(\\mathbf{x}), \\bar{y})= \\frac{1}{N - 1} \\sum _{y \\neq \\bar{y}} l(g _{y} (\\mathbf{x}_i)) + l( -g _{\\bar{y}} (\\mathbf{x_i})) = 1 $$\nこの時定数の$M_1 = N, M_2 = 2$\n$$ \\bar{L}_{PC} (f(\\mathbf{x}), y) = \\sum _{y \\neq \\bar{y}} l(g _{y}(\\mathbf{x}_i) - g _{\\bar{y}}(\\mathbf{x}_i)) $$\nこの場合は求まらないが、$M_1 = \\frac{K (K - 1)}{2}$であり、$M_2 = K - 1$となる。\n$l(z) + l(-z) = 1$となる関数はシグモイド関数、ランプ損失、01損失などがある。シグモイドとランプは識別器の訓練に、0-1はハイパーパラメタの訓練に使われる。\nリスク関数の抑え方 # ここはスキップで\u0026hellip;\n通常のラベル付けと補ラベルの融合 # 通常のラベル付けのリスク関数は\n$$ \\mathbb{E} _{p(\\mathbf{x}, y)} [ L(f(\\mathbf{x}), y) ] $$\nなので、組み合わせるだけならば、以下の関数の最適化を考えればよい。以下の関数は融合できてると言える。$\\alpha$は0から1の間の定数。\n$$ \\hat{R} (f) = \\alpha \\frac{1}{N_1} \\sum _{i = 1} ^ {N_1} L(f(\\mathbf{x}_i), y_i) + (1 - \\alpha) (K - 1)\\frac{1}{N_2} \\bar{L} \\sum _{i = 1} ^ {N_2} (f(\\mathbf{x} _i), \\bar{y_i}) $$\nこれの最小化という最適化問題に落ち着く。\n実験結果 # "},{"id":28,"href":"/docs/read_book/Machine-Learning/%E3%83%99%E3%82%A4%E3%82%BA%E6%8E%A8%E8%AB%96%E3%81%AB%E3%82%88%E3%82%8B%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E7%B7%91%E3%83%99%E3%82%A4%E3%82%BA/section-3part1/","title":"第三章その1","section":"ベイズ推論による機械学習(緑ベイズ)","content":" 学習と予測 # ベイズ統計では、各値は一定ではなくある分布に代表されるというもの。 なので、ベイズ統計での機械学習とは、パラメタの事前分布と、学習データを前提条件とした事後分布を求めることに当たる。\nつまり、訓練データ$\\mathbf{D}$を用いて、知りたい未知のパラメタ$\\theta$は、\n$$ p(\\mathbf{D}, \\theta) = p(\\mathbf{D} | \\theta) p(\\theta) $$\nこのような式となる。この$p(\\mathbf{D} | \\theta), p(訓練データ | 知りたいパラメタ)$は尤度関数という。逆の$p(\\theta | \\mathbf{D}), p(知りたいパラメタ | 訓練データ)$は事後分布。ベイズの定理によって、尤度関数から事後分布を計算できる。\n$$ p(\\theta | \\mathbf{D}) = \\frac{p(\\mathbf{D} | \\theta) p(\\theta)}{p(\\mathbf{D})} $$\nこの左辺を計算することが学習にあたる。\n予測分布 # 1章にもあったように、訓練済のパラメタを使って予測結果を得るのは、以下の式になる。グラフィカルモデルからも同じような形が得られる。\n$$ p(x_a | \\mathbf{D}) = \\int p(x_a | \\theta) p(\\theta | \\mathbf{D}) d \\theta \\\\ p(x_a, \\theta | \\mathbf{D}) = \\frac{p(x_a, \\theta, \\mathbf{D})}{p(\\mathbf{D})} = \\frac{p(\\mathbf{\\theta}) p(x_a | \\mathbf{\\theta}) p(\\mathbf{D} | \\theta)}{p(\\mathbf{D})} $$\nこの枠組み、パラメタの事後分布$p(\\theta | \\mathbf{D})$がすべての学習データ$D$を持つことになるが、データ量に対してモデルの表現能力が変化しないという大きな制限をもってしまう。ガウス過程などのベイジアンノンパラメトリクスの手法で改善できるらしい。この本はやらない。\n共役事前分布 # 事前分布$p(\\theta)$と事後分布$p(\\theta | \\mathbf{D})$が、数学的な計算をすると全く同形になる分布のことを共役事前分布という。\n逆に言えば普通は同じ分布にならない。ならないので面倒な積分を解く必要があるし、厳密に計算できるとも限らない場合は常々ある。\nどのような事前分布が共役になるかは、尤度関数$p(\\mathbf{D} | \\theta)$の設計に依存する。\n共役性があると、同じ分布のパラメタの関係式があって、それで積分せずに目的の分布の値がわかっちゃう！すごい！\n特にデータを小分けしながら学習してアップデートする場合、一章の逐次学習の式では、\n$$ p(\\theta | D_n, D_{n - 1}, \\cdots, D_1) \\propto p(\\theta | D_{n - 1}, \\cdots , D_1) p(D_n | \\theta) $$\nで学習できる(ただし、それぞれの小分けしたデータは互いに独立であるという前提で)。 ここで共役事前分布を使うことで、上の逐次学習にあるすべての項は同じ形の分布なのでやりやすい。 そうでないときは色々やり方があるが、一例としてはKLダイバージェンスを最小化するとか。ここら辺の最適化は共役勾配法とかで解くしかないね。\n共役事前分布の表 # 尤度関数 パラメタ 共役事前分布 予測分布 ベルヌーイ分布 $\\mu$ β分布 ベルヌーイ分布 二項分布 $\\mu$ β分布 β・二項分布 カテゴリ分布 $\\mathbf{\\pi}$ ディリクレ分布 カテゴリ分布 多項分布 $\\mathbf{\\pi}$ ディリクレ分布 ディリクレ・多項分布 ポアソン分布 $\\lambda$ γ分布 負の二項分布 1次元ガウス分布 $\\mu$ 1次元ガウス分布 1次元ガウス分布 1次元ガウス分布 $\\sigma ^ 2$ γ分布 1次元のstudentのt分布 1次元ガウス分布 $\\mathbf{\\mu}, \\sigma ^ 2$ ガウス・γ分布 1次元のstudentのt分布 多次元ガウス分布 $\\mathbf{\\mu}$ 多次元ガウス分布 多次元ガウス分布 多次元ガウス分布 $\\mathbf{\\sigma} ^ 2$ ウィシャート分布 多次元のstudentのt分布 多次元ガウス分布 $\\mathbf{\\mu}, \\mathbf{\\sigma} ^ 2$ ガウス・ウィシャート分布 多次元のstudentのt分布 離散確率分布の学習と予測 # $$ p(x | \\mu) = \\mathrm{Bern}(x | \\mu) = \\mu ^ x (1 - \\mu) ^ {1 - x} $$\nに従い、ベルヌーイ分布を考える。\nベイズ統計ではパラメタは定まった値ではなく、ある分布に従って値を産むという考え。なので、パラメタ$\\mu \\in (0, 1)$を産む分布=共役事前分布を考える。 ここで、 表によれば、ベルヌーイ分布の共役事前分布はβ分布。 よって、以下のようになる。パラメタ$a, b$はモデルの生成する値をコントロールするハイパーパラメタであり、ここではあらかじめ与えられてると考える。\n$$ p(\\mu) = \\mathrm{Beta}(\\mu | a, b) = \\frac{\\Gamma(a + b)}{\\Gamma(a - 1) \\Gamma(b - 1)} \\mu ^ {a - 1} (1 - \\mu) ^ {b - 1} $$\nこの事前分布$p(\\mu)$と訓練データ点の分布$p(x)$について、事後分布$p(\\mu | x)$を計算したい。ベイズの定理により、以下が成り立ち代入する。ここで、分布の概形だけわかってあとで正規化係数を入れればいいので、概形だけに着目する。\n$$ p(\\mu | x) = \\frac{p(x | \\mu) p(\\mu)}{ p(x)} \\propto p(x | \\mu) p(\\mu) \\\\ \\propto \\mu ^ x (1 - \\mu) ^ {1 - x} \\times \\mu ^ {a - 1} (1 - \\mu) ^ {b - 1} = \\mu ^ {a - 1 + x} (1 - \\mu) ^ {b - x} $$\nやはり、概形は事前分布のβ分布と同じ形になっている。$x = 1$の時は$a \\leftarrow a + 1$で、$x = 0$の時は$b \\leftarrow b + 1$となる。意味付けとしては、\nβ分布の$a, b$はそれぞれ事前に$a - 1, b - 1$回コインの表と裏が出たことを意味している。 ベルヌーイ試行を通して、β分布のハイパーパラメタをアップデートした形。 なお、経験ベイズ法(Empirical Bayes)はハイパーパラメタの値をも学習によって変動させるやつ。ハイパーパラメタにも事前分布を用意するという感じ。\nこの例では、事前分布がお互い違うとしても、やはりどんどん近づいていくということに。ただし、事前分布で0と設定されている確率密度=絶対に無いと確信している場合、いくら学習を繰り返しても事後分布ではそこは0から変わらない。\n次に、未観測の値$x_n \\in 0, 1$に対して予測を行ってみる。今までの訓練データを$\\mathbf{X}$とする。また事後分布は$\\mathrm{Beta}(\\mu | a, b)$に従うと学習で分かったとする。\n$$ p(x_n | \\mathbf{X}) = \\int p(x_n | \\mu) p(\\mu | \\mathbf{X}) d\\mu = \\frac{\\Gamma(a + b)}{\\Gamma(a) \\Gamma(b)} \\int \\mu ^ {x_n} (1 - \\mu) ^ {1 - x_n} \\times \\mu ^ {a - 1} (1 - \\mu) ^ {b - 1} d\\mu \\\\ = \\frac{\\Gamma(a + b)}{\\Gamma(a) \\Gamma(b)} \\int \\mu ^ {a - 1 - x_n} (1 - \\mu) ^ {b - x_n} d\\mu $$\nβ関数について、以下の式が成り立つことを利用して式変形をする。\n$$ \\int \\mu ^ {a - 1} (1 - \\mu) ^ {b - 1} d\\mu = \\frac{\\Gamma(a) \\Gamma(b)}{\\Gamma(a + b)} \\\\ \\frac{\\Gamma(a + b)}{\\Gamma(a) \\Gamma(b)} \\int \\mu ^ {a - 1 - x_n} (1 - \\mu) ^ {b - x_n} d\\mu = \\frac{\\Gamma(a + b)}{\\Gamma(a) \\Gamma(b)} \\frac{\\Gamma(a + x_n) \\Gamma(b - x_n + 1)}{\\Gamma(a + b + 1)} \\\\ = \\frac{\\Gamma(a + b)}{\\Gamma(a + b + 1)} \\frac{ \\Gamma(a + x_n) \\Gamma(b - x_n + 1)}{\\Gamma(a) \\Gamma(b)} = \\frac{a}{a + b} (\\mathrm{if} :: x_n = 1), \\frac{b}{a + b} (\\mathrm{if} :: x_n = 0) $$\nこのように予測分布を得られた。β分布の期待値でもある。そして、この予測分布出さえ、ベルヌーイ分布である。\n$$ (\\frac{a}{a + b}) ^ {x_n} (\\frac{b}{a + b}) ^ {1 - x_n} = \\mathrm{Bern}(x_n | \\frac{a}{a + b}) $$\nカテゴリ分布の学習と予測 # 先ほどは2択試行を1回だったベルヌーイ分布であったが、今回は3択以上1回の選択のカテゴリ分布を考える。パラメタ$\\mathbf{\\pi}$を使って、\n$$ p(\\mathbf{x} | \\mathbf{\\pi}) = \\mathrm{Cat}(\\mathbf{x} | \\mathbf{\\pi}) = \\prod _{i = 1} ^ {N} \\pi_i ^ {x_i} \\\\ \\log p(\\mathbf{x} | \\mathbf{\\pi}) = \\sum _{i = 1} ^ {N} x_i \\log \\pi_i $$ パラメタ$\\mathbf{\\pi}$は、カテゴリ分布の共役事前分布であるディリクレ分布に従う。$\\mathbf{\\alpha}$はハイパーパラメタ。\n$$ p(\\mathbf{\\pi}) = \\mathrm{Dir} (\\mathbf{\\pi} | \\mathbf{\\alpha}) = \\frac{\\Gamma(\\sum _{i = 1} ^ N \\alpha_i)}{\\prod _{i = 1} ^ N \\Gamma(\\alpha_i)} \\prod _{i = 1} ^ {N} \\pi _i ^ {\\alpha _i - 1} \\\\ \\log p(\\pi) = \\sum _{i = 1} ^ N (\\alpha _i - 1) \\log \\pi _i + \\log \\Gamma ( \\sum _{i = 1} ^ {N} \\alpha_i) - \\sum _{i = 1} ^ N \\Gamma(\\alpha _i) $$\n累乗は扱いづらいので、ここでlogを取ったもので計算する。 なお、概形を見るために、係数に関してはいつも通り無視しておく。\n$$ \\log p(\\mathbf{\\pi} | \\mathbf{x}) \\propto \\log p(\\mathbf{x}, \\mathbf{\\pi}) = \\log p(\\mathbf{x} | \\mathbf{\\pi}) p(\\mathbf{\\pi}) \\\\ = \\sum _{i = 1} ^ {N} x_i \\log \\pi_i + \\sum _{i = 1}^{N} (\\alpha_i - 1) \\log \\pi_i = \\sum _{i = 1} ^ {N} (\\alpha_i - 1 + x_i) \\log \\pi_i \\\\ p(\\mathbf{\\pi} | \\mathbf{x}) \\propto \\prod _{i = 1} ^ {N} \\pi_i ^ {\\alpha_i - 1 + x_i} $$\nこのように事後分布も、ディリクレ分布。ハイパーパラメタは$\\mathbf{\\hat{\\alpha}} = (\\alpha_1 + x_1, \\cdots, \\alpha_N + x_N)$。 これも、事前分布が学習によって事後分布となって記憶されるということになる。\nまた、予測を考えてみる。予測は\n$$ \\int p(\\mathbf{x} _{new} | \\mathbf{\\pi}) p(\\mathbf{\\pi}) d\\pi = \\int \\mathrm{Cat}(\\mathbf{x} _{new} | \\mathbf{\\pi}) \\mathrm{Dir}(\\mathbf{\\pi} | \\mathbf{\\alpha}) d \\mathbf{\\pi} $$\n$$ \\propto \\int \\prod _{i = 1} ^ {N} \\pi_i ^ {x _{new} ^ {i}} \\prod _{i = 1} ^ {N} \\pi _{i} ^ {\\alpha_i - 1} d \\mathbf{\\pi} = \\int \\prod _{i = 1} ^ {N} \\pi_i ^ {x _{new} ^ {i} + \\alpha_i - 1} d\\pi $$\nポアソン分布の学習と予測 # ポアソン分布の意味は、ある指定期間の間に$\\lambda$回起きる、とわかっている事象が$x$回起こる確率の分布。\nポアソン分布の事後分布は、\n$$ p(x | \\lambda) = \\mathrm{Poi}(x, \\lambda) = \\frac{\\lambda ^ x}{x!} e ^ {-\\lambda} $$\n共役事前分布はγ分布であると知られている。\n$$ p(\\lambda) = \\mathrm{Gam}(\\lambda | a, b) = \\frac{b ^ a}{\\Gamma(a)} \\lambda ^ {a - 1} e^ {-b \\lambda} $$\nよって、学習すると、\n$$ p(x | \\lambda) p(\\lambda) = \\frac{b ^ a}{\\Gamma(a) \\Gamma(x + 1)} \\lambda ^ {x + a - 1} e ^ {-(b + 1)\\lambda} \\\\ = \\frac{b ^ a}{\\Gamma(a + x + 1)} \\lambda ^ {x + a - 1} e ^ {-(b + 1)\\lambda} $$\nこれは、パラメタが$x + a, b + 1$のγ分布となる。$b$は回数、$a$は重みみたいな感じ。\n予測分布の計算もしてみる。\n$$ \\int p(x_{new} | \\lambda) p(\\lambda) d \\lambda = \\int \\mathrm{Poi} (x_{new} | \\lambda) \\mathrm{Gam}(\\lambda | a, b) d \\lambda \\\\ = \\int \\frac{b ^ a}{\\Gamma(a + x + 1)} \\lambda ^ {x_{new} + a - 1} e ^ {-(b + 1)\\lambda} d \\lambda $$\nこれを計算すると、負の二項分布となるらしい。 普通の二項分布は試行の回数を固定。負の二項分布は失敗回数を固定。\n"},{"id":29,"href":"/docs/read_book/Machine-Learning/%E3%83%99%E3%82%A4%E3%82%BA%E6%8E%A8%E8%AB%96%E3%81%AB%E3%82%88%E3%82%8B%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E7%B7%91%E3%83%99%E3%82%A4%E3%82%BA/section-3part2/","title":"第三章その2","section":"ベイズ推論による機械学習(緑ベイズ)","content":" 1次元ガウス分布 # ガウス分布はハイパーパラメタとして、平均値$\\mu$、分散$\\sigma ^ 2$の2つがあり、どれを学習するのかで話が変わる。なお、精度$\\lambda = \\frac{1}{\\sigma ^ 2}$とする。\n平均の推定 # 観測値$x$に対して、ガウス分布を考える。平均だけ推定するので、精度$\\lambda$は既知だとする。\n$$ p(x | \\mu) = \\mathcal{N} (x | \\mu, \\lambda ^ {-1}) $$\nこの平均値$\\mu$自体の事前分布も、共役事前分布の別のガウス分布に従うとする。$m, \\lambda _{\\mu}$はハイパーパラメタ。\n$$ p(\\mu) = \\mathcal{N}(\\mu | m, \\lambda _{\\mu} ^ {-1}) $$\n今回、一連の観測データ$\\mathbf{x} = (x_1, \\cdots, x_n)$を得たとする。ここから事後分布$p(\\mu | \\mathbf{x})$を学習する。\n$$ p(\\mu | \\mathbf{x}) \\propto p(\\mathbf{x} | \\mu) p(\\mu) = \\prod _{i = 1} ^ N (\\mathcal{N} (x_i | \\mu, \\lambda ^ {-1})) \\cdot \\mathcal{N} (\\mu | m, \\lambda _{\\mu} ^ {-1}) \\\\ $$\nここで、掛け算なので正規分布の再生性は使えない。対数を取って丁寧に計算していく。\n$$ \\log \\mathcal{N} (x | \\mu, \\lambda ^ {-1}) = \\frac{1}{2} \\log \\lambda - \\frac{1}{2} \\log 2 \\pi - \\frac{(x - \\mu) ^ 2}{2} \\lambda $$\nであるので、\n$$ \\log p(\\mu | \\mathbf{x}) \\propto \\frac{1}{2}(\\log \\lambda ^ {N} \\lambda _{\\mu}) - \\frac{N + 1}{2} \\log 2\\pi - \\frac{\\lambda}{2} \\sum _{i = 1} ^ {N} (x_i - \\mu) ^ 2 - \\frac{\\lambda _{\\mu}}{2} (\\mu - m) ^ 2f \\\\ = -\\frac{1}{2} \\mu ^ 2 (\\lambda N + \\lambda _{\\mu}) + \\mu (\\lambda _{\\mu} m + \\lambda \\sum _{i = 1} ^ N x_i) + \\mathrm{const} $$\n天下り的に計算するとこれは、$p(\\mu | \\mathbf{x}) = \\mathcal{N} (\\mu | \\hat{m}, \\hat{\\lambda _{\\mu}})$と分布を書くと、\n$$ \\hat{\\lambda _{\\mu}} = N \\lambda + \\lambda _{\\mu} \\\\ \\hat{m} = \\frac{1}{\\hat{\\lambda _{\\mu}}} (\\lambda \\sum _{i = 1} ^ N x_i + \\lambda _{\\mu} m) $$\nこれが意味するのは、\n$\\hat{m}$は観測データを集めれば集めるほど、当初の事前分布の期待値$m$の影響が薄まり、代わりに学習した$x_i$が決定に寄与するようになるということ。 $\\hat{\\lambda _{\\mu}}$は観測データを集めれば集めるほど高くなる=分散は小さくなる。つまり、観測データが集まるほど、$\\mu$の事後分布のばらつきは小さくなる。 次は同様に予測分布を計算する。\n$$ p(x _{pred}) = \\int p(x _{pred} | \\mu) p(\\mu) d\\mu = \\int \\mathcal{N} (x _{pred} | \\mu, \\lambda ^ {-1}) \\mathcal{N} (\\mu | m, \\lambda _{\\mu} ^ {-1}) d\\mu \\\\ \\log p(x _{pred} | \\mu) p(\\mu) = \\frac{1}{2} (\\log \\lambda + \\lambda _{\\mu}) - \\frac{1}{2} (\\log 2 \\pi) - \\frac{1}{2}(\\lambda(x _{pred} - \\mu) ^ 2 + \\lambda _{\\mu} (\\mu - m) ^ 2) \\\\ = -\\frac{1}{2} \\mu ^ 2 (\\lambda + \\lambda _{\\mu}) + 2\\mu (\\lambda x _{pred} + \\lambda _{\\mu} m) + \\mathrm{const} $$\n同様に、予測分布$p(x _{pred}) = \\mathcal{N} (x _{pred} | \\hat{m}, \\hat{\\lambda})$として、\n$$ \\frac{1}{\\hat{\\lambda}} = \\frac{1}{\\lambda} + \\frac{1}{\\lambda _{\\mu}} \\\\ \\hat{m} = m $$\nとなる。予測分布の期待値はまさに事前分布$p(\\mu)$の期待値そのものであり、予測分布の分散は事前分布と観測分布の分散の和であると言える。\n精度が未知の場合 # 平均$\\mu$は既知であるが、精度$\\lambda$を学習したい場合を考える。データ$x$は次の分布に従う。\n$$ p(x | \\lambda) = \\mathcal{N}(x | \\mu, \\lambda ^ {-1}) $$\nパラメタ$\\lambda$は次の事前分布に従うとする。γ分布を選ぶことで、共役事前分布となる。\n$$ p(\\lambda) = \\Gamma(\\lambda | a, b) = \\frac{b ^ a}{\\Gamma(a)} \\lambda ^ {a - 1} e ^ {-b \\lambda} $$\n学習するデータは複数個あると考えると、$p(\\mathbf{x} | \\lambda) = \\prod _{i = 1} ^ N p(x_i | \\lambda)$\nここで、ベイズの定理を使って同様に事後確率$p(\\lambda | \\mathbf{x})$を学習してみる。\n$$ p(\\lambda | \\mathbf{x}) \\propto p(\\mathbf{x} | \\lambda) p(\\lambda) = \\mathcal{N}(\\mathbf{x} | \\mu, \\lambda ^ {-1}) \\Gamma(\\lambda | a, b) $$\nここで、確率密度関数の対数はそれぞれ以下のとおりである。\n$$ \\log \\mathcal{N} (x | \\mu, \\lambda ^ {-1}) = \\frac{1}{2} \\log \\lambda - \\frac{1}{2} \\log 2 \\pi - \\frac{(x - \\mu) ^ 2}{2} \\lambda \\\\ \\log \\Gamma(\\lambda | a, b) = a \\log b - \\log \\Gamma(a) + (a - 1) \\log \\lambda - b \\lambda $$\nこれを使って、$p(\\lambda | \\mathbf{x})$を計算し、そこから$\\lambda$に関係する項だけを選び出すと、\n$$ p(\\mathbf{x} | \\lambda) p(\\lambda) = \\mathcal{N}(\\mathbf{x} | \\mu, \\lambda ^ {-1}) \\Gamma(\\lambda | a, b) = \\prod _{i = 1} ^ N \\mathcal{N}(x | \\mu, \\lambda ^ {-1}) \\Gamma(\\lambda | a, b)\\\\ = \\frac{N}{2}(\\log \\lambda - \\log 2 \\pi) - \\frac{\\lambda}{2} \\sum _{i = 1} ^ N (x _i - \\mu) ^ 2 + a \\log b - \\log \\Gamma(a) + (a - 1) \\log \\lambda - b\\lambda \\\\ = -\\lambda (\\frac{1}{2} \\sum _{i = 1} ^ N (x _i - \\mu) ^ 2 + b) + \\log \\lambda (\\frac{N}{2} - a + 1) + \\mathrm{const} $$\nこの形はγ分布の対数版と同じ形であるので、この形は$\\Gamma(\\lambda | \\hat{a}, \\hat{b})$として、\n$$ \\hat{a} = \\frac{N}{2} + a \\\\ \\hat{b} = \\frac{1}{2} \\sum _{i = 1} ^ N (x _i - \\mu) ^ 2 + b $$\nこれが意味することは、ようわからん。パラメタの更新がこんな風ってことかな\u0026hellip;?\n次は例によって予測分布を計算する。\n$$ p(x _{pred}) = \\int p(x _{pred} | \\lambda) p(\\lambda) d \\lambda = \\int \\mathcal{N} (x _{pred} | \\mu, \\lambda ^ {-1}) \\Gamma(\\lambda | a, b) d \\lambda $$\nこれは計算してもいいが、ベイズの定理$p(\\lambda | x _{pred}) = \\frac{p(x _{pred} | \\lambda) p(\\lambda)}{p(x _{pred})}$を対数を取って、$p(\\lambda)$の項を無視すれば、\n$$ \\log p(x _{pred}) = \\log p(x _{pred} | \\lambda) - \\log p(\\lambda | x _{pred}) + \\mathrm{const} $$\nこれから計算を進めると、\n$$ \\log p(x _{pred}) = -\\frac{2a + 1}{2} \\log (1 + \\frac{1}{2b} (x _{pred} - \\mu) ^ 2) + \\mathrm{const} $$\nこれは実はstudentのt分布である。\n平均と精度いずれも未知である。 # $$ p(x | \\mu, \\lambda) = \\mathcal{N} (x | \\mu, \\lambda ^ {-1}) $$\nこの時、事前分布は以下のようなガウス・γ分布である。なお、お互いに独立である(はず)ので、上記のようなガウス分布、γ分布でそれぞれやってもよいが、1つにまとめられる感じ。ハイパーパラメタは$m, \\beta, a, b$。同様にデータは複数個ある$\\mathbf{x}$と考える。\n$$ \\mathcal{N} (\\mu | m, (\\beta \\lambda) ^ {-1}) \\mathrm{Gam} (\\lambda | a, b) $$\n事後分布は、以下の値となる。\n$$ \\mathcal{N} (x | \\mu, \\lambda ^ {-1}) \\mathcal{N} (\\mu | m, (\\beta \\lambda) ^ {-1}) \\mathrm{Gam} (\\lambda | a, b) $$\n先ほどの平均、精度での計算法を流用すると、まずは前二項から平均を計算する。これは平均計算のパートと同じなので、流用すると\n$$ \\hat{\\beta} = N + \\beta \\\\ \\hat{m} = \\frac{1}{\\hat{\\beta}} (\\sum _{i = 1} ^ N x _n + \\beta m) $$\n"}]