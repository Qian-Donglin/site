---
title: "第三章"
weight: 1
# bookFlatSection: false
# bookToc: true
# bookHidden: false
# bookCollapseSection: false
# bookComments: false
# bookSearchExclude: false
---

# 学習と予測

ベイズ統計では、**各値は一定ではなくある分布に代表される**というもの。
なので、**ベイズ統計での機械学習とは、パラメタの事前分布と、学習データを前提条件とした事後分布を求める**ことに当たる。

つまり、訓練データ$\mathbf{D}$を用いて、知りたい未知のパラメタ$\theta$は、

$$
p(\mathbf{D}, \theta) = p(\mathbf{D} | \theta) p(\theta)
$$

このような式となる。この$p(\mathbf{D} | \theta), p(訓練データ | 知りたいパラメタ)$は尤度関数という。逆の$p(\theta | \mathbf{D}), p(知りたいパラメタ | 訓練データ)$は事後分布。ベイズの定理によって、尤度関数から事後分布を計算できる。

$$
p(\theta | \mathbf{D}) = \frac{p(\mathbf{D} | \theta) p(\theta)}{p(\mathbf{D})}
$$

**この左辺を計算することが学習にあたる**。

## 予測分布

1章にもあったように、訓練済のパラメタを使って予測結果を得るのは、以下の式になる。グラフィカルモデルからも同じような形が得られる。

$$
p(x_* | \mathbf{D}) = \int p(x_* | \theta) p(\theta | \mathbf{D}) d \theta \\\\ 
p(x_*, \theta | \mathbf{D}) = \frac{x_*, \theta, \mathbf{D}}{p(\mathbf{D})} = \frac{p(\mathbf{\theta}) p(x_* | \mathbf{\theta}) p(\mathbf{D} | \theta)}{p(\mathbf{D})}
$$

この枠組み、パラメタの事後分布$p(\theta | \mathbf{D})$がすべての学習データ$D$を持つことになるが、**データ量に対してモデルの表現能力が変化しないという大きな制限をもってしまう**。ガウス過程などのベイジアンノンパラメトリクスの手法で改善できるらしい。この本はやらない。

## 共役事前運否

事前分布$p(\theta)$と事後分布$p(\theta | \mathbf{D})$が、数学的な計算をすると**全く同形になる分布のことを共役事前分布**という。

逆に言えば普通は同じ分布にならない。ならないので面倒な積分を解く必要があるし、厳密に計算できるとも限らない場合は常々ある。

どのような事前分布が共役になるかは、尤度関数$p(\mathbf{D} | \theta)$の設計に依存する。

| 尤度関数         | パラメタ                            | 共役事前分布             | 予測分布               |
| ---------------- | ----------------------------------- | ------------------------ | ---------------------- |
| ベルヌーイ分布   | $\mu$                               | β分布                    | ベルヌーイ分布         |
| 二項分布         | $\mu$                               | β分布                    | β・二項分布            |
| カテゴリ分布     | $\mathbf{\pi}$                      | ディリクレ分布           | カテゴリ分布           |
| 多項分布         | $\mathbf{\pi}$                      | ディリクレ分布           | ディリクレ・多項分布   |
| ポアソン分布     | $\lambda$                           | γ分布                    | 負の二項分布           |
| 1次元ガウス分布  | $\mu$                               | 1次元ガウス分布          | 1次元ガウス分布        |
| 1次元ガウス分布  | $\sigma ^ 2$                        | γ分布                    | 1次元のstudentのt分布  |
| 1次元ガウス分布  | $\mathbf{\mu}, \sigma ^ 2$          | ガウス・γ分布            | 1次元のstudentのt分布  |
| 多次元ガウス分布 | $\mathbf{\mu}$                      | 多次元ガウス分布         | 多次元ガウス分布       |
| 多次元ガウス分布 | $\mathbf{\sigma} ^ 2$               | ウィシャート分布         | 多次元のstudentのt分布 |
| 多次元ガウス分布 | $\mathbf{\mu}, \mathbf{\sigma} ^ 2$ | ガウス・ウィシャート分布 | 多次元のstudentのt分布 |

**共役性があると、同じ分布のパラメタの関係式があって、それで積分せずに目的の分布の値がわかっちゃう！すごい！**

特にデータを小分けしながら学習してアップデートする場合、一章の逐次学習の式では、

$$
p(\theta | D_n, D_{n - 1}, \cdots, D_1) \propto p(\theta | D_{n - 1}, \cdots , D_1) p(D_n | \theta)
$$

で学習できる(ただし、それぞれの小分けしたデータは互いに独立であるという前提で)。
ここで共役事前分布を使うことで、上の逐次学習にあるすべての項は同じ形の分布なのでやりやすい。

