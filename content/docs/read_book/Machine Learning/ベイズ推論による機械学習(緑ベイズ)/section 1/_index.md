---
title: "第一章"
weight: 1
# bookFlatSection: false
# bookToc: true
# bookHidden: false
# bookCollapseSection: false
# bookComments: false
# bookSearchExclude: false
---

# 機械学習とは

機械学習とは、**いい感じに予測する学習器を作る**こと。代表的なタスクは以下の通り。

## 回帰

いい感じのデータ点を当てはめる式を作る。線形回帰ならば$y_n = \mathbf{w} ^ \mathbf{x}_n + \epsilon_n$ (1.1)。$\epsilon_n$はノイズで、ある分布に従う。

線形回帰のうち、$\mathbf{x}_n = (1, x_n, x_n^2, \cdots ) ^ T $のものを**多項式回帰**という。

## 分類

二値分類など。連続値から離散値にする。よく使われるのは、

$$
\mathrm{Sig}(x) = \frac{1}{1 + e^{-x}}
$$

これを使うことで、$(-\inf, +\inf)$を$(0, 1)$に変換できる。これが確率らしきものなので、確率と考えてもいい。

これを**多クラス分類に拡張**すると、**ソフトマックス関数を使えばいい**。
入力は$\mathbf{a} = (a_1, a_2, \cdots, a_K) ^ T$という$K$次元のベクトルで、それぞれが**各クラスを代表する値**。クラス$k$である確率は

$$
\mathrm{SM}_k(\mathbf{a}) = \frac{e^{- a_k}}{\sum _{i = 1}^{K} e^ {- a_i}}
$$

こうすることで、$\sum _{i = 1} ^ K \mathrm{SM}_i (\mathbf{a}) = 1$を満たす確率らしきものに変換できる。

## クラスタリング

与えられたデータの近くのグループをまとめる感じ、

## 次元削減

$D \times N$の行列$\mathbf{Y}$を、$M \times D$の行列$\mathbf{W}$と$M \times N$の行列$\mathbf{X}$で近似する分解手法。

$$
\mathbf{Y} \approx \mathbf{W} ^ T \mathbf{X}
$$

ここで、$D, N >> M$となると、本来$O(DN)$必要なメモリが$O(M(D + N))$に圧縮できる。**完全な復元は無理であるが、近似できればいいと考えれば有効な削減方法**。

# 機械学習のアプローチ

## ツールボックス

既知の機械学習の手法について、何かしらの基準に従って性能が良い手法を選んで識別器を作るという考え。教師データでラベル付きのものを使うので、教師アリ学習となる。

複雑な数学の知識無くても使えるが、本当に上手くfitする手法があるとは限らない。

## モデリング

**データに関するモデルがあると仮定して、事前にそれを構築する**。そのパラメタを学習データから訓練する。数学バリバリ使う。

柔軟性は高いが訓練は大変で数学も大変。

# ベイズの定理

$$
p(x | y) = \frac{p(y | x) p(x)}{p(y)} = \frac{p(y | x) p(x)}{\int p(x, y) dx}
$$

# 事前分布、事後分布

事前分布は、**条件$x$の分布**→$p(x)$

事後分布とは、**結果$y$がわかっているときの、前提条件$x$の分布**→$p(y | x)$。

事後で情報がわかることによって、この2つは全然違う分布によくなったりする。

p.20まで